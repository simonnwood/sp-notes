epi <- seir(bmu=7e-5,bsc=7e-5);hist(epi$beta,xlab="beta",main="")
plot(epi$S,ylim=c(0,max(epi$S)),xlab="day",ylab="N")
points(epi$E,col=4);points(epi$I,col=2)
epi <- seir(bmu=4.6e-5,bsc=2e-6);hist(epi$beta,xlab="beta",main="")
plot(epi$S,ylim=c(0,max(epi$S)),xlab="day",ylab="N")
points(epi$E,col=4);points(epi$I,col=2)
n.rep <- 10000; n.ci <- 0; n <- 20;p <- .2
for (i in 1:n.rep) {
x <- rbinom(1,n,p)
p.hat <- x/n
sig.p <- sqrt(p.hat*(1-p.hat)/n)
if (p.hat - 1.96*sig.p <p && p.hat+1.96*sig.p > p) n.ci <- n.ci + 1
}
n.ci/n.rep
getwd()
getwd()
setwd("/home/sw283/lnotes/StatProg/sp-notes/code")
getwd()
source("hello.r")
a <- "some stuff"
alice <- list(age=45,height=1.65,weight=60)
Ba <- matrix(1:16,4,4)
dump(c("a","alice","Ba"),file="stuff.R")
rm(list=ls());ls()
source("stuff.R")
a <- scan("data.txt",what="c");a
b <- scan("data.txt",what=list("a",1,1),skip=1); b
scan("data.txt",what="c",n=3)
names(b) <- scan("data.txt",what="c",n=3)
b
d <- data.frame(b); d
d <- read.table("data.txt",header=TRUE)
d
d$name[1] <- "susan"
d
write.table(d,file="data1.txt")
A <- matrix(1:6,3,2); y <- rnorm(100)
save(A,y,file="stuff.Rd")
rm(list=ls());ls()
load("stuff.Rd")
raw <- readLines("https://michaelgastner.com/DAVisR_data/homicides.txt")
raw[1]
raw[300]
PlantGrowth)
PlantGrowth
X <- model.matrix(weight~group,PlantGrowth)
X
library(ggplot2)
mpg1 <- data.frame(mpg) ## convert to regular data frame
head(mpg1)
mpg1$trans <- factor(gsub("\\(.*\\)","",mpg1$trans)) ## convert to 2 level
head(mpg1)
head(model.matrix(cty~trans+displ,mpg1)) ## get model matrix
head(model.matrix(cty~trans*displ,mpg1))
library(ggplot2)
mpg1 <- data.frame(mpg) ## convert to regular data frame
head(mpg1)
mpg1$trans <- factor(gsub("\\(.*\\)","",mpg1$trans)) ## convert to 2 level
mpg1$trans <- factor(gsub("\\(.*\\)","",mpg1$trans)) ## convert to 2 level
head(mpg1)
m1 <- lm(cty ~ trans + displ, mpg1)
m1
plot(m1)
plot(mpg1$displ,residuals(m1))
m2 <- lm(cty~trans+displ+I(displ^2),mpg1)
plot(m2)
summary(m2)
m3 <- lm(cty~(trans+displ+I(displ^2))^2,mpg1)
m0 <- lm(cty~displ+I(displ^2),mpg1)
anova(m0,m3) ## F ratio test of H_0: m0 is correct vs H_1: we need m3
summary(lm(cty~trans,mpg1))
n <- 2000; y <- runif(n) ## example vector
A <- matrix(runif(n*n),n,n) ## example matrix
B <- matrix(runif(n*n),n,n) ## example matrix
system.time(a1 <- A %*% B %*% y)
system.time(a2 <- A %*% (B %*% y))
range(a1-a2) ## results the same
A <- matrix(1:12,3,4)
A;t(A)
crossprod(A); t(A) %*% A
diag(crossprod(A))
A*A
A+A
A-A
b <- 5:8
A %*% b
drop(A%*%b)
A <- matrix(runif(16),4,4)
c <- runif(4)
A
c
b <- solve(A,c)
drop(A%*%b);c
Ai <- solve(A)
Ai
round(Ai%*%A,digits=14)
n <- 3000
A <- crossprod(matrix(runif(n*n),n,n)) ## example +ve def matrix
b <- runif(n)  ## example n vector
system.time(c0 <- solve(A,b)) ## solve
system.time({R <- chol(A);    ## solve with cholesky
c1 <- backsolve(R,forwardsolve(t(R),b))})
range(c0-c1)/norm(A) ## confirm same result
ldmvn <- function(y,m,S) {
## Cholesky based evaluation of log density of MVN at y.
## m is mean and S the covariance matrix.
R <- chol(S) ## Get Cholesky factorization R'R = S
z <- forwardsolve(t(R),y-m) ## R^{-T}(y-m)
-sum(z^2)/2 - sum(log(diag(R))) - length(m)*log(2*pi)/2
} ## ldmvn
y <- c(3,4.5,2); m <- c(.6,.8,1.2)
S <- matrix(c(1,.5,.3,.5,2,1,.3,1,1.5),3,3)
ldmvn(y,m,S)
n <- 3000
A <- crossprod(matrix(runif(n*n),n,n)) ## example +ve def matrix
b <- runif(n)  ## example n vector
system.time(c0 <- solve(A,b))
system.time({R <- chol(A);    ## solve with cholesky
c1 <- backsolve(R,forwardsolve(t(R),b))})
range(c0-c1)/norm(A)
ldmvn <- function(y,m,S) {
## Cholesky based evaluation of log density of MVN at y.
## m is mean and S the covariance matrix.
R <- chol(S) ## Get Cholesky factorization R'R = S
z <- forwardsolve(t(R),y-m) ## R^{-T}(y-m)
-sum(z^2)/2 - sum(log(diag(R))) - length(m)*log(2*pi)/2
} ## ldmvn
y <- c(3,4.5,2); m <- c(.6,.8,1.2)
S <- matrix(c(1,.5,.3,.5,2,1,.3,1,1.5),3,3)
S
ldmvn(y,m,S)
X <- model.matrix(~group,PlantGrowth) ## model matrix (note: response not needed!)
head(X)
PlantGrowth
X <- model.matrix(~group,PlantGrowth)
X
qrx <- qr(X)
qr.R(qrx)
y <- PlantGrowth$weight
p <- ncol(X)              ## number of parameters
Qty <- qr.qty(qrx,y)[1:p]
Qty
beta <- backsolve(qr.R(qrx),Qty)
beta
coef(lm(weight~group,PlantGrowth))
pairs(iris)
X <- sweep(as.matrix(iris[,1:4]),2,colMeans(iris[,1:4])) ## col centred data matrix
ec <- eigen(t(X)%*%X/(nrow(X)-1))    ## eigen decompose the covariance matrix
U <- ec$vectors;lambda <- ec$values  ## exract eigenvectors and values
Z <- X %*% U;                        ## the principle co-ordinated
plot(Z[,1],Z[,2],col=c(1,2,4)[as.numeric(iris$Species)],main="iris PCA",
xlab="PCA 1",ylab="PCA 2")      ## plot first two components
pairs(iris)
X <- sweep(as.matrix(iris[,1:4]),2,colMeans(iris[,1:4])) ## col centred data matrix
ec <- eigen(t(X)%*%X/(nrow(X)-1))
U <- ec$vectors;lambda <- ec$values
Z <- X %*% U;
plot(Z[,1],Z[,2],col=c(1,2,4)[as.numeric(iris$Species)],main="iris PCA",
xlab="PCA 1",ylab="PCA 2")      ## plot first two components
apply(Z,2,var);lambda
sum(lambda[1:2])/sum(lambda)
library(ggplot2)
head(mpg)
mpg$transmission <- rep("manual",nrow(mpg)) ## simplified transmission variable
mpg$transmission[grep("auto",mpg$trans)] <- "automatic" ## note automatics
par(mfrow=c(1,2),mar=c(4,4,1,1)) ## two panel window
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",
pch=19,col=(mpg$transmission=="automatic")+1)
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",type="n") # empty plot
rect(0,0,8,50,col="lightgrey") # add a grey rectangle over plot area
abline(v=2:7,h=seq(10,45,by=5),col="white") # add grid lines
points(mpg$displ,mpg$hwy,col=(mpg$transmission=="automatic")+1,pch=19,cex=.5)
a <- ggplot(mpg,aes(x=displ,y=hwy)) + geom_point(aes(colour=transmission)) +
geom_smooth(method=gam,formula=y~s(x)) ## add a smooth as it's easy!
a <- ggplot(mpg,aes(x=displ,y=hwy)) + geom_point(aes(colour=transmission)) +
geom_smooth(method="gam",formula=y~s(x)) ## add a smooth as it's easy!
a <- ggplot(mpg,aes(x=displ,y=hwy))
a
a <- a + geom_point(aes(colour=transmission))
a
a <- a + geom_smooth(method="gam",formula=y~s(x)) ## add a smooth as it's easy!
a
b1 <- a + coord_cartesian(xlim=c(0,8),ylim=c(0,50))
b1
b2 <- b1 + scale_x_continuous(limits=c(1,7),name="displacement (litres)")
b2
b2 <- a + scale_x_continuous(limits=c(1,7),name="displacement (litres)")
b2
b2 <- a + scale_x_continuous(limits=c(1,6),name="displacement (litres)")
b2
b2 <- a + scale_x_continuous(limits=c(2,6),name="displacement (litres)")
b2
View(X)
library(ggplot2)
head(mpg)
mpg$transmission <- rep("manual",nrow(mpg)) ## simplified transmission variable
mpg$transmission[grep("auto",mpg$trans)] <- "automatic" ## note automatics
par(mfrow=c(1,2),mar=c(4,4,1,1)) ## two panel window
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",
pch=19,col=(mpg$transmission=="automatic")+1)
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",type="n") # empty plot
rect(0,0,8,50,col="lightgrey")
abline(v=2:7,h=seq(10,45,by=5),col="white")
points(mpg$displ,mpg$hwy,col=(mpg$transmission=="automatic")+1,pch=19,cex=.5) # data
a <- ggplot(mpg,aes(x=displ,y=hwy))
a
a <- a + geom_point(aes(colour=transmission))
a
a <- a + geom_smooth(method="gam",formula=y~s(x))
a
b1 <- a + coord_cartesian(xlim=c(0,8),ylim=c(0,50))
b1
b2 <- a + scale_x_continuous(limits=c(2,6),name="displacement (litres)")
b2
rm(list=ls())
library(ggplot2)
head(mpg)
mpg$transmission <- rep("manual",nrow(mpg)) ## simplified transmission variable
mpg$transmission[grep("auto",mpg$trans)] <- "automatic" ## note automatics
par(mfrow=c(1,2),mar=c(4,4,1,1)) ## two panel window
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",
pch=19,col=(mpg$transmission=="automatic")+1)
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",type="n")
rect(0,0,8,50,col="lightgrey")
abline(v=2:7,h=seq(10,45,by=5),col="white")
points(mpg$displ,mpg$hwy,col=(mpg$transmission=="automatic")+1,pch=19,cex=.5) # data
a <- ggplot(mpg,aes(x=displ,y=hwy))
a
a <- a + geom_point(aes(colour=transmission))
a
a <- a + geom_smooth(method="gam",formula=y~s(x))
a
b1 <- a + coord_cartesian(xlim=c(0,8),ylim=c(0,50))
b1
b2 <- a + scale_x_continuous(limits=c(2,6),name="displacement (litres)")
b2
par(mfrow=c(1,2)) ## set up to allow 2 histograms side by side
hist(faithful$eruptions) ## default
hist(faithful$eruptions,breaks=quantile(faithful$eruptions,seq(0,1,length=15)),
xlab="eruption time (mins)",main="Old Faithful geyser",col="purple")
n <- nrow(faithful);set.seed(8)
erupt <- faithful$eruption + rnorm(n*1000,sd=.2) ## simulate much bigger sample
hist(erupt,breaks=100,freq=FALSE) ## plot it
plot(density(faithful$eruptions,adjust=0.6),type="l",xlab="eruption time (mins)",main="kernel denisty estimate")
ggplot(faithful,aes(x=eruptions,after_stat(density))) + ## data, base mappings etc
geom_histogram(bins=20,col="green",fill="purple") +     ## garish histogram
geom_density(adjust=.5,col="red")                       ## added kernel density estimate
library(ggplot2)
ggplot(faithful,aes(x=eruptions,after_stat(density))) + ## data, base mappings etc
geom_histogram(bins=20,col="green",fill="purple") +     ## garish histogram
geom_density(adjust=.5,col="red")                       ## added kernel density estimate
par(mfrow=c(1,2))
acf(faithful$eruptions); pacf(faithful$eruptions)
d <- c(3362,428,290,369,978,1723,2471,3731,5607,7926,12934,20688,29905,40456,53579,
84784,106448,136269,150325,178684) ## all deaths
cd <- c(3,1,3,10,26,65,133,257,438,740,1418,2587,4250,6337,8528,13388,18622,25220,
27718,30247)  ## Covid-19 deaths
names(cd) <- names(d) <- c("0","1-4","5-9","10-14","15-19","20-24","25-29","30-34",
"35-39","40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79",
"80-84","85-89","90+") ## ONS age classes
par(mfrow=c(1,2))
barplot(d,las=2,main="All deaths Mar 20-Aug 21")
barplot(cd,ylim=c(0,max(d)),las=2,main="Covid deaths Mar 20-Aug 21")
par(mfrow=c(1,2))
D <- rbind(cd,d-cd) ## create matrix of counts to plot
rownames(D) <- c("covid","not covid")
barplot(D,las=2,main="All deaths Mar 20-Aug 21",legend.text=TRUE,
args.legend=list(x=7,y=150000))
par(mfrow=c(1,2))
D <- rbind(cd,d-cd) ## create matrix of counts to plot
rownames(D) <- c("covid","not covid")
barplot(D,las=2,main="All deaths Mar 20-Aug 21",legend.text=TRUE,
args.legend=list(x=7,y=150000))
barplot(D[,1:10],las=2,main="Age 0-44 deaths  Mar 20-Aug 21")
par(mfrow=c(1,3),mar=c(5,5,1,1))
boxplot(len~supp,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose+supp,data=ToothGrowth,cex.lab=1.5)
library(ggplot2)
ToothGrowth$comb <- with(ToothGrowth,paste(supp,dose))
ggplot(ToothGrowth,aes(y=len,group=comb)) + geom_boxplot()
ggplot(ToothGrowth,aes(y=len,x=as.factor(dose))) + geom_violin()
foo <- function(x,y) { ## function to be visualized
r <- sqrt(x^2+y^2)
exp(-r*2)*cos(r*10)
} ## foo
n <- 50; x <- seq(-1,1,length=n); y <- seq(-1,1,length=n)
xy = expand.grid(x=x,y=y) ## regular x,y grid over
xy$z <- foo(xy$x,xy$y)    ## evaluate foo on the grid
z <- matrix(xy$z,n,n)     ## put evaluated function in a matrix
par(mfrow=c(1,3),mar=c(4,4,1,1)) ## divide plot devise into 3 columns
persp(x,y,z,theta=30,phi=30,col="lightblue",shade=.3)
image(x,y,z)
contour(x,y,z,xlab="x",ylab="y")
ggplot(xy,aes(x=x,y=y)) + geom_raster(aes(fill=z)) + geom_contour(aes(z=z))
ToothGrowth
View(ToothGrowth)
View(ToothGrowth)
head(ToothGrowth)
par(mfrow=c(1,3),mar=c(5,5,1,1))
boxplot(len~supp,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose+supp,data=ToothGrowth,cex.lab=1.5)
library(ggplot2)
ToothGrowth$comb <- with(ToothGrowth,paste(supp,dose))
View(ToothGrowth)
View(ToothGrowth)
ggplot(ToothGrowth,aes(y=len,group=comb)) + geom_boxplot()
ggplot(ToothGrowth,aes(y=len,x=as.factor(dose))) + geom_violin()
foo <- function(x,y) { ## function to be visualized
r <- sqrt(x^2+y^2)
exp(-r*2)*cos(r*10)
} ## foo
n <- 50; x <- seq(-1,1,length=n); y <- seq(-1,1,length=n)
xy = expand.grid(x=x,y=y)
head(xy)
xy$z <- foo(xy$x,xy$y)
z <- matrix(xy$z,n,n)
par(mfrow=c(1,3),mar=c(4,4,1,1)) ## divide plot devise into 3 columns
persp(x,y,z,theta=30,phi=30,col="lightblue",shade=.3)
image(x,y,z)
contour(x,y,z,xlab="x",ylab="y")
image(x,y,z)
contour(x,y,z,xlab="x",ylab="y",add=TRUE)
head(xy)
ggplot(xy,aes(x=x,y=y)) + geom_raster(aes(fill=z)) + geom_contour(aes(z=z))
mat.fun <- function(A,fun=I) {
if (!is.matrix(A)) A <- matrix(A) ## coerce to matrix if not already a matrix
if (ncol(A)!=nrow(A)) stop("matrix not square") ## halt execution in this case
if(max(abs(A-t(A)))>norm(A)*1e-14) stop("matrix not symmetric")
ea <- eigen(A,symmetric=TRUE) ## get eigen decomposition
## generalize 'fun' to matrix case...
ea$vectors %*% (fun(ea$values)*t(ea$vectors)) ## note use of re-cycling rule!
}
## test fun=I and inverse
n <- 10; A <- matrix(runif(n*n)-.5,n,n)
A <- A + t(A) ## make symmetric
B <- mat.fun(A) ## identity function case
if (max(abs(A-B))>norm(A)*1e-10) warning(" mat.fun I failure")
B <- mat.fun(A,function(x) 1/x) ## inverse function case
if (max(abs(solve(A)-B))>norm(A)*1e-10) warning("mat.fun inv failure")
A <- crossprod(A) ## ensure +ve def for sqrt case
B <- mat.fun(A,sqrt) ## sqrt function case
if (max(abs(A-B%*%B))>norm(A)*1e-10) warning("mat.fun sqrt failure")
mat.fun(matrix(1:12,4,3))
try(mat.fun(matrix(1:12,4,3)),silent=TRUE)
inherits(try(mat.fun(matrix(1:12,4,3)),silent=TRUE),"try-error")
if (!inherits(try(mat.fun(matrix(1:12,4,3)),silent=TRUE),"try-error"))
warning("error handling failure")
t80 <- 1:13 ## years since 1980
y <- c(12,14,33,50,67,74,123,141,165,204,253,246,240) ## AIDS cases
plot(t80,y)
nll <- function(theta,t,y) {
## -ve log likelihood for AIDS model y_i ~ Poi(alpha*exp(beta*t_i))
## theta = (alpha,beta)
mu <- theta[1] * exp(theta[2] * t) ## mu = E(y)
-sum(dpois(y,mu,log=TRUE)) ## the negative log likelihood
} ## nll
fit <- optim(c(10,.1),nll,y=y,t=t80,hessian=TRUE)
fit
V <- chol2inv(chol(fit$hessian)) ## solve(fit$hessian) also possible
se <- diag(V)^.5 ## extract standard deviations
se
gll <- function(theta,t,y) {
## grad of -ve log lik of Poisson AIDS early epidemic model
alpha <- theta[1];beta <- theta[2] ## enhances readability
ebt <- exp(beta*t) ## avoid computing twice
-c(sum(y)/alpha - sum(ebt),     ## -dl/dalpha
sum(y*t) - alpha*sum(t*ebt)) ## -dl/dbeta
} ## gll
fd <- th0 <- c(10,.1) ## test param value, and approx grad vec
nll0 <- nll(th0,t=t80,y=y) ## nll at th0
eps <- 1e-7  ## finite difference interval
for (i in 1:length(th0)) { ## loop over parameters
th1 <- th0; th1[i] <- th1[i] + eps ## increase th0[i] by eps
nll1 <- nll(th1,t=t80,y=y) ## compute resulting nll
fd[i] <- (nll1 - nll0)/eps ## approximate -dl/dth[i]
}
fd;gll(th0,t80,y) ## compare
fit <- optim(c(10,.1),nll,gr=gll,y=y,t=t80,method="BFGS",hessian=TRUE)
fit
hll <- function(theta,t,y) {
## Hessian of -ve log lik of Poisson AIDS early epidemic model
alpha <- theta[1];beta <- theta[2] ## enhances readability
ebt <- exp(beta*t) ## avoid computing twice
H <- matrix(0,2,2) ## matrix for Hessian of -ve ll
H[1,1] <- sum(y)/alpha^2
H[2,2] <-  alpha*sum(t^2*ebt)
H[1,2] <- H[2,1] <- sum(t*ebt)
H
} ## hll
gll0 <- gll(th0,t=t80,y=y) ## gran of nll at th0
eps <- 1e-7  ## finite difference interval
Hfd <- matrix(0,2,2) ## finite diference Hessian
for (i in 1:length(th0)) { ## loop over parameters
th1 <- th0; th1[i] <- th1[i] + eps ## increase th0[i] by eps
gll1 <- gll(th1,t=t80,y=y) ## compute resulting nll
Hfd[i,] <- (gll1 - gll0)/eps ## approximate second derivs
}
Hfd;hll(th0,t=t80,y=y)
nll2 <- function(theta,t,y) {
## wrapper function for nll and its grad and Hessian,
## suitable for optimization by nlm
z <- nll(theta,t,y) ## the objective
attr(z,"gradient") <- gll(theta,t,y)
attr(z,"hessian") <- hll(theta,t,y)
z
} ## nll2
nlm(nll2,th0,y=y,t=t80)
nll3 <- function(theta,t,y) {
## -ve log likelihood for AIDS model y_i ~ Poi(alpha*exp(beta*t_i))
## theta = (log(alpha),beta)
alpha <- exp(theta[1]) ## so theta[1] unconstrained, but alpha > 0
beta <- theta[2]
mu <- alpha * exp(beta * t) ## mu = E(y)
-sum(dpois(y,mu,log=TRUE))  ## the negative log likelihood
} ## nll3
optim(c(log(10),.1),nll3,y=y,t=t80,method="BFGS")
exp(3.14)
rb <- deriv(expression(k*(z-x^2)^2 + (1-x)^2),
c("x","z"), ## diff w.r.t. these
function.arg=c("x","z","k"),
hessian=TRUE)
rb
nlli <- deriv(expression(-y*(log(alpha)+beta*t)+alpha*exp(beta*t)+lgamma(y+1)),
c("alpha","beta"), ## diff w.r.t. these
function.arg=c("alpha","beta","t","y"),
hessian=TRUE)
nlli(10,.1,t80,y)
nll4 <- function(th,t,y) {
nli <- nlli(th[1],th[2],t,y)
nll <- sum(nli)
attr(nll,"gradient") <- colSums(attr(nli,"gradient"))
attr(nll,"hessian") <- apply(attr(nli,"hessian"),c(2,3),sum)
nll
} ## nll4
nll4(th0,t80,y)
nll2(th0,t80,y)
library(rjags)                ## load rjags (and coda)
setwd("~sw283/lnotes/StatProg/sp-notes/code")  ## set working directory
## compile model...
mod <- jags.model("basic.jags",data=list(x=nhtemp,N=length(nhtemp)))
sam <- jags.samples(mod,c("mu","tau"),n.iter=10000)
str(sam)
sam.coda <- coda.samples(mod,c("mu","tau"),n.iter=10000)
str(sam.coda)
plot(sam.coda)
acfplot(sam.coda)
plot(sam.coda)
crosscorr(sam.coda)
effectiveSize(sam.coda)
HPDinterval(sam.coda[[1]])
apply(sam.coda[[1]],2,quantile,prob=(c(0.025,.975)))
mod2 <- jags.model("basic2.jags",data=list(x=nhtemp,t=1:N,N=N))
sam2.coda <- coda.samples(mod2,c("alpha","beta","tau","df"),n.iter=10000)
plot(sam2.coda)
N <- length(nhtemp)
mod2 <- jags.model("basic2.jags",data=list(x=nhtemp,t=1:N,N=N))
sam2.coda <- coda.samples(mod2,c("alpha","beta","tau","df"),n.iter=10000)
plot(sam2.coda)
plot(sam2.coda)
?plot.coda
par(mfrow=c(2,4))
plot(sam2.coda)
plot(sam2.coda[[1]])
plot(sam2.coda[[1]][1:2,])
str(sam.coda)
plot(sam2.coda[[1]][,1:2])
effectiveSize(sam2.coda)
crosscorr(sam2.coda)
acfplot(sam2.coda,aspect=1)
mod2 <- jags.model("basic2.jags",data=list(x=nhtemp,t=1:N-N/2,N=N))
sam2.coda <- coda.samples(mod2,c("alpha","beta","tau","df"),n.iter=10000)
plot(sam2.coda)
effectiveSize(sam2.coda)
crosscorr(sam2.coda)
acfplot(sam2.coda,aspect=1)
mod <- jags.model("basic.jags",data=list(x=nhtemp,N=length(nhtemp)),n.chains=2)
mod2 <- jags.model("basic2.jags",data=list(x=nhtemp,t=1:N,N=N),n.chains=2)
dic.samples(mod,n.iter=10000)
dic.samples(mod2,n.iter=10000)
library(rjags)                ## load rjags (and coda)
setwd("~sw283/lnotes/StatProg/sp-notes/code")
mod <- jags.model("basic.jags",data=list(x=nhtemp,N=length(nhtemp)))
sam <- jags.samples(mod,c("mu","tau"),n.iter=10000)
str(sam)
sam.coda <- coda.samples(mod,c("mu","tau"),n.iter=10000)
str(sam.coda)
plot(sam.coda)
acfplot(sam.coda)
crosscorr(sam.coda)
effectiveSize(sam.coda)
HPDinterval(sam.coda[[1]])
apply(sam.coda[[1]],2,quantile,prob=(c(0.025,.975)))
N <- length(nhtemp)
mod2 <- jags.model("basic2.jags",data=list(x=nhtemp,t=1:N,N=N))
sam2.coda <- coda.samples(mod2,c("alpha","beta","tau","df"),n.iter=10000)
plot(sam2.coda)
plot(sam2.coda[[1]][,1:2])
effectiveSize(sam2.coda)
crosscorr(sam2.coda)
acfplot(sam2.coda,aspect=1)
mod2 <- jags.model("basic2.jags",data=list(x=nhtemp,t=1:N-N/2,N=N))
sam2.coda <- coda.samples(mod2,c("alpha","beta","tau","df"),n.iter=10000)
plot(sam2.coda)
effectiveSize(sam2.coda)
crosscorr(sam2.coda)
acfplot(sam2.coda,aspect=1)
mod <- jags.model("basic.jags",data=list(x=nhtemp,N=length(nhtemp)),n.chains=2)
mod2 <- jags.model("basic2.jags",data=list(x=nhtemp,t=1:N,N=N),n.chains=2)
dic.samples(mod,n.iter=10000)
dic.samples(mod2,n.iter=10000)
