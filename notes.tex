\documentclass[10pt] {article}
\usepackage{epsf}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage{makeidx,epsfig,lscape}
\usepackage{natbib}
\usepackage{rotating}
\usepackage{floatpag}
\rotfloatpagestyle{empty}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{epsf}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{comment}
\usepackage{bm}
\usepackage{times}
\setlength{\textheight}{22.5cm}
\setlength{\textwidth}{16cm}
\setlength{\oddsidemargin}{-5mm}
\setlength{\topmargin}{-0.8cm}
\setlength{\evensidemargin}{-5mm}
\usepackage{listings}
\newcommand{\grad}{\nabla}
\newcommand{\tr}[1]{\text{tr}(#1)}
\newcommand{\bp}{{\vm \beta}}
\newcommand{\X}{{\vf X}}
\newcommand{\E}{E}
\newcommand{\vf}{\bf} %% vector type-setting
\newcommand{\vm}{\bm} %% vector type-setting
\newcommand{\ts}{^{\rm T}}
\newcommand{\its}{^{-\rm T}}
\newcommand{\bmat}[1]{\left [ \begin{array}{#1}}
\newcommand{\emat}{\end{array}\right ]}
\newcommand{\eps}[3]
{{\begin{center}
 \rotatebox{#1}{\scalebox{#2}{\includegraphics{#3}}}
 \end{center}}
}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax} %% original {arg\,max} to space arg max
\lstset{language=R,
        basicstyle={\ttfamily\small},
        keywordstyle=,
        showstringspaces=false,
        columns=flexible}

%% Definitions
\newcommand {\hide}[1] {\typeout{ #1 }}
%Comment out to print all
%\newcommand {\hide}[1] {{\it #1 }}
%Comment out to hide some
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\dif}[2]{\frac{{\rm d} #1}{{\rm d} #2}}
\newcommand{\ildif}[2]{{\rm d} #1/{{\rm d} #2 }}
\newcommand{\ilpdif}[2]{\partial #1/{\partial #2 }}
\newcommand{\pdif}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pddif}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\ilpddif}[3]{\partial^2 #1/{\partial #2 \partial #3}}
\newcommand{\comb}[2]{\left (\begin{array}{c}{#1}\\{#2}\end{array}\right )}
\newcommand{\gfrac}[2]{\mbox{$ { \textstyle{ \frac{#1}{#2} }\displaystyle}$}}
\newcommand{\R}{{\sf R }}
\theoremstyle{definition}
  \newtheorem*{definition}{Definition}
  \newtheorem*{example}{Example}
%\newcommand{\vm}{\bm}
% comment out next line unless double spacing needed
%\renewcommand{\baselinestretch}{2}

\newtheorem{theorem}{Theorem}

\makeindex

\begin {document}

\centerline{\huge \bf Statistical Programming}

\tableofcontents


\section{Software Requirements: R, git, JAGS etc}

To complete this course you will need to get yourself a free github account and install the following software on your computer: 
\begin{itemize}
\item R or Rstudio, git, pandoc and JAGS.
\item R packages: ggplot2, rjags, rmarkdown, debug.
\item Only if you do not already have latex installed, install tinytex. 
\end{itemize}
It is assumed that you have the basic computer skills to do this. If not, you will need to spend some time online acquiring them (this course is about programming, not basic computer skills). If you have difficulty, you can post questions on piazza for other students to answer. Please answer other student's questions on installation on piazza. 

\index{git!installation}\index{github!account}\index{R!instalation}\index{JAGS!installation}\index{Rstudio!installation}
\index{CRAN}
In more detail.
\begin{itemize}
\item {\tt R} is the free statistical programming language and environment that we will use. You can get a copy from the Comprehensive R Archive Network (CRAN)\\
\lstinline+https://cran.r-project.org/+\\
just follow the links under `Download and Install R'. 
\item {\tt Rstudio} is an alternative front end for R, which many people prefer to use. It provides an R code editor, R session window, R graphics window and other information, all in a convenient `integrated environment'. If you prefer this to R as available from CRAN, you can get it from\\
\lstinline+https://www.rstudio.com/+\\
at the foot of the page, under `R studio desktop', or go straight to \\
\lstinline+https://www.rstudio.com/products/rstudio/download/#download+
\item {\tt git} is a version control system. It helps you to write code in teams, as you must do for this course, without breaking each other's work. It also lets you keep a record of the changes you make, so that you can go back to earlier versions, if you need to. To install {\tt git} follow the instructions at\\
\lstinline+https://www.git-scm.com/+\\
On Windows use the default options. {\tt linux} systems often have it installed already and something like \lstinline+sudo apt install git+ will install it if not. 
\item For Mac or linux you also need to install the Git Credential Manager Core (this is automatically installed for Windows). Instructions are at:\\
{\scriptsize \verb+https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git+}\\
Linux is slightly fiddly (I used the plain text option 4 for credential storage). 
\item {\tt pandoc} is document conversion software used by the {\tt rmarkdown} package. Installation instructions:\\
\lstinline+https://pandoc.org/installing.html+ 
\item {\tt JAGS} stands for `Just Another Gibbs Sampler'. We will use it for programming Bayesian models and sampling from them. Downloads are available here:\\
\lstinline+https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/+\\
but under linux something like \lstinline+sudo apt install jags+ can be used. Note that if you have a new apple/mac with an apple silicon processor (rather than intel), then you will need to install MacPorts from\\ 
\lstinline+https://www.macports.org/install.php+\\
then install JAGS as described here\\
\lstinline+https://ports.macports.org/port/jags/+
\end{itemize}

Once the above are installed, you will need to install some packages from the R session command line. R/Rstudio can install packages in a local directory/folder for you, but I tend to start R as administrator/super user/root and then install them (e.g. on linux I would start R from the command line as \lstinline+sudo R+). Here are the packages you need:
\begin{itemize}
\item {\tt ggplot2} provides a nice alternative to built in R graphics. Install by typing\\
\lstinline+install.packages("ggplot2")+\\
at the R command line.
\item {\tt rmarkdown} provides a neat way of documenting data analyses using R. Install using\\
 \lstinline+install.packages("rmarkdown")+
\item {\tt rjags} lets you use JAGS directly from R. Install using\\
 \lstinline+install.packages("rjags")+
\item Finally we need a debugging package, to help you to find errors in your code, or understand how code is working by stepping through it. What is built into R and Rstudio is not great, and a much better option is Mark Bravington's debug package. This is not available from CRAN, but only from Mark's repository, so to install, you need to do this:
\begin{lstlisting}
options(repos=c("https://markbravington.github.io/Rmvb-repo",
        getOption( "repos")))
install.packages("debug")
\end{lstlisting}
\item {\tt rmarkdown} requires a latex installation in order to allow you to produce pdf documents. So, {\em only if you do not already have latex installed}, install {\tt tinytex} with
\lstinline+install.packages("tinytex")+
then if you are {\em really sure} that you do not already have latex installed run 
\lstinline+tinytex::install_tinytex()+
WARNING: this command may break existing latex installations!
\end{itemize}
Once you have the software installed, you need to do one more thing. Get a free {\tt github} account (if you do not already have one).  \lstinline+https://github.com/join+ is where to get one. 


\subsection{Using a terminal window, choose a text editor}

This course involves programming - that is writing text instructions that get a computer to do something. Anyone who programmes rapidly discovers that writing text to get a computer to do something is often quicker, more convenient and more reproducible than clicking your way through a graphical interface. This is sometimes true even for basic tasks such as file copying, or moving between directories/folders. 

Whatever operating system you are using you will be able to launch a `terminal window' for this purpose. Make sure that for your operating system you know how to do this, how to list the names of the files in a directory/folder, how to change from one directory/folder to another, how to delete a file, and how to close the terminal window. 

You will also need to edit text files containing the computer code you write. If you choose to use {\tt Rstudio}, there is a built in editor, and you might choose to just use that. If you use plain {\tt R} then you will need an external editor. {\tt Word} is not suitable, as it will insert hidden characters in your code that will cause problems, so you instead should use something simpler (e.g. {\tt wordpad} or {\tt notepad} on windows). Make sure you know what is available on your computer and use that. I tend to use {\tt emacs}, but this has more features than you actually need for this course. 

\section{git and github}

{\tt git} is a system that lets you track changes in code files and to manage those files when several people are working on them. In particular it lets you maintain a central {\em repository} of your code, which serves as the backed up master copy. The repository is just a folder/directory containing the master copy of your code. Several people can make working local copies of the repository (on their own computers) and work on the code, only merging those changes into the master copy when they are ready. {\tt git} provides the tools to allow management of the master copy in a way that avoids or resolves conflicts. A conflict is when two people made contradictory changes to the code. \index{git}

\subsection{Setting up a repo on {\tt github}}

An easy way to set up a central repository is to use {\tt github}.\index{github!repository}  
\begin{itemize}
\item Login to your {\tt github} account on {\tt github.com}, click on {\tt Repositories} and then click on {\tt New} to set up a new repository.
\item Follow the instructions. You probably want your repository to be private if it is for working on assessed coursework.
\end{itemize}
For group homework, one person sets up the repo and invites their team mates to collaborate on it. 
\begin{itemize}
\item Log on to {\tt github}, nagivate to your repo, and click on {\tt Settings}.
\item Click on {\tt Manage Access} from the left hand menu, and then on {\tt Invite a collaborator} - you will need their github user name. 
\end{itemize}
You can add files directly to your repo, via the web interface, but we will shortly cover how to add files to the local copy of your repo, and then {\em push} them to github. 

\subsection{Using {\tt git}}

Mostly, you will interact with your {\tt github} repo via the {\tt git} software, and your local copy of your {\tt github} repo. You will use {\tt git} by typing commands in a terminal window. To check everything is working before using {\tt git} for the first time you might open a terminal window and check the {\tt git} version, a follows:\index{git}
\begin{lstlisting}
git --version
\end{lstlisting}
This gives the answer \lstinline+git version 2.25.1+ for me. Here we will cover only the most basic use of {\tt git} and {\tt github}: the minimum needed to collaborate on projects and keep your work backed up. You can find much more at:
\begin{itemize}
\item \lstinline+https://git-scm.com/docs/gittutorial+
\item \lstinline+https://guides.github.com/+
\item \lstinline+https://training.github.com/+
\end{itemize}
Note that {\tt Rstudio} offers built in facilities for using {\tt github}, which you may find useful. However before using them it is better to first learn how {\tt git} and {\tt github} work by using them in the way described here, which is transferable to any project, not just R programming with {\tt Rstudio}.  

Before moving on, if you are using a Mac or linux you probably want to issue the command:
\begin{lstlisting}
git config --global core.autocrlf input
\end{lstlisting}
which deals with the fact that line ends are dealt with differently by different operating systems and handles this gracefully (Windows is set to do this by default).\index{git!line endings}

\subsubsection{Making a local working copy of the repo}

After creating your github repo you need to make a local copy of it on your computer. 
\begin{itemize}
\item From your {\tt github} repo page (at {\tt github.com}) click on {\tt Code} select {\tt Clone} and copy the {\tt htpps} address for the repository that appears. For example, for my repo containing these notes, the address is \lstinline+https://github.com/simonnwood/sp-notes.git+
\item In the terminal window on your local machine change directory ({\tt cd}) to the folder where you would like the local repo to be located. Then type {\tt git clone} followed by the address you copied. e.g. 
\begin{lstlisting}
git clone https://github.com/simonnwood/sp-notes.git
\end{lstlisting}
\end{itemize}
A local copy of the repo is then created on your computer. \index{git!local copy} \index{git!clone}

\subsection{Modifying work and synchronising with the github repo}

The local copy of your repo is just a directory/folder containing the files in your repo, and a hidden {\tt .git} subdirectory that {\tt git} uses to maintain an index of the files that it should keep track of, and the history of changes made. {\tt git} knows about all the files in the {\tt github} repo you cloned, but will only start keeping track of other files that you may add to your local repo when you tell it to do so (with \lstinline+git add+). 

Similarly {\tt git} does not keep a record of all the changes you make to your code as you work on it. Rather, it takes and stores a `snapshot' of the state of the files it is tracking only when you tell it to, using a {\tt git commit} command. \index{git!commit}

Neither does {\tt git} automatically save all these snapshots to your master {\tt github} repo. That only happens when you tell it to using {\tt git push}. This is quite useful, you can keep a detailed record of the changes you make while working on code locally, without modifying the {\tt github} master repo until you are satisfied that the changes are complete and working.   \index{git!push}

Note that you can access the help pages for the main {\tt git} commands by typing {\tt git help} followed by the command name. For example, if you want to know more about {\tt git add} type \index{git!help}
\begin{lstlisting}
git help add
\end{lstlisting}

\subsubsection{Simple work cycle}

Suppose we want to make some code changes in a file \lstinline+foo.r+ that already exists in the master repo. A typical work sequence would be as follows.
\begin{enumerate}
\item  Change directory ({\tt cd}) to the local repo.\index{git!pull} 
\item If collaborating with others, you might want to make sure that you local repo is up to date with the master copy on {\tt github}, using \lstinline+git pull+ to get (`pull') any changes from {\tt github}. (\lstinline+git diff @{upstream}+ can be used to view the changes first, if you prefer.) 
\item Work on {\tt foo.r} until you are happy with the changes made, and have tested them. 
\item Have {\tt git} take a snapshot of the changes made using something like 
\begin{lstlisting}
git commit -a -m"added a wibbleblaster to foo.r"
\end{lstlisting}  \index{git!commit}
The \lstinline+-a+ option tells {\tt git} to take a snapshot of every file that it is tracking that you have changed. If you omit \lstinline+-a+ then you must tell {\tt git} which files you want it to snapshot explicitly, for example using \lstinline+git add foo.r+, before committing. The option \lstinline+-m+ adds a message describing the changes. If you omit the \lstinline"-m" then {\tt git} will open an editor, in which you enter the message - this can be useful if you want to include a longer set of comments. These comments can be viewed on the {\tt github} repo, providing a useful record of what the individual code changes were for. \index{git!add} 
\item Now `push' the changes to {\tt github}. 
\begin{lstlisting}
git push
\end{lstlisting}\index{git!add}
If only you are working on the code, then you will be done at this point, but if others are working on it at the same time, then the {\tt push} command may fail because your new code conflicts with the code someone else has pushed to {\tt github} since you last pulled the repo. How to fix this is described next. 
\end{enumerate} 

Note that \lstinline+git log+ lets you view the commit history of your project (to review what has been done and why).
\index{git!log}

\subsubsection{Simple conflict resolution}

If your {\tt git push} command fails (and it will tell you if it has!) then you need to resolve the conflict. This is not so hard. 

\begin{enumerate}\index{git!resolve conflict}
\item After your {\tt git push} command has failed, issue the command
\begin{lstlisting}
git pull
\end{lstlisting}
This will get the latest versions of the files from the {\tt github} repo, compare them to your local copies, and attempt to resolve conflicts automatically where possible. Where auto-fixing is not possible, it will modify your local copies so that whenever there is a conflict your local files contain both your code and the conflicting code from the {\tt github} master, with clear marking of which is which. 
\item You check the conflicting files, changing the code to resolve any remaining flagged conflicts (often just selecting one or other of the alternative). 
\item Now redo the commit and push steps
\begin{lstlisting}
git commit -a -m"resolved conflict in favour of small end cracking"
git push
\end{lstlisting} \index{git!commit}\index{git!push}
If this fails because a teammate has meanwhile made another change, you really need to sort out your team communication - {\tt git} can't do that for you. 
\end{enumerate}

\subsubsection{Adding and deleting files with {\tt git}}

You can add as many files as you like to your local copy of the repo, but {\tt git} will take no notice of them until you tell it to. For example, suppose you created a file {\tt bar.r} which should be treated as part of the project, tracked and included in the master repo on {\tt qithub}
\begin{lstlisting}
git add bar.r
git commit -a -m"added file bar.r containing the gribbler code"
git push
\end{lstlisting}\index{git!add}
(You could omit the \lstinline+-a+ in this case, as the preceding \lstinline+add+ command will ensure \lstinline+bar.r+ is included in the next commit.)

You might also want to remove files, of course. 
\begin{lstlisting}
git rm bar.r
\end{lstlisting}
deletes {\tt bar.r} from the local copy, and will cause it to be removed from the master repo at the next {\tt commit} and {\tt push}. \index{git!delete file}

\subsection{More advanced use}

The above covers the most basic use of {\tt git}. It is sufficient for working on small projects in small teams on this course, and for understanding the basic principles of version control systems. We have not covered some key components of  
{\tt git} and {\tt github} that are extremely useful for larger projects. In particular we have not covered project {\em branches}. {\tt git} allows you to simultaneously have several versions of your project ({\em  branches}) in addition to the master version. These versions can all be tracked and backed up, just as the main master branch is. 

A typical use of branches is to code and work on complicated modifications - tracking and backing up the changes in those modifications, while not modifying the main project until is is clear that the modified code is really working and ready to be merged into the main project. 

To give a concrete example, suppose you maintain a large R package, which relies on code written in C and Fortran, and you decide that it would be advantageous to replace all the Fortran code with C code. This is a major undertaking, and you would not want to simply start work on the stable working code for the main package, since this will almost certainly break it initially. That would be a real nuisance if you then needed to deal with a minor bug in the original package, but had only the half completed unstable modified code available. Much better to create a branch of the project on which to develop the new C based code, only merging into the master branch, once everything in the revision is fully working.  
\index{git!branch}


\subsection{A simple {\tt git}/{\tt github} exercise}

At this stage, it is {\em essential} that you try out {\tt git} and {\tt github}. So before reading on try the following exercise. If you are not sure how to do any part, then refer back to the material above that you have just read.
\begin{enumerate}
\item Using a web browser, set yourself up a new repo on {\tt github}, and edit the default {\tt README.md} file on {\tt github} to say something interesting (making sure to save and commit the change).
\item Clone your {\tt github} repo to your local machine.
\item Add a file to your local repo, edit it and add it to the files tracked by {\tt git}.
\item Commit your edits and push the repo to {\tt github}.
\item Check that the repo on {\tt github} now contains your newly added file. 
\item As soon as you know someone else on the course to work with, try out sharing repositories, and resolving conflicts.
\end{enumerate}


\section{Programming for statistical data analysis}

Programming is the process of writing instructions to make a computer perform some task. The instructions have to be written in standard way that both the programmer and the computer can interpret. The rules and key words defining such a standard way of communicating define a computer language. There are many alternative languages designed for different classes of task. Here we will concentrate on the R language (and environment) for programming with data, which is widely used for the statistical analysis of data. A huge amount of statistical analysis software is written in R and is freely available. \index{R}

{\em Statistical} analysis of data is concerned with the analysis of data that are in some sense a random {\em sample} from a larger {\em population}. We want to learn about the population from the sample, without being misled by particular features of the sample that arose by chance as part of the random sampling process. The random sampling approach is powerful because: \index{statistical analysis}
\begin{enumerate}
\item it allows reliable conclusions with known levels of accuracy to be drawn without having to gather data from the whole population, which may be impossible, or prohibitively expensive.       
\item random sampling eliminates the unknowably large bias that occurs if we try to learn about the population from a non-random sample, replacing that bias with a random uncertainty of known magnitude.   
\end{enumerate}
Note that while `population' might mean something concrete like `population of people in the UK' it might be much more abstract like `the population of all experimental results that this experiment could have produced when replicated under the same conditions'. \index{random sample}\index{population}

To understand the difference between statistical and non-statistical data analysis consider data on reported cases of Covid-19 each day. Many charts of these data are produced, and analyses are performed, such as producing running averages or looking for trends in the data. None of these analyses are statistical, as no attempt is made to consider what population the case data might be viewed as a random sample of. They are certainly not a random sample of the people who have Covid-19 on a given day, and neither is it remotely clear how the number of cases relates to the number of people with Covid on a particular day.  The government and media present these data {\em as if} they were a random sample from the population of Covid cases, but this is simply misleading. Any decent applied statistician should be able to list several sources of bias likely to occur by treating them as such. 

In contrast, each week the UK Office for National Statistics publishes the results of testing a randomly selected sample of the UK population for Covid. The analysis includes estimates of trends and uncertainties, from a proper statistical analysis. Unsurprisingly the ONS analysis often appears to contradict the naive interpretations of the case data given by the media and government. More surprisingly the media and government seem to give more weight to the non-statistical analyses of case data than to the statistical analyses of the ONS data, despite the latter being essentially unbiased and of known accuracy, while the former have bias of unknown magnitude. Even more surprisingly, the UK appears to be unique in even conducting unbiased sampling to establish Covid prevalence.

The Covid cases example illustrates why statistical analysis software is dangerous. The easier software is to use, the easier it is to produce an analysis of data that resembles a statistical analysis in every superficial respect, but not in the key one that the data are in some sense a random sample from a population of interest.  Beware!

\subsection*{Exercise}

The following link is to an article in a national newspaper by two professors of statistics that appeared on 19th April 2020.
{\small \lstinline+https://www.theguardian.com/commentisfree/2020/apr/19/coronavirus-deaths-data-uk+} 
There is an astonishing statement in the second paragraph. Identify the statement and what is wrong with it.


\section{Getting started with R}

In this course we will concentrate on learning programming skills that as far as possible are transferable to other programming languages in addition to R. For this reason we will largely stick to programming using what is available with R itself, and we will avoid over-reliance on any particular set of add on packages. There is an add on package providing functions for almost any simple task you might want to accomplish in R. The fact that we will here examine how to programme tasks for which it would be simpler to find an add on package, is not through a desire to re-invent the wheel. The point is not to learn the quickest way to perform the particular task, but rather to illustrate how to programme. 

A word of warning. In much university work, getting something 80\% right is a first class performance. Unfortunately programming is not like that. 80\% right means 20\% wrong, and 20\% wrong computer code will likely result in your programme doing 0\% of what it is supposed to do. This fact calls for a more careful approach to working than is necessary for other topics.  

\subsection{A first R session \label{firstR}}

\index{R!basics}
To get started, let's do some trivial things in R. Start {\tt R} or {\tt Rstudio}, go to the terminal window and type
\begin{lstlisting}
a <- 2
\end{lstlisting}
You just created an object {\tt a} that contains the number 2. \lstinline+<-+ is the assignment operator. If you assign something to an object that does not yet exist, it is created. If you type the name of an object at the R terminal and nothing else, then the contents of the object gets printed. Exactly how this is done depends on the class of the object - more later. For example (\lstinline+>+ is just the R prompt, not something to type): \index{assignment operator}
\begin{lstlisting}
> a
[1] 2
\end{lstlisting}
We can make objects from other objects of course. For example
\begin{lstlisting}
> b <- 1/a
> b
[1] 0.5
\end{lstlisting}

R is a functional programming language: it is structured around functions that take objects as arguments and produce other objects as results. Many functions are built in to base R and even basic operators like \lstinline^+^ and \lstinline+/+ are actually implemented as functions. Suppose we want to create a function to take arguments $x$ and $y$ and return the value of $x \cos(y - k)$ where $k$ is a constant usually taking the value 0.5. Here is the code to define such a function object, named {\tt foo} in this case. \index{functions}
\begin{lstlisting}
foo <- function(x,y,k=0.5) {
  x * cos(y - k)
}
\end{lstlisting}
Curly brackets, \lstinline+{}+, enclose the R code defining how the function arguments are turned into its result. There can be as many lines of code as you like, but what ever is produced on the last line is taken as the object to be returned as the function result. \lstinline+k=0.5+ is used to indicate that if no value of {\tt k} is supplied, then it should take the default value of 0.5. This default system is used extensively by R and its add on packages. Let's try out {\tt foo}. \index{functions!argument}\index{functions!default argument}
\begin{lstlisting}
> foo(3,2)   ## using default k=0.5
[1] 0.2122116
> foo(3,2,0) ## setting k=0
[1] -1.248441
\end{lstlisting}

R is an interpreted language. Code\footnote{I'll use `code' to mean programmes and commands written in R (or indeed other computer languages).} is interpreted and executed line by line as it is encountered. This contrasts with compiled languages, where code is converted to binary instructions en masse, and this binary `machine code' (the native language of the computers processor) is then executed separately. R evaluates lines of code when they appear complete, and a line end has been encountered. If you want to split code over several lines then you need to be careful that the line does not appear complete before you meant it to be. e.g. suppose we want to evaluate $a=1+2+3+4+5$\index{interpreted language}
\begin{lstlisting}
> a <- 1 + 2 + 3 ## split line but it looks complete to R!
> + 4 + 5 
[1] 9
> a 
[1] 6  ## oops
> a <- 1 + 2 + 3 + ## split line that does not look complete
+ 4 + 5
> a
[1] 15 ## success
\end{lstlisting}
Several complete statements can be included on the same line by separating them with a `{\tt ;}'. e.g.
\begin{lstlisting}
a <- 2; b <- 1/a; d <- log(b)
\end{lstlisting}
Now leave R with the \lstinline+q()+ command. By default you will be asked if you want to save your workspace - usually you do not. You can avoid being asked by typing \lstinline+q("no")+. \index{quiting R}

\subsection{Dissecting a simple programming example}

Let us continue with an example of a simple data manipulation task in R. Suppose that we have a vector of 1 or 2 digit numbers and want to create a new vector of the individual digits, in order. For example if the original vector is $[12,5,23,2]$ the new vector should be $[1,2,5,2,3,2]$. Before trying this we need to write down {\em how} we are going to do it. For example:
\begin{enumerate}
\item Identify the number and locations of the double digit numbers in the original vector (and hence the length of the new vector).
\item Work out the locations of the `tens' digits in the new vector, compute and insert them.
\item Work out the locations of the `units' digits in the new vector and insert them. 
\end{enumerate}
The following is R code for one way of implementing this. It could be typed, line by line, into the R console, but it is better to type it into a file and then copy and paste to the R console, or {\tt source} the file into R, or run it in Rstudio. \index{which}\index{length}\index{rep}\index{integer division}\index{mod (integer remainder)}
\begin{lstlisting}
x <- c(10,2,7,89,43,1) ## an example vector to try out
ii <- which(x%/%10 > 0) ## indices of the double digits in x?
xs <- rep(0,length(ii)+length(x)) ## vector to store the single digits
iis <- ii+1:length(ii)-1 ## where should 10s digits go in xs?
xs[iis] <- x[ii]%/%10 ## insert 10s digits
xs[-iis] <- x%%10  ## insert the rest (units)
\end{lstlisting}  
Anything after \lstinline+#+ on a line is ignored by R, so is a comment. It is a good idea to use lots of these. Here is what the code does in detail, line by line.\index{concatenate}\index{TRUE}\index{FALSE}\index{index vector}
\begin{enumerate}
\item \lstinline+x <- c(10,2,7,89,43,1)+ creates an example vector to work on. The {\tt c} function takes the individual numbers supplied to it, and concatenates them into a single vector (it can also concatenate vectors). The results are stored in vector \lstinline+x+ using the assignment operator \lstinline+<-+. Notice how {\tt x} is created automatically by this assignment. Unlike in many computer languages, we do not have to declare {\tt x} first. 
\item \lstinline+ii <- which(x%/%10 > 0)+ creates a vector, {\tt ii}, of the indices of the double digit numbers in \lstinline+x+. It does this by computing the result of integer division by 10, for each element of {\tt x}, using the \lstinline+%/%+ operator, and then testing whether this result is greater than 0. The result of \lstinline+x%/%10 > 0+ will be a vector of {\tt TRUE} or {\tt FALSE} values of the same length as {\tt x}. For the given example it is $({\tt TRUE},{\tt FALSE},{\tt FALSE},{\tt TRUE},{\tt TRUE},{\tt FALSE})$. This vector is supplied directly to the {\tt which} function, which returns the indices for which the vector is {\tt TRUE} (1,4 and 5 for the example given). 
\item \lstinline^xs <- rep(0,length(ii)+length(x))^ creates a vector of zeroes into which the individual digits will be inserted, using the {\tt rep} function. {\tt rep} simply repeats its first argument the number of times specified in its second argument. The {\tt length} function returns the number of elements in an existing vector. We need {\tt xs} to be the length of {\tt x} plus an extra element for each double digit number.
\item \lstinline^iis <- ii+1:length(ii)-1^ creates a vector, {\tt iis}, containing the indices (locations) of the 10s digits in {\tt xs}. We have to account for the fact that each time we insert a digit, all the digits after move along one place, relative to where they were. So, the first 10s digit will occupy the same slot in {\tt x} and {\tt xs}, but the next 10s digit will be one element later in {\tt xs} than in {\tt x}, the next 2 elements later, and so on. \lstinline+1:length(ii)-1+ creates a sequence 0,1,2\ldots to add to {\tt ii} to achieve this. It uses the \lstinline+:+ operator --- if {\tt a} and {\tt b} are integers (and $a<=b$) \lstinline+a:b+ generates the sequence $a,a+1,a+2,\ldots,b$.\index{sequence}\index{vector!indexing}
\item \lstinline+xs[iis] <- x[ii]%/%10+ computes the 10s digits of the double digit numbers, indexed by {\tt ii}, using \lstinline+x[ii]%/%10+, and assigns them to the elements of {\tt xs} indexed by {\tt iis}.
\item \lstinline+xs[-iis] <- x%%10+ computes the units digit for all the numbers in {\tt x} using \lstinline+x%%10+ where \lstinline+%%+ is the operator computing the remainder after integer division. These digits are stored in the elements of {\tt xs} {\em not} reserved for 10s digits. That is, \lstinline+xs[-iis]+, all the elements {\em except} those indexed by {\tt iis}. 
\end{enumerate}
Make sure you really understand what this code is doing. Run it one line at a time in {\tt R}, and examine the result created by each line to make sure you understand exactly what is happening. To see what is in an R object, just type its name at the console, and it will be printed, by default. For example:
\begin{lstlisting}
> xs  ## > just denotes the R prompt here
[1] 1 0 2 7 8 9 4 3 1
\end{lstlisting}

The preceding example has quite a bit of R packed in, and we used quite a few R functions and operators on the way. R provides a large number of functions for a variety of tasks, and the available add on packages vastly more. You can't hope to learn them all, so it is essential to learn how to use the R help system. This is easy. For any operator of function you know the name of then just type \lstinline+?+ followed by the function name, or the operator in quotes. For example
\begin{lstlisting}
?which ## get the help for the 'which' function
?"<-"  ## get help for the assignment operator
help.start() ## launch html help in a browser
\end{lstlisting}  
\ldots the last option is often best if you are not sure what you are looking for. \index{R!help}

\subsection{A second simple example: data are not always numbers}

R vectors are not restricted to containing numbers. Character strings are another common data type that we can hold in a vector. For example \lstinline+x <- c("jane","bill","sue")+ creates a 3-vector, containing the 3 given character strings. Functions are provided for manipulating such character string data in various ways. For the moment suppose we want to achieve the same task as in the previous example, but for the case in which the numbers are supplied as character strings, and we want the separated digits as character strings too\footnote{As the point is to provide illustration of string handling, I'll resist the temptation to use {\tt as.numeric} to convert the character data to numbers, run the previous code, and then use {\tt as.character} to convert back.}.  \index{string}\index{character data}

We can use more or less the same approach as in the last section, but with one slight modification to the logic. For numbers it was easy to separate out 10s and units, with only the 2 digit numbers having a 10s digit. When the numbers are represented as character strings it is easier to separate out first and second digits, with only the two digit numbers having second digits. This means that rather than finding the indices of 10s digits, we'll find the indices of second digits. Here is the modified code: \index{nchar}\index{substr}\index{character string!substr}

\begin{lstlisting}
x <- c("10","2","7","89","43","1") ## example vector
ii <- which(nchar(x)>1) ## which elements of x are double digit?
xs <- rep("",length(ii)+length(x)) ## vector to store the single digits
iis <- ii+1:length(ii) ## where should second digit go in xs?
xs[iis] <- substr(x[ii],2,2) ## insert 2nd digits
xs[-iis] <- substr(x,1,1)    ## insert 1st digits
\end{lstlisting}
Line by line, here is what it does:
\begin{enumerate}
\item \lstinline+x <- c("10","2","7","89","43","1")+ example vector, as before, but now of character type.
\item \lstinline+ii <- which(nchar(x)>1)+ uses the {\tt nchar} function to count the characters in each element of {\tt x}.\\ \lstinline+which(nchar(x)>1)+ returns the indices for which the corresponding element of {\tt x} has $>1$ character.
\item  \lstinline^xs <- rep("",length(ii)+length(x))^ creates {\tt xs} as before, but this time it is a character vector.
\item \lstinline^iis <- ii+1:length(ii)^ computes the locations for second digits in {\tt xs}. Same idea as before, but second digits are all located one place after the 10s digits.
\item \lstinline+xs[iis] <- substr(x[ii],2,2)+. Function \lstinline+substr+ is used to obtain the 2nd character from each 2 digit element of {\tt x} ({\tt ii} indexes the 2 digit elements). \lstinline+substr(x[ii],2,2)+ extracts the characters between characters 2 and 2, from the elements of {\tt x[ii]} and returns them in a vector, which is copied into the appropriate locations in {\tt xs}.
\item \lstinline+xs[-iis] <- substr(x,1,1)+ the first digits are then inserted in the locations not reserved for second digits. 
\end{enumerate}

\subsubsection{Another simple text processing task}

The above example is a bit artificial. Let's consider a slightly more realistic task to undertake on character data. Suppose we have a string containing  some `poetry', and we want to count the number of words, tabulate the number of letters per word, count the number of words containing at least one `e' and mark all words containing an `a' and an `e' with a `*'. Here is R code to do this.\index{character string!nchar}\index{character string!strsplit}\index{strsplit}\index{character string!grep}\index{grep}\index{character string!paste}\index{paste}\index{character string!split}\index{character string!join}\index{character string!search}\index{tabulate}
\begin{lstlisting}
poem <- paste("Inside me is a skeleton, of this I have no doubt,",
        "now it's got my flesh on, but it's waiting to get out.")
pow <- strsplit(poem," ")[[1]] ## vector of poem words
n.words <- length(pow) ## number of words
freq <- tabulate(nchar(pow)) ## count frequency of n-letter words
ie <- grep("e",pow,fixed=TRUE) ## find `e' words
n.e <- length(ie)   ## number of `e' words
ia <- grep("a",pow,fixed=TRUE) ## find `a' words
iea <- ia[ia %in% ie] ## find words with `e' and `a'
pow[iea] <- paste(pow[iea],"*",sep="") ## mark `e' `a' words
paste(pow,collapse=" ") ## and put words back in one string.
\end{lstlisting}
Line by line it works like this:
\begin{enumerate}
\item The first line just creates a text string, {\tt poem}, containing the given text. The {\tt paste} function joins the two given strings into one string. The only reason to use it here was to split the string nicely across lines for these notes - we could just as well have written everything in one string to start with.
\item \lstinline+pow <- strsplit(poem," ")[[1]]+ splits {\tt poem} into a vector of its individual words, using\\ \lstinline+strsplit(poem," ")+, which splits the string in {\tt poem} at the breaks given by spaces, \lstinline+" "+. {\tt strsplit} can take a vector of strings as its first argument, and returns a {\em list} of vectors containing the split strings. In our case the list only has one element, which is what the \lstinline+[[1]]+ part of the code accesses. 
\item \lstinline+n.words <- length(pow)+ counts the words in {\tt poem}, since there is one element of {\tt pow} per word.
\item \lstinline+freq <- tabulate(nchar(pow))+ counts how many 1 letter, 2 letter, 3 letter, etc. words are in {\tt poem}. First function {\tt nchar} counts the letters in each word and then {\tt tabulate} tallies them up. Really we should have stripped out punctuation marks first - {\tt gsub} could be used to do this.
\item \lstinline+ie <- grep("e",pow,fixed=TRUE)+ finds the indices of the words containing an `e' using the {\tt grep} function. By default {\tt grep} can do much more complicated pattern matching using `regular expressions (see {\tt ?regex}). For the moment this is turned off using \lstinline+fixed=TRUE+ (otherwise characters like \lstinline+.+ and \lstinline+*+ are not matched as you might expect, but treated differently).
\item \lstinline+n.e <- length(ie)+ is the count of `e' words.
\item \lstinline+ia <- grep("a",pow,fixed=TRUE)+ finds the indices of the words containing an `a'.
\item \lstinline+iea <- ia[ia %in% ie]+ finds the indices of words containing an `a' and an `e'. \lstinline+ia %in% ie+ gives a {\tt TRUE} for each element of {\tt ia} that occurs in {\tt ie} and a {\tt FALSE} for each element of {\tt ia} that doesn't. Hence {\tt iea} will contain the indices of the words containing both letters.
\item \lstinline+pow[iea] <- paste(pow[iea],"*",sep="")+ adds a `*' to each word in {\tt pow} containing an `e' an an `a'. The {\tt paste} function is used for this, with \lstinline+sep=""+ indicating that no space is wanted between a word and `*'.
\item \lstinline+paste(pow,collapse=" ")+ is finally used to put the words in {\tt pow} back into a single string. It is the setting of {\tt collapse} to something non-NULL (here a space) that signals to {\tt paste} that this should happen.
\end{enumerate}

\subsection*{Exercises}

\begin{enumerate}
\item Look up the help page for {\tt gsub} function, and use {\tt gsub} to remove the commas and full stops from the poem before tabulating the length of words. 
\item For the original {\tt poem}, write R code to insert a line end character, \lstinline+"\n"+, after each comma or full stop, and print out the result using the {\tt cat} function.
\item Run the code \lstinline+set.seed(0);y <- rt(100,df=4)+. Use the {\tt hist} function to visualize {\tt y}. Now using the {\tt mean} and {\tt sd} functions to find the mean and standard deviation of {\tt y}, write code to remove any points in {\tt y} that are further than 2 standard deviations from the mean, and calculate the mean of the remainder.
\item Using your code from the previous section to write a function that will take any vector {\tt y} and compute its mean after removal of points more than {\tt k} standard deviations from the original mean of {\tt y}. Set the default value of {\tt k} to 2.

\end{enumerate}



\section{A slightly more systematic look at R}

When you start the {\tt R} programme, two important things are created. The first is an {\tt R} terminal, into which you can type commands written in the R programming language - this is visible. The second is an {\em environment}, known as the user workspace or global environment which will hold the objects created by your commands - this is invisible, but is there as an extendable piece of computer memory. In R an environment consists of a set of symbols used as the names of objects along with the data defining those objects (known together as a {\em frame}) and a pointer to an enclosing `parent' environment. R makes extensive use of sets of nested environments, but we don't need to go into too much detail on this aspect at the moment.

Like any computer language, the R language defines basic data structures, key words used to control programme flow, operators and functions, plus a set of rules about how these things are used and combined. This section introduces these. 

\subsection{Objects, classes and attributes}

Everything in R is an object living in an environment, including R commands themselves. Objects have {\em classes} which R can use to determine how the object should be handled (for example which version of the print function is appropriate for it).  Objects can also be given {\em attributes}: these are basically other objects that have been `stuck onto' the object and are carried around with it. A bit like a set of virtual post-it notes. Attributes are useful for storing information about an object.  For example, matrices in R have class \lstinline+"matrix"+ and a \lstinline+dim+ attribute. The \lstinline+dim+ attribute stores the number of rows and columns in the matrix. \index{R!objects} \index{matrix} \index{attributes}

\subsection{Data structures: vectors, arrays, lists and data frames}  

Let's look at the basic data structures you {\em must} know about to programme in R. When reading through this, try out the code yourself in R, and also try modifying it to check your understanding. 

\subsubsection{Vectors and recycling}

The most basic type of data structure in R is a vector (a one dimensional array). \lstinline+x[i]+ accesses element {\tt i} of vector {\tt x} ({\tt i} is a positive integer). Even scalars are just vectors of length 1 (so e.g. \lstinline+3[1]+ is perfectly valid and evaluates to 3, of course). As we have already seen, vectors can store data of different {\em types}: integer or real numbers (type `double'), character strings, logical variables. The class of vectors is simply determined by the type of thing they contain. Here are some basic examples. \index{R!class}\index{typeof}\index{data types}\index{vectors}
\begin{lstlisting}
> a3d <- c(TRUE,FALSE,FALSE) ## create an example logical vector
> class(a3d)     ## its class
[1] "logical"
> typeof(a3d)    ## its type
[1] "logical"
> a3d[2:3]       ## print its 2nd and 3rd elements 
[1] FALSE FALSE
> a3d[2] <- TRUE ## change the 2nd element
> a3d            ## print the resulting vector
[1]  TRUE  TRUE FALSE
> 
> bb <- 1:10  ## create numeric example
> class(bb)   ## check class
[1] "integer"
> bb[c(1,4,9)] <- c(.1,-3,2.2) ## change selected elements
> class(bb)   ## note automatic change of class 
[1] "numeric"
> typeof(bb)  ## how it is actually being stored
[1] "double"
> bb[3:8]     ## print elements 3:8 
[1]  3 -3  5  6  7  8
\end{lstlisting}
Whatever the type of a vector, some of its elements can always be set to {\tt NA} (not available), if required. Also numbers can take the values \verb+Inf+, \verb+-Inf+ and \verb+NaN+ (not a number e.g. $log(-1)$). \index{NA}\index{NaN}

Since vectors are the basic data structure, operators and most functions are also vectorized - they operate on whole vectors. For example given two vectors, {\tt a} and {\tt b} of the same length then \lstinline+c <- sin(a) * b+ actually forms \lstinline+c[i] <- sin(a[i]) * b[i]+ for all {\tt i} from 1 to the length of the vector. Similarly \lstinline^c <- a * b + 2^ actually forms \lstinline^c[i] <- a[i] * b[i] + 2^ for the same set of {\tt i} values. \index{vectorized code}

Notice how the scalar {\tt 2} got reused for each {\tt i} in that last example. So is a scalar more than just a length one vector after all? Actually no, {\tt 2} is being treated like any other vector --- R has a {\bf recycling rule} for vector arithmetic. Any operator that combines two vectors will recycle the values in the shorter vector as many times as required to match the length of the longer vector. So if a vector contains only one value, that one value is just recycled as often as needed. Here are a couple of examples  \index{recycling rule} \index{vectors}
\begin{lstlisting}
> a <- 1:4 # a 4-vector 
> b <- 5:6 # a 2-vector
> a*b      # multiplication with recycling
[1]  5 12 15 24
> b <- 5:7 # a 3 vector
> a*b      # multiplication with recycling
[1]  5 12 21 20
Warning message:
In a * b : longer object length is not a multiple of shorter object length
\end{lstlisting}

\subsubsection{Matrices and arrays}

While vectors are one dimensional arrays data, matrices are 2 dimensional arrays, and in general an array can have as many dimensions as we find useful. We can create an array with the {\tt array function}. For example:\index{array}
\begin{lstlisting}
a <- array(1:24,c(3,2,4))
\end{lstlisting}
creates a $3 \times 2 \times 4$ array, filling it with the numbers given in the first argument. To access the array we just give the dimension indices for the elements required. Leaving an index blank implies that we require all elements for that dimension. For example.
\begin{verbatim}
> a[3,2,3] ## element 3,2,3
[1] 18
> a[1:2,1,] 
     [,1] [,2] [,3] [,4]
[1,]    1    7   13   19
[2,]    2    8   14   20
\end{verbatim}  
notice how the second example accesses all elements for which the first index is 1 or 2, and the second index is 1.

Arrays are actually stored as vectors, with class \lstinline+"array"+ and a {\tt dim} attribute. The {\tt dim} attribute is a vector containing the length of each dimension (so $3,2,4$ above). Since the underlying storage is vector, we can also access it as such, we just need to know that the data are stored in the vector in `column major order'. For example if $d$ is the {\tt dim} attribute of a 3 dimensional array, $a$, then $a[i,j,k]$ is equivalent to $a[i + (j-1)d_1 + (k-1)d_1d_2]$. For example \index{array!vector access}
\begin{lstlisting}
> d <- dim(a) ## get the 'dim' attribute
> a[3+1*d[1]+2*d[1]*d[2]] ## vector access to a[3,2,3]
[1] 18
\end{lstlisting}
Notice that this is quite useful if you need to fill in or access several scattered individual elements of an array at once.

Two dimensional arrays, {\bf matrices}, play a central role in statistics. Data properly arranged for analysis are usually in matrix form with columns as variables and rows as observations (often referred to as `tidy data'), while many statistical methods rely heavily on matrix computations. Hence matrices are treated as a special class of array, with their own \lstinline+"matrix"+ class, a {\tt matrix} function used to create them, special operators for matrix multiplication and other matrix products, and functions implementing many matrix decomposition and matrix equation solving tasks. We will cover this in more detail later, but here is a quick example, which uses the matrix multiplication operator, \lstinline+%*%+.\index{matrix}\index{matrix!multiplication}\index{tidy data}
\begin{verbatim}
B <- matrix(1:6,2,3); B ## create a matrix (filled by col)
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6
> B[1,2] <- -1            ## change element 1,2
> a <- c(.3,-1.2,2.3)    ## a 3-vector
> B %*% a                 ## matrix multiplication
     [,1]
[1,] 13.0
[2,]  9.6
> B*a                     ## element wise multiplication with recycling!!
     [,1] [,2] [,3]
[1,]  0.3 -2.3 -6.0
[2,] -2.4  1.2 13.8
\end{verbatim}   
Do make sure you {\em really} understand the difference between the last two commands! It {\em really} matters.

\subsubsection*{Exercise}

Run the code 
\begin{lstlisting}
set.seed(5); n <- 2000
w <- runif(n)
A <- matrix(runif(n*n),n,n)
system.time(B <- diag(w) %*% A )
\end{lstlisting}
noting how long the last line takes to run. \lstinline+diag(w)+ forms the diagonal matrix with leading diagonal elements given by the elements of {\tt w} (try an $n=5$ example if that's not clear). By considering what actually happens when a diagonal matrix multiplies another matrix, and how matrices are stored in R, find a way of calculating {\tt B} using the recycling rule without using {\tt diag} or \lstinline+%*%+. Time this new code, and check that it gives the same answer as the old code (the {\tt range} function might be useful).


\subsubsection{Lists}

Lists are the basic building blocks of all sorts of complicated objects in R. Each item of a list can be any sort of R object, including another list, and each item can be named. Suppose {\tt a} is a list. Individual items can be accessed by number using \lstinline+a[[i]]+ where {\tt i} is an integer. If an item has a name, \lstinline+"foo"+ for example, it can also be retrieved that way using \lstinline+a[["foo"]]+ or \lstinline+a$foo+. We can also access sublists, also by name or number. e.g. \lstinline+a[c(2,4)]+ or \lstinline+a[c("foo","bar")]+ both produce 2 item lists. Note the difference: \lstinline+[[]]+ retrieves the item, \lstinline+[]+ retrieves a list of the required items (even if there is only one). Here is a simple example, which uses function {\tt list} to create an example list.\index{list}
\begin{lstlisting}
> stuff <- list(a=1:6,txt="furin cleavage site",l2 = function(x) log(x^2),
+          more = list(a="er",b=42))
> stuff[[1]]
[1] 1 2 3 4 5 6
> stuff[["l2"]]
function(x) log(x^2)
> stuff$a
[1] 1 2 3 4 5 6
> stuff[c(1,2)]
$a
[1] 1 2 3 4 5 6
$txt
[1] "furin cleavage site"
\end{lstlisting}  

\subsubsection{Data frames and factors -- statistical data structures}

R provides two types of data structure that are particularly useful for statistics: {\bf factor} variables are a special class of variables especially useful for data that consist of labels that serve to classify other data; {\bf data frames} are 2 dimensional arrays of data where the columns are variables, {\em which can be of different types}, and the rows are observations. 

{\bf Factors} are vectors of labels. For example a clinical trial might record the {\tt sex}, {\tt nationality} and {\tt treatment} received for each subject. All three are labels that categorize the subject. The different values that the label can take are known as {\em levels} of the factor (although they usually have no ordering, and it they do it is generally ignored). For example in a multi-centre vaccine trial {\tt nationality} might have levels {\tt "GB"}, {\tt "Brazil"}, {\tt "USA"}. In R factors are of class \lstinline+"factor"+ and the {\tt levels} function can be used to access their \lstinline+"levels"+ attribute. In fact the actual labels of a factor variable are only stored in the \lstinline+"levels"+ attribute, the variable itself is a set of integers indexing the levels. Why all this fuss? Because factor variables are very useful in statistical modelling, and setting them up this way makes them easy for modelling functions to handle. Here is a simple example.\index{data frame} \index{factor}
\begin{lstlisting}
> fac <- factor(c("fred","sue","sue","bill","fred"))
> class(fac)
[1] "factor"
> fac         ## default printing for class factor 
[1] fred sue  sue  bill fred
Levels: bill fred sue
> levels(fac) ## extract the levels attribute
[1] "bill" "fred" "sue" 
> as.numeric(fac)  ## look at the underlying coding
[1] 2 3 3 1 2
\end{lstlisting}

{\bf Data Frames} are basically matrices, in which the columns have names and can have different types (numeric, logical, factor, character, etc). They can be accessed like matrices, or like lists. They provide the best way of organising data for many types of statistical analysis. Here is a basic example 
\begin{lstlisting}
> dat <- data.frame(y = c(.3,.7,1.2),x = 1:3,fac = factor(c("a","b","a")))
> dat      ## a data.frame
    y x fac
1 0.3 1   a
2 0.7 2   b
3 1.2 3   a
> dim(dat) ## like a matrix
[1] 3 3
> dat$fac  ## and like a list!
[1] a b a
Levels: a b
\end{lstlisting}

\subsection{Attributes}

As already mentioned, attributes are simply R objects attached to other objects, and carried around with them (a bit like a set of `post it' notes). They can be useful for storing things that are somehow properties of an object. We can find out about an objects attributes using the \lstinline+attributes+ function. For example
\begin{lstlisting}
> attributes(PlantGrowth) ## PlantGrowth is an R data set
$names
[1] "weight" "group" 

$class
[1] "data.frame"

$row.names
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
[26] 26 27 28 29 30

> A <- matrix(1:6,2,3) ## create a matrix
> attributes(A)        ## check its attributes
$dim
[1] 2 3
\end{lstlisting}
Single attributes can be queried, reset or created using the \lstinline+attr+ function. For example.
\begin{lstlisting}
> attr(A,"dim") ## query attibute
[1] 2 3
> attr(A,"foo") <- list(1:3,"fred") ## create or reset attribute
\end{lstlisting}
Attributes can be quite useful when your programming task requires an object that modifies a standard object (e.g. a matrix) by adding some extra information to it, but you still want all the standard functions that work with the object to function as for the standard object (e.g. matrix decompositions).    

\subsubsection{str and object structure}

Before moving on from data structures it is worth mentioning one useful function: {\tt str} prints a summary of the structure of any R object. This is a good way of seeing what an object actually looks like, since many classes of object have their own {\tt print} function, so that only rather limited information is printed when you simply type the object name at the command line. For example \index{str, structure}
\begin{lstlisting}
> str(dat)
'data.frame':	3 obs. of  3 variables:
 $ y  : num  0.3 0.7 1.2
 $ x  : int  1 2 3
 $ fac: Factor w/ 2 levels "a","b": 1 2 1
\end{lstlisting}

\subsection{Operators} 

Here is a boring table of some operators in R.

\begin{tabular}{cl|cl|cl}
Operator & Description& Operator & Description & Operator & Description\\ \hline
\verb+&+ & logical AND & \verb+|+ & logical OR & \verb+<+ & less than\\
\verb+<=+ & $\le$ &\verb+>+ & greater than & \verb+>=+ & $\ge$\\
\verb+==+ & testing equality & != & not equal & \verb+!+ & logical negation\\
\verb+<-+& assignment & \verb+=+ & assignment & \verb^+^& addition \\
\verb+-+ & subtraction & \verb+*+ & multiplication & \verb+/+ & division\\
\verb+^+ & raise to power & \verb+%/%+ & integer division & \verb+%%+& integer remainder (mod)\\
\verb+%*%+ & matrix multiplication & \verb+%x%+ & Kronecker product & \verb+%in%+ &logical match\\
\hline
\end{tabular}

\bigskip

\noindent These operators act element-wise on vectors, except for the final row, which are matrix operators, or in the case of \verb+%in%+ operates on vectors of different lengths (see \verb+?"%in%"+)  In addition \verb+||+ and \verb+&&+ are logical OR and AND operators that only evaluate the first element of their arguments, returning a single {\tt TRUE} or {\tt FALSE}. 

For example suppose we want to set all elements of vector {\tt x} to zero, for which $x[i]$ is between 1 and 2, or less than -2. \lstinline+x[(x < 2 & x > 1) | x < -2] <- 0+ does it. 

\subsection{Loops and conditional execution}

Many programming tasks require that some operation is repeated many times for different values of some index, or require that the choice of which operation to carry out should depend on some conditions.

Let's start with conditional execution of code. Often this is coded in an implicit vectorized way. For example \lstinline+x[x<0] <- 0+ finds all the elements of \lstinline+x+ that are less than 0 and sets them to zero. i.e. we have set \lstinline+x[i]+ to zero only if originally \lstinline+x[i]+ was less than zero. But sometimes we need a more explicit way of doing this: {\tt if} and {\tt else} are used to achieve this. The basic form is \index{if, else} 
\begin{lstlisting}
if (logical condition) {
  ## one version of code
} else {
  ## another version of code
}
\end{lstlisting}  
Leaving out the {\tt else} means that nothing is done if the {\tt condition} is {\tt FALSE}. For example we could simulate a coin toss:\index{runif}
\begin{lstlisting}
if (runif(1)>.5) cat("heads\n") else cat("tails\n")
\end{lstlisting}
{\tt runif} generates uniform random numbers on $(0,1)$ and {\tt cat} is a simple function for printing (\lstinline+"\n"+ produces a line end). In R you will often see code with the following sort of structure\index{cat}\index{print}
\begin{lstlisting}
a <- if (a<0) 0 else a + 1
\end{lstlisting}
i.e. what is assigned to an object depends on a logical condition. {\tt if} statements can also be chained together of course. Here is a pointless example:
\begin{lstlisting}
if (runif(1)>.5) {
  cat("heads\n")
} else if (runif(1)>.7) {
  cat("tails\n")
} else cat("also tails\n")
\end{lstlisting}



Let's move on to looping. The {\tt for} loop is the most commonly used example. It repeats a set of R commands once for each element of some vector. The basic syntax is \index{for loop}
\begin{lstlisting}
for (a in vec) {
  ## some R code goes here
}
\end{lstlisting} 
which repeats the code between the brackets\footnote{you can drop the brackets if there is only one statement to execute at each iteration.}, for {\tt a} set to each value in {\tt vec} in turn. Here's a simple example\footnote{for which a loop is in no way needed: you should be able to replace this with a single call to {\tt cat}.}
\begin{lstlisting}
> vec <- c("I","am","bored")
> for (a in vec) cat(a," ")
I  am  bored
\end{lstlisting}
Perhaps the most common use of a {\tt for} loop is to loop over all integers between some limits. For example \lstinline+for (i in 1:10) {...}+ evaluates all the commands in \lstinline+{...}+ for $i=1,2,\ldots,10$. (Take care if you have programmed in other languages -- in R \lstinline+for (i in 1:0) {...}+ {\em will} execute the loop for 1 and then for 0). Here is an example iterating a chaotic map:
\begin{lstlisting}
n <- 100;p <- rep(NA,n); p[1] <- 0.1
for (i in 2:n) p[i] <- 3.7 * p[i-1] * (1 - p[i-1]) 
plot(1:n,p,type="l",xlab="i")
\end{lstlisting}
\eps{-90}{.6}{chaos.eps}

\noindent What if I wanted to stop the above loop if \lstinline+p[i]+ exceeded 0.92? I could do that by using the {\tt break} command inside the loop, to break out if the condition was met. i.e. \index{for loop!break}
\begin{lstlisting}
for (i in 2:n) { 
  p[i] <- 3.7 * p[i-1] * (1 - p[i-1])
  if (p[i]>.92) break
}   
\end{lstlisting}
Occasionally you might want to {\tt repeat} indefinitely until a condition is met that causes you to {\tt break}. The basic structure is this \index{repeat loop}
\begin{lstlisting}
repeat {
  some code
  if (condition) break
}
\end{lstlisting}
Another possibility is a {\tt while} loop, something like \index{while loop}
\begin{lstlisting}
while (condition) {
  R code
}
\end{lstlisting}
but {\tt for} loops are the most commonly used.

Note that for a vector oriented language like R, these sorts of tasks can often be accomplished efficiently by exploiting the fact that vectorized operations loop over vectors automatically, while vectors can also be subsetted so that we act only on the parts meeting some condition. Vectorized operations are usually much faster than explicitly coded loops, so generally it is a good idea to only write explicit loops when vectorization is not possible, or if the operations to be conducted at each loop iteration are expensive enough that the overhead of explicit looping is unimportant. For example, you should never write \lstinline+for (i in 1:length(x)) y[i] <- x[i]+ since \lstinline+y <- x+ does the same thing more efficiently, with less code:
\begin{lstlisting}
> n <- 10000000; x <- runif(n) ## simulate 10 million random numbers 
> system.time(y <- x)   ## time a vector copy
   user  system elapsed 
  0.000   0.000   0.001 
> system.time(for (i in 1:n) y[i] <- x[i]) ## time the equivalent copy loop
   user  system elapsed 
  0.538   0.028   0.567
\end{lstlisting}
In an interpreted language like R, there is more interpreting what to do than actual doing in the loop case. 
\index{vectorized code}
\subsection{Functions}

Functions were introduced in Section \ref{firstR}, but some more detail is required to write them effectively.  Formally a function consists of an argument list, a body (the code defining what it does), and an environment (which is the environment where it was created). Generally, functions take objects as arguments and manipulate them to produce an object, which is returned. There are two caveats to this general principle. \index{functions}
\begin{enumerate}
\item A function may have side effects, such as printing some output to the console or producing a plot. Indeed a function may only produce a side effect, and no return object. Generally side effects that modify objects that are external to the function are to be avoided, if code is to be clean and easy to debug. 
\item A function may make use of objects not in its argument list: if R encounters a symbol not in the function argument list and not previously created within the function, then it searches for it, first in the environment in which the function was {\em defined}\footnote{This is known as `lexical scoping', because the parent environment of the \index{R!lexical scoping} function is where it was written down.} (which is not necessarily the environment from which it was called). If that fails it looks in the environments returned by function {\lstinline+search()+}. A benign use of this mechanism is to call other functions not in a function's argument list, or to access constants such as those stored in {\lstinline+.Machine+}. Using this mechanism to provide a function with other objects that you have created is generally bad practice, because it makes for complex hard-to-debug code. Generally all objects that a function needs should be provided as its arguments. If this gets unwieldy, then group the arguments into a smaller number of list arguments.   
\end{enumerate}

Here is an example of a function definition. It generalises one-to-one real functions with power series representations to symmetric matrices using the following idea. The eigen-decomposition of symmetric matrix ${\bf A}$ is ${\bf A} = {\bf U}{\bm \Lambda}{\bf U}\ts$ where the columns of ${\bf U}$ are eigenvectors of $\bf A$ and ${\bm \Lambda}$ is the diagonal matrix of eigenvalues. The generalization of a function, $f$ is then $f({\bf A}) = {\bf U} f({\bm \Lambda}){\bf U}\ts$, where $f$ is applied element wise to $\bm \Lambda$'s diagonal. \index{functions!definition}\index{functions!argument}\index{eigen}\index{matrix!multiplication}
\begin{lstlisting}
mat.fun <- function(A,fun=I) {
  ea <- eigen(A,symmetric=TRUE)
  ea$vectors %*% (fun(ea$values)*t(ea$vectors)) ## note use of re-cycling rule!
}
\end{lstlisting}\index{recycling rule}
`{\lstinline+function(A,fun=I)+}' indicates that a function is to be created with arguments {\lstinline+A+} and {\lstinline+fun+}. In this case the function created is given the name {\lstinline+mat.fun+}, but functions are sometimes used without being given a name (for example, in the arguments to other functions). The argument list gives the names by which the function arguments will be referred to within the function body. Arguments may be given default values to be used in the event that the function is called without providing a value for that argument. This is done using \lstinline$name = default$ in the argument list. \lstinline+fun=I+ is an example of this, setting the default value of {\lstinline+fun+} to the identity function. \index{R!functions!argument list}

Next comes the body of the function given by the R expressions within the curly brackets \lstinline+{ ... }+ (if the function body consists of a single expression, then the brackets are not needed). The function body can contain any valid R expressions. The object created on the last line of the function body is the object returned by the function. Alternatively the object can be returned explicitly using the {\lstinline+return+} function. For \lstinline+mat.fun+, the eigen decomposition of the first argument is obtained and then used to produce the generalised version of {\lstinline+fun+}. \index{R!functions!body}

Now let us use the function, with a random matrix. First a sanity check that the identity function is correct:
\begin{lstlisting}
> set.seed(1)
> m <- 3; B <- crossprod(matrix(runif(m*m),m,m)) ## example matrix
> range(B - mat.fun(B)) ## check input matches output
[1] -2.220446e-16  6.661338e-16
\end{lstlisting}
This confirms that the output matches the first argument when the default identity function is used.

An aside: \index{precision!finite} why was the difference between the input and output not exactly 0? Because real numbers can only be stored in a computer to a finite precision, here equivalent to about 16 places of decimals. This inevitably means that arithmetic computations on real numbers are not exact --- rounding errors accumulate as calculations are performed. \index{rounding errors} A great deal of mathematical work has gone into minimising these numerical errors in matrix computations, but they can not be eliminated. \index{precision!machine} In this case we can say that the input and output matrices are identical {\em to machine precision}. You can get an idea of the size of number that counts as indistinguishable from zero by typing \lstinline+.Machine$double.eps+. Note, however that what counts as machine zero is relative to the size of numbers involved in a calculation. For example, here is what happens if {\tt B} is multiplied by $10^{10}$
\begin{lstlisting}
> B <- B * 1e10
> range(B - mat.fun(B))
[1] -3.814697e-06  3.814697e-06
\end{lstlisting}

Back to functions! What actually happened when the function was called (by \lstinline+mat.fun(B)+). R first matches the arguments of the function to those actually supplied, adopting a rather permissive approach to so doing. First it matches on the basis of exact matches to argument names (`{\lstinline+A+}' and `{\lstinline+fun+}' in the example). This does not mean that R is looking for {\lstinline+B+} to be called {\lstinline+A+} in the example; rather it is looking for statements of the form \lstinline+A=B+, specifying unambiguously that object {\lstinline+B+} is to be taken as argument `{\lstinline+A+}' of {\lstinline+mat.fun+}. After exact matching, R next tries partial matching of names on the remaining arguments; for example \lstinline+mat.fun(B,fu=sqrt)+ would cause the {\lstinline+sqrt+} function to be taken as the object to be used as argument {\lstinline+fun+}. After matching by name, the remaining arguments are matched by position in the argument list: this is how R has actually matched {\lstinline+B+} to {\lstinline+A+} earlier. Any unmatched argument is matched to its default value. \index{functions!argument matching}\index{functions!partial matching}

R next creates an {\em evaluation frame}: an extendible piece of memory in which to store copies of the function arguments used in the function, as well as the other objects created in the function. This evaluation frame has the environment of the function as its parent (which is the environment where the function was defined, remember). \index{functions!evaluation frame}

Having matched the arguments, R does not actually evaluate them immediately, but waits until they are needed to evaluate something in the function body: this is known as {\em lazy evaluation}. Evaluation of arguments takes place in the environment from which the function was called, except for arguments matched to their default values, which are evaluated in the function's own evaluation frame. \index{evaluation!lazy}

Preliminaries over, R then evaluates the commands in the function body, and returns a result. 

Notice that arguments are effectively copied into the function's evaluation frame, so nothing that is done to a function argument within the function has any effect on the object that supplied that argument `outside' the function. Within the body of {\lstinline+mat.mod+} argument {\lstinline+A+} could have been replaced by some poetry, but the matrix {\lstinline+B+} would have remained unaltered.

Here is an example of calling {\lstinline+mat.mod+} to find a matrix inverse:
\begin{lstlisting}
> mat.fun(A = B, fun = function(x) 1/x)
          [,1]      [,2]      [,3]
[1,] 10.108591 -2.164337 -3.070143
[2,] -2.164337  4.192241 -2.707381
[3,] -3.070143 -2.707381  4.548381
\end{lstlisting}
In this case both arguments were supplied by their full name, and a function definition was used to supply argument {\lstinline+fun+}. 

\subsection{The `{\tt \ldots}' argument}

Functions can also have a special argument `\lstinline$...$', which is used to create functions that can have variable numbers of arguments. It is also used to pass arguments to a function that may in turn be passed on to other functions, without those arguments having to be declared as arguments of the calling function: this is useful for passing arguments that control settings of plotting functions, for example. \index{functions!variable number of arguments}
\index{functions!\ldots argument}

Any arguments supplied in the call to a function, that are not in the argument list in the function definition, are matched to its `\lstinline$...$' argument, if it has one.\footnote{This has the slightly unfortunate side effect that mistyped argument names do not generate obvious warnings.} The elements of `\lstinline$...$' can be extracted into a list, to work with, but here we will just consider how to use `\lstinline$...$' to pass arguments to a function called inside another function {\em without having to know the names of the arguments in advance}. 

To make the problem concrete, suppose that we want to use our \lstinline$mat.fun$ function with the function argument defined by 
\begin{lstlisting}
foo <- function(x,a,b) x/a + b
\end{lstlisting}
Obviously {\tt foo} can not be used directly with \lstinline$mat.fun$ as the code assumes that whatever is passed in as {\tt fun} only has one argument. We certainly don't want to write a new version of \lstinline$mat.fun$ just for 3 argument functions with arguments called {\tt a} and {\tt b}. That's where `\lstinline$...$' comes to the rescue: we'll use it to pass {\tt a} and {\tt b}, or any other extra arguments (or none) to {\tt fun}, as follows
\begin{lstlisting}
mat.fun <- function(A,fun=I,...) { ## ... allows passing of extra arguments
  ea <- eigen(A,symmetric=TRUE)
  ea$vectors %*% (fun(ea$values,...)*t(ea$vectors)) ## to be passed to fun
}
\end{lstlisting}
And it works\ldots
\begin{lstlisting}
mat.fun(B,fun=foo,a=2,b=3)
           [,1]       [,2]       [,3]
[1,] 2685660117 4154166277 4285541075
[2,] 4154166277 8363105089 7782109902
[3,] 4285541075 7782109902 8624247833
\end{lstlisting}

One irritation is worth being aware of. 
\begin{lstlisting}
ff <- function(res=1,...) res;f(r=2)
\end{lstlisting}
will return the answer 2 as a result of partial matching of argument names, even if you meant \lstinline+r+ to be part of the `\lstinline+...+' argument. It is easy to be caught out by this. If you want `\lstinline+...+' to be matched first, then it has to precede the arguments it might be confused with. So the following gives the answer 1:
\begin{lstlisting}
ff <- function(...,res=1) res;f(r=2)
\end{lstlisting}

\subsection{Planning and coding example: plotting an empirical CDF}

The cumulative distribution function, $F$, of a random variable $X$ is defined as $F(x) = \text{Pr}(X\le x)$. Hence if we have a random sample $x_1, x_2, \ldots, x_n$ of observations of $X$ we can define an {\em empirical} cumulative distribution function
$$
\hat F(x) = \frac{1}{n}\sum_{i=1}^n \mathbb{I} (x_i \le x)
$$
where $\mathbb{I}$ is the indicator function. Suppose we want to write a function to take a sample of data in a vector $\bf x$ and plot its CDF. There are many ways to do this, so the first thing we have to do is to write down a plan. Usually this will involve jotting down ideas and maybe sketches on a piece of paper. Here are my notes for this task\ldots\index{CDF}\index{planning}

\eps{0}{.7}{cdf-plan.pdf}

The sketch of the CDF  is me trying to get my head around what the definition of $\hat F$ really means, and whether the $x_i$ values are at the start of a step or just before it (the former). As a result of this figuring out, I get to a plan that is a bit better than just evaluating the formula for $\hat F$ for a finely spaced sequence of $x$ values. Obviously more complicated tasks may involve several sheets of paper, false starts, and the need to summarize the design at the end. Code that involves no sheets of paper and none of this planning process is often poorly designed, error prone and slow to write. 

To implement the design I used built in functions, {\tt sort}, {\tt plot} and {\tt lines}. Look up the help pages for these. {\tt plot} is a high level function for producing scatter plots, while {\tt lines} is a lower level function for adding lines to an existing plot ({\tt points} is an equivalent for adding points). For a little extra flourish I used {\tt expression} for defining the y axis label --- see {\tt ?plotmath} for how that works. Here is the code -- makes sure you understand each line. \index{plot}\index{sort}\index{lines}\index{points}
\begin{lstlisting}
ecdf <- function(x) {
## function to plot empirical cdf of sample x
  n <- length(x)
  p <- 1:n/n ## create cumulative probability vector 
  xs <- sort(x) ## sort the x values
  ## plot the x, p points
  plot(xs,p,pch=19,cex=.5,xlab="x",ylab=expression(P(X<=x)))
  ## now add in the lines of constant probability between x values
  lines(c(2*xs[1]-xs[n],xs[1]),c(0,0))
  for (i in 1:(n-1)) lines(c(xs[i],xs[i+1]),c(p[i],p[i]))
  lines(c(2*xs[n]-xs[1],xs[n]),c(1,1))
} ## ecdf

## test it
set.seed(8);
x <- rgamma(50,shape=3,scale=2) ## test data
ecdf(x)
\end{lstlisting}
\eps{-90}{.4}{ecdf.eps}
\subsubsection{Avoiding the loop and vectorization}
You will notice that {\tt ecdf} uses a {\tt for} loop for plotting each line segment in turn, but actually if you check the help page {\tt ?lines} you will see that {\tt lines} has a mechanism for plotting multiple separate line segments all at once - we just separate the co-ordinates defining the lines with an {\tt NA} (not available) to indicate that the points either side of the {\tt NA} should not be joined by a line. Since vectorized code is usually faster than loop code, lets think about how to uses this feature.
\begin{enumerate}
\item For $n$ data we have $n+1$ line segments, each defined by 2 $x,y$ points, and between each line segment we will need {\tt NA} values ($n$ in total). 
\item So let's create vectors, {\tt p1} and {\tt x1} each of length $3n+2$ to contain the line segment definitions and breaks, and fill them in.
\end{enumerate}
Here is the code that could replace everything after \lstinline+## plot the x, p points+ in {\tt ecdf}.
\begin{lstlisting}
p1 <- x1 <- rep(NA,3*n+2) ## create p1 and x1 with NAs everywhere
p1[1:n*3+2] <- p1[1:n*3+1] <- p ## fill in the step heights
x1[1:n*3+1] <- xs ## step start
x1[1:n*3+2] <- c(xs[-1],2*xs[n]-xs[1]) ## step ends
p1[1:2] <- 0 ## initial prob is zero
x1[1:2] <- c(2*xs[1]-xs[n],xs[1]) ## x co-ords for zero prob
plot(xs,p,pch=19,cex=.5,xlab="x",ylab=expression(P(X<=x)))
lines(x1,p1)
\end{lstlisting}\index{vectorized code}
Notice how this code is entirely vectorized --- no repeating of code $n$ times in a loop. It is therefore faster to run. But also notice that it is slower to read and understand. Also, with 500 data points the vectorized version of {\tt ecdf} is about 50 times faster than the original version. But it still takes only 0.7 seconds to run on my machine. So was it worth the effort of speeding up? That really depends on how often the function will be used and on what size data set. These are common trade-offs, which mean that you should beware of making a fetish out of vectorizing your code. Always vectorize if it is easy to write and understand the vectorized code, but otherwise think about whether the computer time savings will justify the effort.  

\subsection{Useful built-in functions}

R has a huge number of useful built in functions. This section is about how to find them.  Recall that R's extensive help system can be accessed by typing {\lstinline+help.start()+} at the command prompt, to obtain help in navigable HTML form, or by typing {\lstinline+?foo+} at the command line, where {\lstinline+foo+} is the function or other topic of interest. \index{functions!built in to R}

\begin{center}
\begin{tabular}{ll}
{Help topic} & {Subject covered} \\ \hline
{\tt ?Arithmetic} & Standard arithmetic operators \\
{\tt ?Logic} & Standard logical operators \\
{\tt ?sqrt} & Square root and absolute value functions \\
{\tt ?Trig} & Trigonometric functions ({\lstinline+sin+}, {\lstinline+cos+}, etc.) \\
{\tt ?Hyperbolic} & Hyperbolic functions ({\lstinline+tanh+}, etc.) \\
{\tt ?Special} & Special mathematical functions ($\Gamma$ function, etc.)\\
{\tt ?pgamma} & Partial gamma function\\
{\tt ?Bessel} & Bessel functions\\
{\tt ?log} & Logarithmic functions\\
{\tt ?max} & Maximum, minimum and vectorised versions\\
{\tt ?round} & Rounding, truncating, etc.\\
{\tt ?distributions} & Statistical distributions built into R\\ \hline 
\end{tabular} 
\end{center}

The {\lstinline+?distributions+} topic requires some more explanation. R has built-in functions for the {\lstinline+beta+}, {\lstinline+binomial+}, {\lstinline+cauchy+}, {\lstinline+chisq+}uared, {\lstinline+exp+}onential, {\lstinline+f+}, {\lstinline+gamma+}, {\lstinline+geom+}etric, {\lstinline+hyper+}geometric, {\lstinline+lnorm+}al (log-normal), {\lstinline+multinom+}ial, {\lstinline+nbinomial+} (negative binomial), {\lstinline+norm+}al, {\lstinline+pois+}son, {\lstinline+t+}, {\lstinline+unif+}orm and {\lstinline+weibull+} distributions. The R identifying names for these are shown in {\lstinline+courier+} font in this list. \index{R!distributions}\index{R!random numbers}

For each distribution, with name {\lstinline+dist+}, say, there are four functions:\index{random deviates}
\begin{enumerate}
\item {\lstinline+ddist+} is the probability (density) function of {\lstinline+dist+}.
\item {\lstinline+pdist+} is the cumulative distribution functions of {\lstinline+dist+}.
\item {\lstinline+qdist+} is the quantile function of {\lstinline+dist+}.
\item {\lstinline+rdist+} generates independent pseudorandom deviates from {\lstinline+dist+}. 
\end{enumerate}

\section{Simulations I: stochastic models and simulation studies}

One major class of statistical programming task involves {\em stochastic simulation}. Simulating processes that involve random numbers. Strictly speaking {\em pseudorandom} numbers. There are several main sorts of simulation that we might need to undertake.
\begin{enumerate}
\item Simulate from a stochastic model. e.g. a model of disease spread, or a model of football teams competing, or a model of how shoppers will move around a store.
\item Simulate the process of sampling data from a population, and analysing it using a statistical method, in order to assess how well the statistical method is working. 
\item Simulate the process of sampling data directly, as part of a statistical analysis moethod.
\item Simulate from the posterior distribution of the parameters in a Bayesian data analysis.
\end{enumerate}  
This section will take a look at examples of the first two. 

\subsection{Random sampling building blocks}

There are a couple of sorts of random sampling that we might want to do:
\begin{enumerate}
\item Sample observations from a set of data. 
\item Sample random deviates from some particular probability distribution. 
\end{enumerate}

\subsubsection{Sampling data}
Suppose {\tt x} is a vector and we would like to sample {\tt n} of its elements, at random, without replacement. {\tt sample} is a function for doing this. For example, let's draw a random sample of size 10 from the integers $1,2,\ldots,100$.
\begin{lstlisting}
> set.seed(0) ## just so you can reproduce what I got
> sample(1:100,10)
 [1]  14  68  39   1  34  87  43 100  82  59
\end{lstlisting}
This sampled each number with equal probability, {\em without replacement}. So each observation had the same chance of occurring in the sample, but could do so only once. This is the sort of sampling that you get if you put numbered balls in a bag, shake them up and then draw out a sample from the bag without looking. \index{sample}

Notice the use of \lstinline+set.seed()+. We can not generate truly random numbers, but only sequences of numbers that appear indistinguishable from random using any statistical test you might apply. What ever sort of random number we eventually want, R starts by generating sequences that appear to be uniform random deviates from $U(0,1)$, and then transforming these. These uniform sequences are generated using a {\em random number} generator, and are in fact completely deterministic. If we repeatedly start the sequence off from the same state, it will generate the same sequence every time. Setting the random number generator to a particular state is known as {\em setting the seed}. To fix ideas, here is how it works for the basic uniform generator
\begin{lstlisting}
> set.seed(3);runif(5) ## 5 random numbers after setting seed
[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007
> runif(5) ## another 5 - all different
[1] 0.6043941 0.1246334 0.2946009 0.5776099 0.6309793
> set.seed(3);runif(5) ## and after setting the seed to 3 again - same as first time
[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007
\end{lstlisting}

Let's get back to sampling. We might also want to sample {\em with replacement}. This is as if we sampled numbered balls from a bag, as before, but this time after noting the number we return the ball to the bag and give the bag a shake, before drawing the next ball.
\begin{lstlisting}
> sample(1:100,10,replace=TRUE)
 [1] 51 97 85 21 54 74  7 73 79 85
\end{lstlisting}
Notice that 85 occurred twice in this sample. Obviously we can sample any vector in this way, not just a vector of integers, but sampling integers like this is especially convenient if you want to sample whole rows of data from a matrix or data.frame. You sample the row indices, and then use these to extract the sampled rows. For example, suppose we want a random sample of 30 rows of data frame, {\tt dat}:
\begin{lstlisting}
ii <- sample(1:nrow(dat),30) ## 'nrow' returns the number of rows of 'dat'
dat1 <- dat[ii,]             ## 30 rows selected at random from 'dat'
\end{lstlisting}
It is also possible to sample with different probabilities of sampling each observation. For example let's sample the integers $i=1,2,\ldots,100$ each with probability $\propto 1/i$.  
\begin{lstlisting}
> sample(1:100,10,prob=1/1:100)
 [1] 13  1  5 19 24 14 12 17 20 63 
\end{lstlisting}
Note that we are sampling with probability proportional to {\tt prob} -- {\tt sample} will normalise the probabilities so that they sum to one internally, before using them. In this example we only got one observation $>25$. Does that seem as expected? Let's work out the probability of sampling an observation $<25$ as a sanity check\ldots 
\begin{lstlisting}
> sum(1/1:24)/sum(1/1:100)
[1] 0.7279127 ## seems fine!
\end{lstlisting}

\subsubsection*{Example: the birthday problem}

A standard toy problem in probability is the birthday problem. What is the probability of at least 2 people in a group of 30 sharing the same birthday? Let's investigate this by simulation. As usual we start by planning. For each group we want to sample 30 birthdays at random from the numbers $1 \ldots 366$, with one of the numbers occurring only in leap years and therefore having 1/4 of the probability of the others. For each group we can then find the unique birthdays, and see how often there are fewer than 30 of them. So  
\begin{enumerate}
\item Randomly sample the $n $ lots of 30 birthdays, and put them in an $n \times 30 $ matrix.
\item For each matrix row, count the unique birthdays.
\item Find the proportion for which that count is $<30$ 
\end{enumerate} 
Here is an implementation, it uses the {\tt apply} functions to apply a function counting the unique birthdays to each row of the birthday matrix. The second argument of {\tt apply} gives the dimensions of the first argument over which the function should be applied. Here the first argument is a matrix, so I give one as the second argument, to apply over rows.
\begin{lstlisting}
n <- 1000000 ## number of groups
bd <- matrix(sample(1:366,30*n,replace=TRUE,prob=c(rep(4,365),1)),n,30) ## birthdays
p <- mean(apply(bd,1,function(x) length(unique(x)))!=30)
\end{lstlisting}\index{apply}
It gives the answer p=0.706. Obviously there is some stochastic variability in that answer - we could work out the standard error of the answer using the variance of a Bernoulli random variable: \lstinline+(p*(1-p)/n)^.5+ is about 0.0005. Or if we are lazy we could just run the simulation repeatedly to assess the variability of the answer. 

There are two rather appealing features of this way of answering the question. Firstly, it was very easy, in a way that most people do not find the probability calculation based answer. More importantly, it is very easy to generalize to situations for which the exact answer would be very difficult to work out. For example, births are not spread out evenly throughout the year, so the probabilities of the different birthdays are not all equal. Given appropriate data it would be easy to adjust the probabilities in the sampling step, and get an adjusted probability for that case too.  

\subsubsection{Sampling from distributions}

R has functions for simulating random variables from a wide variety of basic distributions: see {\tt ?distributions} for an overview. The functions all start with an {\tt r} followed by the distribution name. Their first argument is the number of values to simulate, and remaining arguments are vectors of distribution parameters. For example, if I would like 3 normal random deviates, with means 1, 3.2 and -0.5 and standard deviations 1, 1 and 2.5 then the following produces them\index{random numbers}
\begin{lstlisting}
> rnorm(3,mean=c(1,3.2,-.5),sd=c(1,1,2.5))
[1] 0.4268462 2.1288700 2.3638236 ## you should get different answers here!
\end{lstlisting}
An immediate use for simulation is to quickly get approximate probabilities that may be awkward to compute otherwise. 

For example the time from infection to onset of Covid-19 symptoms follows a log normal distribution with log scale mean and sd of 1.63 and 0.50. Meanwhile one estimate has the distribution of time from symptom onset to death estimated to be a gamma distribution with shape parameter 4 and scale parameter 4.2. Assuming independence of the 2 durations, what does the distribution of time from infection to death look like, and what is the probability that it is less than one week? 
\begin{lstlisting}
> n <- 100000
> d <- rlnorm(n,1.63,.5) + rgamma(n,shape=4,scale=4.2)
> mean(d<7)
[1] 0.00646
> hist(d,breaks=0:100) ## plots a histogram with bar breaks as given
\end{lstlisting}
\eps{-90}{.5}{i2d.eps}\index{histogram}

\subsection{Simulation from a simple epidemic model}

Let's put a few things together and build a slightly more complicated simulation. SEIR (Susceptible-Exposed-Infective-Recovered\footnote{often a euphemism for recovered or dead}) models are commonly used in disease modelling. In the simplest version almost everyone starts out susceptible, and then move to the exposed class according to their daily probability of being infected by someone in the infected class. This daily probability is assumed proportional to the number of people in the infected class, with constant of proportionality $\beta$. People move from the exposed to infected class with a constant probability, $\gamma$ , each day, and move from the infected to the R class with a different constant probability, $\delta$. 

Suppose that we want to simulate from this stochastic SEIR model, maintaining a record of the populations in the different classes over time. And suppose in particular that we are interested in investigating scenarios in which people's daily probability of catching the disease is variable from person to person, so that each has a different $\beta$ value. In fact let's assume that each person has their own $\beta_i$ drawn from a gamma distribution. As usual we first need to think about code design. 
\begin{enumerate}
\item Since the code is for investigating multiple scenarios, we should produce a function that takes parameter values and control constants as inputs, and outputs vectors of the S, E, I and R populations over time.
\item We could maintain vectors for the different stages, and move individuals between them, but this seems clumsy. It is probably better to have a single vector maintaining an integer state indicating each individual's current status 0 for S, 1 for E, 2 for I and 3 for R.
\item The individual states are then updated daily according to the model, and the numbers in each state summed. 
\end{enumerate}    
Here is an implementation. 
\begin{lstlisting}
seir <- function(n=10000,ni=10,nt=100,gamma=1/3,delta=1/5,bmu=5e-5,bsc=1e-5) {
## SEIR stochastic simulation model.
## n = population size; ni = initially infective; nt = number of days
## gamma = daily prob E -> I; delta = daily prob I -> R;
## bmu = mean beta; bsc = var(beta) = bmu * bsc
  x <- rep(0,n) ## initialize to susceptible state
  beta <- rgamma(n,shape=bmu/bsc,scale=bsc) ## individual infection rates
  x[1:ni] <- 2 ## create some infectives
  S <- E <- I <- R <- rep(0,nt) ## set up storage for pop in each state
  S[1] <- n-ni;I[1] <- ni ## initialize
  for (i in 2:nt) { ## loop over days
    u <- runif(n) ## uniform random deviates
    x[x==2&u<delta] <- 3 ## I -> R with prob delta
    x[x==1&u<gamma] <- 2 ## E -> I with prob gamma
    x[x==0&u<beta*I[i-1]] <- 1 ## S -> E with prob beta*I[i-1]
    S[i] <- sum(x==0); E[i] <- sum(x==1)
    I[i] <- sum(x==2); R[i] <- sum(x==3)
  }
  list(S=S,E=E,I=I,R=R,beta=beta)
} ## seir
\end{lstlisting}
Hopefully by now most of this should be relatively easy to understand. The lines like 
\begin{lstlisting}
x[x==2&u<delta] <- 3 
\end{lstlisting}
are doing most of the real work, so these are especially important to understand. First note that elements of the logical vector \lstinline+u<delta+ will be {\tt TRUE} with probability {\tt delta} and {\tt FALSE} otherwise. This is because the elements of {\tt u} are $U(0,1)$ deviates and hence have probability {\tt delta} of being less than {\tt delta}. Elements of \lstinline+x==2&u<delta+ are then {\tt TRUE} only when the corresponding elements of \lstinline+u<delta+ {\bf and} \lstinline+x==2+ are {\tt TRUE}. As a result  \lstinline+x[x==2&u<delta] <- 3+ takes each element of {\tt x} in state 2 and with probability {\tt delta} sets it to state 3. The other lines act similarly, with the probability for the state 0 to 1 transition varying between elements of {\tt x}. 

The fixed $\beta$ version of this basic epidemic model is close to 100 years old. A great deal of the modelling work on Covid uses variations on this structure, perhaps surprisingly, given the simplistic way that the infection process is modelled. Even more surprisingly most of the models ignore the fact that there is wide variability in the number of contacts people have with other people each day, and in their chance of becoming infected at each contact -- i.e $\beta$ is not treated as random. We can use our simple implementation to investigate whether this matters. In particular consider a simulation with almost no variability in $\beta$, with {\tt bmu} and {\tt bsc} set to {\tt 7e-5} and {\tt 1e-7}. Then compare that to a simulation in which $\beta$ varies more widely (\lstinline+bsc=7e-5+). Finally consider a universal social distancing simulation, in which the mean and variability in $\beta$ are reduced (\lstinline+bmu=5e-5+, \lstinline+bsc=2e-6+).  
\begin{lstlisting}
par(mfcol=c(2,3),mar=c(4,4,1,1)) ## set plot window up for multiple plots
epi <- seir(bmu=7e-5,bsc=1e-7)   ## run simulation
hist(epi$beta,xlab="beta",main="") ## beta distribution
plot(epi$S,ylim=c(0,max(epi$S)),xlab="day",ylab="N") ## S black
points(epi$E,col=4);points(epi$I,col=2) ## E (blue) and I (red)
...
\end{lstlisting}
\eps{-90}{.6}{seir.eps}
Comparing the left two columns we see that neglecting the variability in $\beta$ means that the severity of the epidemic will be overestimated by the model. Comparing the right two columns we see that social distancing slows the epidemic down, but can make it larger in the long run, by suppressing the variability in $\beta$ (as everyone has to follow the same rules). Perhaps most surprising of all, scientists who tried to point out the problems with neglecting variability in $\beta$ had extreme difficulty getting published, and were often vilified on social media. Some models used for policy did represent the differences in contact rates between different age groups. But the main survey used to do this was based on one day diaries kept by survey participants of whom 7 were over 75 and none over 80. This is quite limited data for the age groups that make up 3/4 of Covid deaths to date. 

\subsection{Simple simulation study: testing an approximate confidence interval}

Suppose we observe a binomial random variable $X \sim \text{binom}(n,p)$ (number of successes in $n$ independent trials, each with probability of success $p$). $\hat p = X/n$ is an obvious estimator, of $p$, and a simple plug in estimate of its variance is then $\hat \sigma^2_p = \hat p (1-\hat p)/n$, suggesting approximate 95\% confidence interval $\hat p \pm 1.96 \hat \sigma_p$. The central limit theorem implies that this interval will have the correct coverage probability in the $n \to \infty$ limit, but how will it behave when $n=20$, for example? Let's investigate this question for the case $p=.2$. 
  
The basic idea, is that we should generate a large number of observations $x$ and from each one compute a 95\% confidence interval for $p$. We then count up what proportion of those intervals include the true value of $p$ we used in the simulation. If the intervals were exact then that proportion would be very close to 0.95.

\begin{lstlisting}
n.rep <- 10000 ## number of CIs to compute 
n.ci <- 0 ## counter for number that include the truth 
n <- 20;p <- .2
for (i in 1:n.rep) { 
  x <- rbinom(1,n,p) ## generate x
  p.hat <- x/n       ## compute the estimate of p
  sig.p <- sqrt(p.hat*(1-p.hat)/n) ## and its standard error
  if (p.hat - 1.96*sig.p <p && p.hat+1.96*sig.p > p) n.ci <- n.ci + 1 ## truth included?
}
n.ci/n.rep ## proportion of intervals including the truth
\end{lstlisting}
The result is about 0.92 --- so when $n=20$, 8\% of intervals fail to include the true value, instead of the 5\% that should. Many statistical methods rely on infinite sample size approximations, and this sort of simulation study is an essential way of checking out their performance at finite sample sizes.

{\bf Exercise:} write vectorized code to do the preceding calculation without a loop, and investigate how the coverage probability changes as you increase the sample size to 200, 2000 etc.


\section{Reading and storing data in files}

Since statistics is about understanding what data are telling us about the world, a statistical programmer needs to know how to read data into computer memory. Recall that computer memory (or `random access memory') is the fast access temporary working memory of a computer. What is in it vanishes when you turn the computer off. It should not be confused with the long term storage provided by a hard disk, or solid state storage. The latter persists even when the computer is turned off, and is where `computer files' are stored, organised in a hierarchy of folders/directories. Persistent storage is also sometimes accessed remotely over the internet, but we will start with reading in files stored locally on your computer. 

\subsection{working directories}

R maintains a record of the current `working directory'. This is the directory on your computer's file system where it will assume that a file is stored if you don't tell it otherwise. You can find out what the working directory currently is as follows. \index{getwd}\index{setwd}\index{working directory}
\begin{lstlisting}
> getwd()
[1] "/home/sw283" ## my working directory
\end{lstlisting}
The exact format depends on your operating system, but note that R uses \verb+/+ to separate subdirectories from parent directories, even on Windows. You can change the working directory as follows:
\begin{lstlisting}
> setwd("/home/sw283/lnotes")
\end{lstlisting}
making sure that you specify a valid path for your filesystem. For example, on windows the path includes a letter identifying the disk or other storage device where the directory is located. e.g. \lstinline+setwd("c:/foo/bar")+ for subdirectory `bar' of directory `foo' on \verb+c:+. Note the use of forward slashes in place of the windows default back slashes.

\subsection{Reading in code: source()}
\index{source}
Before reading in data, consider reading in code. The \lstinline+source+ function reads in code and runs it from a file. This is particularly useful for reading in function definitions from a file. For example:
\begin{lstlisting}
source("mycode.r")
\end{lstlisting}
reads in the contents of {\tt mycode.r}, first checking that it is valid R code and then running it. Running R code from a file like this is not exactly the same as typing it, or pasting it, into the R console. One main difference is that if you type the name of an object, and nothing else, at the console, then the object will be printed. If a line of sourced code contains the name of an object and nothing else, then the line causes nothing to happen. You would need to explicitly print the object if you want it to be printed. See {\tt ?source} for full details.

A useful function related to {\tt source} is {\tt dump}, which writes a text representation of some R objects to a file as R code, in a way that can later be read back in to R using {\tt source}. For example suppose we have objects {\tt a}, {\tt alice} and {Ba} to save. All we have to do is supply {\tt dump} with a text vector of the object names, and the name of the file to write them to: \index{dump}
\begin{lstlisting}
dump(c("a","alice","Ba"),file="stuff.R")
\end{lstlisting}

\subsection{Reading from and writing to text files of data}

Text files have information stored in human readable form. You can open the file in an editor and understand it. Text files are a very portable way of storing data, but the files are quite a bit larger than is strictly necessary to store the information in the file. R has a number of functions for reading data from text files. All have many arguments to let you adapt to the way that the data has been stored. \index{file!text}

As a simple example consider a file called {\tt data.txt} containing 
\begin{verbatim}
"name" "age" "height"
"sue"  21    1.6
"fred" 19    1.7
\end{verbatim}

{\tt scan} reads data from a file into a vector. For example:\index{scan}
\begin{lstlisting}
> a <- scan("data.txt",what="c");a
Read 9 items
[1] "name"   "age"    "height" "sue"    "21"     "1.6"    "fred"   "19"  "1.7"
\end{lstlisting}
The type of the {\tt what} argument tells {\tt scan} what type of items it will be reading in. Character in this case, given \verb+"c"+ is of type character. {\tt scan} treats white space as the separating items to read, but sometimes the separator is something else, for example a comma. In that case you can specify the separator using the {\tt sep} argument. e.g. \lstinline+sep=","+.

{\tt scan} can also read in data in which different columns of data have different types, returning the different columns as different items of a list. In that case {\tt what} is supplied as a list, with the type of the list items giving the types of the items to be read and stored. Here is an example in which the first line of the file is skipped using the {\tt skip} argument. 
\begin{lstlisting}
> b <- scan("data.txt",what=list("a",1,1),skip=1); b
Read 2 records
[[1]]
[1] "sue"  "fred"

[[2]]
[1] 21 19

[[3]]
[1] 1.6 1.7
\end{lstlisting} 
Actually, the file does not strictly need to have the data arranged in columns, simply in a repeated pattern.

Often the list form of the data is not as useful as having it in a data frame. So let's name the items in the list according to the first row of {\tt data.txt} and convert to a data frame.
\begin{verbatim}
> names(b) <- scan("data.txt",what="c",n=3)
> d <- data.frame(b); d
  name age height
1  sue  21    1.6
2 fred  19    1.7
\end{verbatim}

Reading columns of data into a data frame in R is such a common task that a number of functions are provided for this purpose. The main one is {\tt read.table}. Take a look at {\tt ?read.table} for the variations. An easier way of reading {\tt data.txt} directly into {\tt d} would be \index{data frame!read in}\index{read.table}
\begin{lstlisting}
d <- read.table("data.txt",header=TRUE) 
\end{lstlisting}
where \lstinline+header=TRUE+ simply indicates that the first line gives the variable names. {\tt sep} and {\tt skip} can also be used with {\tt read.table}. Unsurprisingly, the opposite task of writing a data frame out to a text file has it's own function {\tt write.table}. For example:\index{write.table}
\begin{lstlisting}
write.table(d,file="data1.txt") 
\end{lstlisting}

On occasion it is necessary to read lines of text into a character vector, for further processing. The {\tt readLines} function will do this, and there is an equivalent {\tt writeLines} function as well.\index{readLines}\index{writeLines}

To write data other than a data frame or character vector to a text file you can use the {\tt cat} or {\tt write} functions: both take a {\tt file} argument specifying the file name to write to. {\tt dump} is another option covered in the previous section.\index{file!write text}

\subsection{Reading and writing binary files}

To save large R objects or data frames to file, it is more efficient to use a binary format and compression. {\tt save} can do this. It takes a sequence of R objects, and writes them to the file specified in its {\tt file} argument. {\tt  load} will read the objects back in from the file. Here is a simple example:
\begin{lstlisting}
A <- matrix(1:6,3,2); y <- rnorm(100) ## creat some objects
save(A,y,file="stuff.Rd")  ## save them to a binary file
rm(y,A)          ## delete the original objects
load("stuff.Rd") ## load them back in again from the file
\end{lstlisting}
{\tt save} does allow you to choose to save the objects as text as well, but this requires larger files: for the above example twice as large, but sometimes {\em much} larger.\index{save}\index{load}\index{file!binary}

\subsection{Reading data from other sources}

Rather than reading standard files from storage on your local computer, you might want to read data from a file subject to some standard compression algorithm, or over a local network, or from a URL. Most of the functions for reading and writing data can use one of these other types of {\em connection} as their file argument. See {\tt ?connection} for details. Here is a single common example of reading data from a web address
\begin{lstlisting}
raw <- readLines("https://michaelgastner.com/DAVisR_data/homicides.txt") 
\end{lstlisting}
in this case lines of data are read into a character vector. \index{readLines} \index{connections}

\section{Data re-arrangement and tidy data}

In raw form many data sets are messy and have to be tidied up for statistical analysis. By tidying is meant that the data are re-arranged conveniently for analysis, not that the information in the data is in anyway modified unless an actual error is identified (no removal of outliers, for example). Since `data tidying' or `data re-arrangement' sound too boring and clerical for many enthusiasts for Modern Data Science, you will often hear terms like `data wrangling' and `data munging' used instead. These sound suitably rugged, as if the types engaged in them return home aching and covered in dust and mud after a day engaged in the honest toil of these muscular pursuits.

By tidy data is meant that data are arranged in a matrix so that:\index{tidy data}\index{data frame}
\begin{enumerate}
\item Each column is a variable.
\item Each row is an observation (a unique combination of variables).
\end{enumerate}
and it is recognised that what counts as `an observation' can depend on the subset of variables of interest. For example,
this data frame is in tidy form if interest is in student's grades each week. 
\begin{verbatim}
name    d.o.b.  nationality week grade
George  11/2/01 GB          1    B
George  11/2/01 GB          2    C
George  11/2/01 GB          3    A
Anna    15/6/00 GB          1    A
Anna    15/6/00 GB          2    B
Anna    15/6/00 GB          3    A
.       .       .           .    .
\end{verbatim}
But if we are only interested in birth dates and nationality of students, then we would really want to simplify to
\begin{verbatim}
name    d.o.b.  nationality 
George  11/2/01 GB          
Anna    15/6/00 GB          
.       .       .           
\end{verbatim}
It is easy to get the second data frame form the first (\lstinline+marks+, say) in base R using
\begin{lstlisting}
student <- unique(marks[,c("name","d.o.b.","nationality")])
\end{lstlisting}
which has selected the named columns of interest, and then found the unique rows. If you want to refer each row in \lstinline+marks+ to its row in \lstinline+student+ then you can use 
\begin{lstlisting}
library(mgcv)
student <- uniquecombs(marks[,c("name","d.o.b.","nationality")])
ind <- attr(student,"index") 
## ... ind[i] is row of 'student' corresponding to row i of 'marks'
\end{lstlisting}

At this point, you are hopefully wondering about replicate data, and the definition of `observation' as `unique combination of variables'? If I repeat an experiment under identical conditions, could I not occasionally get identical results, and hence two identical rows in a data frame that are different observations? Well yes, but only if you do not include a variable recording the `replicate number'. If you don't record the replicate number then you do not really have tidy data, as it is not possible to distinguish the common error of accidental replication of some recorded data, from genuine replicates.

\section{Graphics}

Visualization of data and modelling results is an essential part of statistical analysis and data science more widely. As we have already seen, base R itself provides graphics functions, but there are also add on packages for various tasks. For example the {\tt lattice} package shipped with R is very useful for visualizing grouped data. Here let's cover base R graphics and the popular {\tt ggplot2} add on package. For a good many tasks {\tt ggplot2} lets you build quite complicated visually appealing plots fairly easily. The `gg' stands for `grammer of graphics' and in keeping with this the package uses its own language to describe plot elements, which is not always obvious. 

In broad overview, the difference between base R graphics and {\tt ggplot} graphics is as follows.
\begin{itemize}
\item In base R a high level plotting command is called with user specified data to produce an initial plot on a graphics device (a plot window or graphics file driver). That plot can then be added to using various auxiliary graphics functions for adding things like lines, arrows, points, polygons and text to the last high level plot produced. The plotting data is passed to these auxiliary functions as needed.  When we are done, we close the device or call the next high level plotting function. 
\item In {\tt ggplot2} plots are built up as R objects, and only actually plotted once the object is {\tt print}ed to a graphics device. A plot object consists of a data frame, a co-ordinate system set up with reference to variables in the data frame, and `geometric' elements such as points, lines, polygons, etc. The plot object is added to and changed by modifier functions. Each modifier function expects some standard variables (such as {\tt x} and {\tt y}), which it usually picks up from the plot's data frame. Mapping the data frame names to these standard variables, or providing the standard variable values directly, is achieved via the {\tt aes} function, as we will see. \index{ggplot}\index{ggplot!aes}    
\end{itemize}
So in {\tt ggplot2} you build up an R object representing a plot, which is then `printed' to a graphics device, and in base R you create plots by calling a sequence of functions whose side effect is to draw things on a graphics device. Base R high level plot functions tend to have huge numbers of arguments to control the look of the plot. {\tt ggplot2} instead has a set of standard plot modification functions, which can make the code for producing a plot less messy, and often quicker and easier to use. The downside of {\tt ggplot2} is that it has its own jargon, which often serves to obscure as much as it illuminates. A useful summary of {\tt ggplot2} functions is provided by\\ 
\verb+https://github.com/rstudio/cheatsheets/raw/master/data-visualization.pdf+\\
but it's incomprehensible without understanding some basics first!

\subsection{Scatterplots, ggplot and base R} 

Let's start by producing a scatterplot in base R and with {\tt ggplot2}. As an example consider the {\tt mpg} data frame supplied with {\tt ggplot2}, on fuel consumption of cars. The following code loads it, takes a look and simplifies the {\tt trans} variable so that it simply differentiates between automatic and manual {\tt transmission}. Then it plots miles per gallon in highway driving against engine displacement (capacity), plotting the data for manual cars in black and automatic in red. 
\begin{lstlisting}
library(ggplot2)
head(mpg)
mpg$transmission <- rep("manual",nrow(mpg)) ## simplified transmission variable
mpg$transmission[grep("auto",mpg$trans)] <- "automatic" ## note automatics
par(mfrow=c(1,2),mar=c(4,4,1,1)) 
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",
     pch=19,col=(mpg$transmission=="automatic")+1)
\end{lstlisting}\index{plot}\index{plot!pch}\index{plot!col}\index{plot!xlab}\index{par!mfrow}\index{par!mar}
The result is provided in the left hand plot below. Note that \lstinline+par(mfrow=c(1,2),mar=c(4,4,1,1))+ sets up the plot device to have 1 row of plots with 2 columns, and for the margins to be 4 characters deep on the left and bottom, and 1 deep on top and right. The {\tt plot} function call is fairly self explanatory. \lstinline+mpg$displ+ and \lstinline+mpg$hwy+ are provided as the $x$ and $y$ variables of the plot, and nice axis labels are also given. \verb+pch=19+ chooses the plotting symbol, while  \lstinline^col=(mpg$transmission=="automatic")+1^ sets the symbol colour to 2 (red) for an automatic, and 1 (black) otherwise.

The plot is fairly basic, and you might like a more modern look, with a nice grey background and white grid lines. Also the symbols are overlapping in an unhelpful way, so they could usefully be smaller. These changes are easy to produce, with a few lines of code.
\begin{lstlisting}
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",type="n") # empty plot
rect(0,0,8,50,col="lightgrey") # add a grey rectangle over plot area 
abline(v=2:7,h=seq(10,45,by=5),col="white") # add grid lines
points(mpg$displ,mpg$hwy,col=(mpg$transmission=="automatic")+1,pch=19,cex=.5) # data
\end{lstlisting}\index{rect}\index{abline}\index{points}\index{cex}
\lstinline+cex=.5+ sets the symbol size to half its default (we could have used it as an argument to {\tt plot} in the original plot).
\eps{-90}{.7}{mpg-base.eps}

{\tt ggplot2} produces plots like the right hand one with a bit less effort, and has lots of other built in features for quickly building plots that would otherwise take rather more coding in base R. For example to produce the sort of plot on the right hand side above might involve this:\index{ggplot}\index{ggplot!aes}\index{ggplot!\verb+geom_point+}
\begin{lstlisting}
a <- ggplot(mpg,aes(x=displ,y=hwy)) + geom_point(aes(colour=transmission)) +
      geom_smooth(method=gam,formula=y~s(x)) ## add a smooth as it's easy!
a    ## 'print' plot object 'a' - that is plot it!
\end{lstlisting}
\eps{0}{.7}{mpg-gg.pdf}
The first line of code is creating the R object representing the plot. 
\begin{itemize}
\item \lstinline+ggplot(mpg,aes(x=displ,y=hwy))+ sets up the plot object, specifying the data frame on which it is based and then which variables in the data frame will be the default $x,y$ variables in the plot. The co-ordinate system and axis are set up with reference to those. This mapping of data frame names to axis variables is achieved by the {\tt aes} function. Any modifications we make to the base plot will use these variable as the $x, y$ variables, unless we explicitly tell the modifier function to use a different mapping.
\item \lstinline^+ geom_point(aes(colour=transmission))^ adds points to the empty plot created by {\tt ggplot}. Modifications are always added to an existing plot using the \verb^+^ operator (there is no \verb^-^ operator). \lstinline+geom_point+ requires {\tt x} and {\tt y} variables to plot. Since we did not specify which variables to use for these, it will use the default $x,y$ variables already specified for the plot in the {\tt ggplot} call. Optionally we can also specify a {\tt colour} variable for the plotted points. Again {\tt colour} is a variable that may be mapped to a variable in the plot's data frame, so it is specified using the {\tt aes} function. In this case {\tt colour} is mapped to the {\tt transmission} variable. 
\item \lstinline^ + geom_smooth(method=gam,formula=y~s(x))^ adds a smooth curve fitted to the data (blue above). It uses the \lstinline+gam+ function to do this - a modelling function that expects a \lstinline+formula+, also supplied.
\item \lstinline+a+ by default invokes the {\tt print} function for a {\tt ggplot} plot object, which causes the plot to be displayed. If you were wanting to display the plot from within a function you have written, you would need to invoke {\tt print} explicitly. That is \lstinline+print(a)+.    
\end{itemize} 
There are two unusual things about {\tt ggplot2} relative to the standard function based R programming that we met so far. One is the \verb^+^ operator for adding plot modifications. The other is that some of the variables required by the modifier functions are not provided as function arguments, but rather as mappings to the plot data frame variables, or as explicit values, using the {\tt aes} function. To be comfortable with {\tt ggplot} you need to understand these.
\begin{enumerate}
\item Like all operators in R, the \verb^+^ operator for adding modifications to a plot is actually implemented as a function with 2 arguments. R has a mechanism for different versions of functions to be used for different classes of objects, so it knows to call the {\tt ggplot} specific version of the {\tt +} operator function when `adding' modifications to a plot. So instead of modification being done using code like \lstinline^a <- a + geom_point()^ ({\tt a} being a plot object), the {\tt ggplot2} authors could have chosen to implement modification like this \lstinline+a <- ggmod(a,geom_point())+ which is actually what the underlying code does. 
\item Supplying variables to a modifier function as mappings to the plotting data, rather than as function arguments emphasises the plot to data link. The {\tt aes} function used to create such mappings takes its name from some peculiar {\tt ggplot} terminology: the non-argument variables used by a function, such as {\tt x} and {\tt y}, are termed {\em aesthetics}. Given the dictionary definition of {\em aesthetic}, that's an odd term to use for the actual data being plotted. I tend to think of {\tt aes} as {\em any extra stuff} instead, which gets the notion of adding definitions not inherited from the existing plot. However, when looking up the help file for a modifier function, these non argument variables are listed under the heading {\tt Aesthetics} (and not under {\tt Arguments}, obviously). In case that's all too clear - some of these {\tt Aesthetics} can also be passed to the modifier functions as regular arguments matched to \lstinline+...+: for example if you want to change the plot {\tt colour} to \lstinline+"red"+.  
\end{enumerate}\index{ggplot!\verb+coord_cartesian+}\index{ggplot!axis range}\index{ggplot!aesthetics}
As a further example suppose we do not like the axis ranges of the last plot and want to modify them. This is easily achieved with the modifier \lstinline+coord_cartesian()+. Looking this up, we find that it is does not need to be supplied with any non-argument `Aesthetic' variables, just regular arguments, so the following would do the job.
\begin{lstlisting}
a <- a + coord_cartesian(xlim=c(0,8),ylim=c(0,50))
\end{lstlisting} 
But what if we wanted more general changes to the axes scales? Browsing the {\tt ggplot2} documentation we might use \lstinline+scale_x_continuous+, e.g.
\begin{lstlisting}
a <- a + scale_x_continuous(limits=c(1,7),name="displacement (litres)")
\end{lstlisting} 
Note that {\tt limits} here does not reset the axis limits unless the existing axis limits are insufficient to cover the range given in {\tt limits}, and those axis limits were set automatically, rather than by \lstinline+coord_cartesian+. In fact {\tt limits} is an altogether dangerous option: it drops the plotting data outside of the interval defined by the limits, which can have undesirable knock on effects. Sometimes you need to read {\tt ggplot2} documentation quite carefully, and then experiment to test your understanding! 

\subsection{Plotting to a file and graphical options}

What if you want to save your plot to a graphics file for incorporation in a report? For {\tt ggplot} or base R, you can simply open a file type graphics device, run the code to display your plot and then close the devise. For a complete list of the available graphics devices see {\tt ?Devices}. If you know how to use one graphics file device it is pretty easy to use any of them. So here is an example of producing an {\tt .eps} file using the {\tt postscript} device. 
\begin{lstlisting}
postscript("mpg-base.eps",width=8,height=3)
## the code to produce the plot goes here - e.g. from previous section
dev.off()
\end{lstlisting}
This was actually the command I used to produce the first plot in the previous section. The first argument to {\tt postscript} is the filename. You either need to change to the directory where you want the file to go using \lstinline+setwd+ before calling \lstinline+postscript+, or give a full path in the filename. {\tt width} and {\tt height} give the width and height of the plot in inches. See {\tt ?postscript} for how to use other units. It is essential to close the postscript file using \lstinline+dev.off()+ when done --- you won't get a properly formed file otherwise. Other commonly use devises are {\tt pdf}, {\tt png}, {\tt jpeg} and {\tt tiff}. \index{graphics file} \index{graphic device}

A very common adjustment needed when printing to file is to change the size of the axis lables and axis titles. In base R the arguments \lstinline+cex.axis+ and \lstinline+cex.lab+ can provided to most high level plot functions to change these. Values $>1$ enlarge, while $<1$ shrinks. You may also want to change plot symbols sizes, with \lstinline+cex}, or change the symbol with \lstinline+pch+ (a number or a character, including \lstinline+"."+ for the smallest possible point). Line widths are controlled with \lstinline+lwd+. See \lstinline+?par+ for all the graphical parameters that can be set. Several are high level parameters set at plot device level, but the rest can be supplied to most high level plotting functions. There are so many options that they are not listed for each high level plotting function, but are passed via \lstinline+...+, which makes sense as they often control lower level plotting functions. \index{plot options}\index{par}\index{plot!sizing}  

{\tt ggsave} is an even easier option for plotting {\tt ggplot2} plots to file. For example
\begin{lstlisting}
ggsave("mpg-gg.eps",width=5,height=3)
\end{lstlisting}
produced the second plot in the previous section. {\tt ggsave} infers the plot file type from its extension ({\tt .eps} here).\index{ggplot!graphics file}

\subsection{Some univariate data plots}

Suppose that you are interested in visualizing the distribution of a single variable. For a continuous variable perhaps the simplest plot is a histogram. This divides the range of your data into a series of contiguous intervals, and counts the number data falling within each of them. The intervals (also know as {\em bins}) are then arranged along a horizontal axis, and bars are plotted above each interval, with {\em area} proportional to the number of observations in the interval. If the intervals all have the same width, then the {\em height} of the bar is also proportional to the number of observations in the interval.   

As an example the {\tt faithful} data frame in R contains 272 observations of the duration of the Old Faithful geyser's {\tt eruptions} in minutes (it also contains {\tt waiting} time between eruptions). Suppose we are interested in the distribution of eruption times. Here is the code to produce a couple of alternative histograms.
\begin{lstlisting}
par(mfrow=c(1,2)) ## set up to allow 2 histograms side by side
hist(faithful$eruptions) ## default
hist(faithful$eruptions,breaks=quantile(faithful$eruptions,seq(0,1,length=15)),
     xlab="eruption time (mins)",main="Old Faithful geyser",col="purple")
\end{lstlisting}
\eps{-90}{.6}{geyser-base.eps}
The left plot is from the first default call to \lstinline+hist+. The number and location of the x axis breaks defining the intervals has been chosen automatically, and the intervals are all of equal width. The y axis shows the count in each bar. Note the obvious interpretation of the bar heights as being proportional to counts per unit x axis interval. 

We do not have to stick with the automatically computed intervals. The second call to \lstinline+hist+ produces the right hand plot. In addition to its lurid colour and nicer labels, the main noticeable feature is that I have supplied the breaks between intervals, and have not spaced those breaks evenly. In fact I have used the {\tt quantile} function to set the breaks at 15 empirical quantiles (equally spaced in probability) of the distribution of eruption durations\footnote{Recall that the $q^{th}$ quantile, $\tau_q$, of random variable $X$ is defined so that $P(X<\tau_q) = q$.}. So here we have bars of uneven width, the height of which is still proportional to the number of observations per unit interval, but whose area is proportional to the counts in each interval. One caveat about `area' here: by area I mean width as measured on the x axis, multiplied by height as measured on the y axis. Since the axes are plotted with different scalings on the page, the bars will not have the same `on the page' area. Note also that the y axis label is changed to `Density', since while the bar heights are proportional to density, they are not any longer proportional to the frequency (count) of observations in the intervals. 

Actually uneven intervals are rarely used when plotting histograms, and it is more usual to use the {\tt breaks} argument  to supply a larger number of evenly spaced breaks, to provide more detail. One problem with histograms is that simply shifting the interval breaks a constant small amount can change the shape of the histogram, as points get moved between intervals. This is one reason for sometimes favouring a direct estimate of the pdf of the data, based on the sample. 

{\em Kernel density estimation} is a way to do this, which makes not too many assumption. The basic idea is to recognise that any data point we observed could equally well have been a bit bigger or a bit smaller than it turned out to be. So we could simulate a much larger sample than the one we have by simulating noisy versions of the data we observed. That much larger sample could be plotted using a histogram with very narrow bars, to get an almost continuous pdf estimate. For example:
\begin{lstlisting}
n <- nrow(faithful);set.seed(8)
erupt <- faithful$eruption + rnorm(n*1000,sd=.2) ## simulate much bigger sample
hist(erupt,breaks=100,freq=FALSE) ## plot it
\end{lstlisting}   
\eps{-90}{.6}{geyser-kernel.eps}
The plot is the left hand one above. Notice how the recycling rule has been used to produce 1000 slightly perturbed versions of each original eruption duration, and the resulting huge sample then yields a smooth and detailed histogram. It's worth running this code with different \lstinline+sd+ values -- larger values give smoother distributions, and smaller values more wiggly ones. 

But do we really need to simulate at all here? If we simulated an infinite number of perturbed observations around each original datum, we would end up with a sample for each datum that looked just like a normal pdf, centred on the datum. So when we pool the simulated observations we get what looks like the average of all those normal pdfs centred around each original observation. In that case we might as well cut the simulation step, and just average the pdfs. i.e. if $x_i$ denote data points, where $i=1,\ldots,n$, and $\phi(x|\mu,\sigma)$ denotes a normal pdf with mean $\mu$ and standard deviation $\sigma$, then the density estimate is 
$$
\hat \pi (x) = \frac{1}{n} \sum_{i=1}^n \phi(x|x_i,h)
$$     
where $h$ is our chosen value for the standard deviation. Such an estimate is known as a {\em kernel density estimate}, with {\em bandwidth} $h$. We could use any other valid pdf in place of $\phi$, of course. The {\tt density} function in R will compute such kernel density estimates, choosing the bandwidth parameter automatically. Sometimes the automatic selection appears to over or under smooth the density. For the eruption times it seems to over smooth putting a bit too much probability between the peaks. We can reduce the bandwidth relative to its automatic value by setting the {\tt adjust} parameter to something less than one. Here is code for plotting the density estimate on the right, above.  
\begin{lstlisting}
plot(density(faithful$eruptions,adjust=0.6),type="l",xlab="eruption time (mins)",
     main="kernel denisty estimate")
\end{lstlisting}

\noindent{\bf Exercise}: overlay a kernel density estimate over a histogram of the eruptions data (hint: used {\tt lines}).

Let's see how the task in the exercise could be done in {\tt ggplot2}.
\begin{lstlisting}
ggplot(faithful,aes(x=eruptions,after_stat(density))) + ## data, base mappings etc
geom_histogram(bins=20,col="green",fill="purple") +     ## garish histogram
geom_density(adjust=.5,col="red")                       ## added kernel density estimate
\end{lstlisting}
\eps{0}{.6}{geyser-gg.eps}
\begin{enumerate}
\item The \lstinline+gglot+ call sets up the data and x-axis for the plot. The \lstinline+after_stat+ function is needed to indicate that what is plotted should depend on densities, rather that counts. The documentation for this is not the clearest.
\item The \lstinline+geom_histogram+ turns the empty plot into a histogram with 20 bins (intervals). The colour of the histogram border and fill is set directly. Note the way that these are supplied as (undocumented) arguments, although described as {\em  aesthetics} in the documentation. This way of supplying options is quite common for modifier functions: it is implemented using the \verb+...+ agument.
\item The \lstinline+geom_density+ modifier adds a kernel density estimate to the plot. Without the 
\lstinline+adjust=.5+ argument this estimate is obviously too smooth. Unfortunately {\tt adjust} is not actually documented as an argument of \lstinline+geom_density+, but it works and is documented as an argument of another kernel density related function.    
\end{enumerate}
You will by now have noticed that the {\tt ggplot2} help files are not always as useful as the base R help files, and to really get to grips with {\tt ggplot2} requires finding online training materials and often searching online forums such as the programmers site {\tt stackexchange} for queries related to what you want to do. However many people who have spent the time getting to grips with it find it extremely useful and say that it saves them alot of time. 

Why did the automatic bandwidth choice lead to a kernel density estimate that was too smooth? Bandwidth selection methods assume that the original data are independent, and can go wrong when this is not the case. The eruption data are really a time series, and it could be that one eruption's duration influences the duration of subsequent eruptions. A standard check for this is to plot the {\em auto-correlation function} (ACF) of the data series. The ACF plots the correlation between data and the previous data point, and the correlation between data and the data 2 data points before it, and so on. The {\em partial} ACF does the same, except that the correlations are measured conditional on the correlations found at shorter lags - that is they are the differences between the correlation found and the correlation expected given the correlations found at shorter lags\footnote{e.g. if the autocorrelation at lag 1 is $0.7$ then the auto-correlation at lag 2 is expected to be $0.7^2 = 0.49$ purely on the basis of the lag 1 autocorrelation. It's really only the difference between the lag 2 correlation and 0.49 that is then interesting - which is what the PACF measures.}. Here is base R code:
\begin{lstlisting}
acf(faithful$eruptions); pacf(faithful$eruptions)
\end{lstlisting}
\eps{-90}{.6}{geyser-acf.eps}    
\ldots the correlations or partial correlations for each lag are plotted. Clearly there is strong negative correlations between the duration of one eruption and the next. Short eruptions follow long eruptions (and the other way around). The PACF suggests that the correlation at lag 2 is close to perfectly explained by the lag 1 correlation, but the negative correlation at lag 3 may be a bit less than would be expected from the lag 1 effect. In both plots the blue lines indicate how large a correlation has to be to be detectably different from zero at the 5\% level.  

\subsubsection{Discrete univariate data}

When the variables of interest are discrete categories, rather than continuous quantities, then a bar chart is often a useful visualization method. The idea is that you count the occurrences of each category and plot bars with height proportional to the count. This is a bit like a constant bar width histogram, except that the ordering of the bars may not matter at all in some cases. As an example of using bar charts, let's visualize the total number of deaths from all causes over the course of the Covid-19 epidemic to August 2021, by calendar age grouping, and compare this to the numbers of Covid deaths. The data are available from the UK Office for National Statistics (ONS). First read in the data\ldots

\begin{lstlisting}
d <- c(3362,428,290,369,978,1723,2471,3731,5607,7926,12934,20688,29905,40456,53579,
       84784,106448,136269,150325,178684) ## all deaths
cd <- c(3,1,3,10,26,65,133,257,438,740,1418,2587,4250,6337,8528,13388,18622,25220,
        27718,30247)  ## Covid-19 deaths
names(cd) <- names(d) <- c("0","1-4","5-9","10-14","15-19","20-24","25-29","30-34",
"35-39","40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79",
"80-84","85-89","90+") ## ONS age classes
\end{lstlisting}     
Then let's plot the deaths by age, side by side. Notice the use of \lstinline+ylim+ to force the same y axis scale. The \lstinline+las=2+ argument forces the category labels to be written vertically, so they fit. 
\begin{lstlisting}
par(mfrow=c(1,2))
barplot(d,las=2,main="All deaths Mar 20-Aug 21")
barplot(cd,ylim=c(0,max(d)),las=2,main="Covid deaths Mar 20-Aug 21")
\end{lstlisting} 
\eps{-90}{.5}{deaths1.eps}   
The side by side presentation is interesting, but because of the way death rate increases with age, it does not really allow comparison of the Covid and non-Covid risks for ages below 45. One way to do this might be to plot the Covid and non-Covid deaths stacked on the same bar chart, and then to produce a 'zoomed in' bar chart looking just at the under 45s. Stacked barcharts are easy. Instead of providing a single vector to \lstinline+barplot+ we just need to provide a matrix, where each row contains a set of counts to be plotted. Notice the arguments for including a legend indicating which bar colour is which.
\begin{lstlisting}
par(mfrow=c(1,2))
D <- rbind(cd,d-cd) ## create matrix of counts to plot
rownames(D) <- c("covid","not covid")
barplot(D,las=2,main="All deaths Mar 20-Aug 21",legend.text=TRUE,
args.legend=list(x=7,y=150000)) 
barplot(D[,1:10],las=2,main="Age 0-44 deaths  Mar 20-Aug 21")
\end{lstlisting}
\eps{-90}{.5}{deaths2.eps} 
This seems better, with the picture for under 45s now much clearer\footnote{If you are surprised by the data, that may be partly down to official messaging. The UK government advisory Scientific Pandemic Influenza Group on Behaviour (SPI-B) wrote in a 22 March 2020 report that: ``\ldots a substantial number of people still do not feel sufficiently personally threatened; it could be that they are reassured by the low death rate in their demographic group\ldots the perceived level of personal threat needs to be increased among those who are complacent, using hard hitting emotional messaging.'' and presumably not using data on the actual risk. }. We can produce a similar plot using {\tt ggplot}. The \lstinline+geom_bar+ function might seem the obvious way to do this, but it is actually designed for data frames in which a variable may take one of several categorical values, and the plotting function should do the counting up of numbers of occurrences of each category. To use it we would need to artificially expand the 20 ONS counts of deaths in each age class to some 0.8 million records of the age category for each death. Rather than do that, we'll need to use \lstinline+geom_col+ which can work with pre-tallied counts in categories.  
 
{\tt ggplot2} requires data in strictly tidy form, and so \lstinline+geom_col+ will not death with multiple rows of data as \lstinline+barplot+ did. Rather it expects counts of {\tt deaths} to be a single variable in a single column, with another variable indicating the {\tt type} of death (covid or non-covid). Similarly the {\tt age} categories should be contained in another variable in its own column.   
\begin{lstlisting}
n <- length(d)
age <- factor(1:n);levels(age) <- names(d) ## age class as a factor
ggdat <- data.frame(age=c(age,age),deaths=c(cd,d-cd),
         type=c(rep("covid",n),rep("not covid",n))) ## tidy data frame
ggplot(ggdat,aes(x=age,y=deaths,fill=type)) + geom_col() +
       labs(title="All UK deaths Mar 2020 - Aug 2021")
\end{lstlisting}
\eps{0}{.5}{deaths3.eps} 
\begin{itemize}
\item The {\tt age} factor variable defines the age categories. The obvious code \lstinline+age <- factor(names(d))+, causes R to order the factor levels in way that puts the \verb+5-9+ level just before the \verb+50-54+ level, and {\tt ggplot} orders the bars by the order of the levels, not their order of appearance in the data frame. Since that is not what we want here, it is better to set the factor up first and then change the names assigned to its levels in a way that we control the ordering.
\item \lstinline+ggdat+ is simply set up in the tidy way that \lstinline+geom_col+ demands.
\item The plot setup is now straightforward. The mappings of data frame variables to the variables \lstinline+geom_col+ requires is done in \lstinline+ggplot+ here, but could also be done in \lstinline+geom_col+. By default  \lstinline+geom_col+ will stack up the bars corresponding to the same age class on top of each other. \lstinline+fill=type+ specifies that we want those stacked bars coloured according to type of death. The \lstinline+labs+ modifier just changes the title.    
\end{itemize}


\subsection{Boxplots and violin plots}

Statistical analysis is often concerned with how the distribution of some variable changes between different groups. That is with plotting a continuous variable against a categorical variable in a meaningful way. {\em Boxplots} are one useful way of summarizing a distribution. A box is plotted, showing the middle 50\% of the data, with a line across it marking the median. Lines (`whiskers') are drawn from either side of the box to the extremes of the data, after removal of any {\em outliers} more that 1.5 times the width of the box away from the box. The outliers are then plotted individually. To see this in action, consider the \lstinline+ToothGrowth+ data frame in R. This reports data from an experiment in which guinea pigs had their diet supplemented with 0.5, 1 or 2 mg of vitamin C per day, either administered as Orange Juice, or ascorbic acid. The response measured was odontoblast length at the end of the experiment. The following code uses boxplots to visualize the distribution of the lengths against dose, method of delivery ({\tt supp}), and their combination. 

\begin{lstlisting}
par(mfrow=c(1,3),mar=c(5,5,1,1))
boxplot(len~supp,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose+supp,data=ToothGrowth,cex.lab=1.5)
\end{lstlisting}
\eps{-90}{.5}{boxplot1.eps} 
\ldots make sure you relate what is plotted back to the description of what a boxplot is. Clearly length increases with increased dose, and there is a suggestion that orange juice is the better delivery method. Looking at the plot on the right, there is a clear suggestion of an {\em interaction}: the effect of dose appears to change with delivery method (equivalently the effect of delivery method changes with dose). In other words it looks as if we can not simply add the effect of delivery method to the effect of dose. This notion of an interaction between predictor variables ({\tt supp} and {\tt dose}) is very important in statistical modelling --- so take some time to understand what it means here. 

{\tt ggplot2} also provides boxplots. If we want to look at interactions of variables, we need to explicitly construct a variable labelling the combinations of variables for which we require separate box plots. Such labels can be created using {\tt paste}. Note the use of the \lstinline+with+ function, which simply tells R to look for variable names in the given data frame. 
\begin{lstlisting}
ToothGrowth$comb <- with(ToothGrowth,paste(supp,dose))
ggplot(ToothGrowth,aes(y=len,group=comb)) + geom_boxplot()
\end{lstlisting}
\eps{0}{.5}{boxplot2.eps} 
Notice that with this small number of data, base R and {\tt ggplot} do not quite agree! {\tt ggplot} also allows slightly more detailed visualization of the data distribution in the form on {\em violin plots}. Rather than drawing simple boxes and whiskers, these draw a mirrored version of the kernel density estimate in each group, truncated at the maximum and minimum observed data values. Let's try this to visualize the (combined) effect of dose. As dose is numeric, it will need to be turned into a factor (group lable) to get what we want.
\begin{lstlisting}
ggplot(ToothGrowth,aes(y=len,x=as.factor(dose))) + geom_violin()
\end{lstlisting}
\eps{0}{.5}{violin.eps} 
so the width of the white shapes at some length value is proportional to the estimated probability density for length having that value.  
 
\subsection{3D plotting}

Sometimes it is useful to be able to plot functions of 2 variables. In statistical work these are often model output, representing, for example $E(z|x,y)$. For now consider the case of visualizing a simple function of two variables. Base R offers 3 approaches - perspective plots, image plots and contour plots. They all require the same input - vectors defining the $x$ and $y$ values against which to plot, and a matrix {\tt z} whose \lstinline+z[i,j]+ value gives the function value at \lstinline+x[i]+, \lstinline+y[i]+. Here is some example code.
\begin{lstlisting}
foo <- function(x,y) { ## function to be visualized
  r <- sqrt(x^2+y^2)
  exp(-r*2)*cos(r*10)
} ## foo
n <- 50; x <- seq(-1,1,length=n); y <- seq(-1,1,length=n)
xy = expand.grid(x=x,y=y) ## regular x,y grid over 
xy$z <- foo(xy$x,xy$y)    ## evaluate foo on the grid
z <- matrix(xy$z,n,n)     ## put evaluated function in a matrix
par(mfrow=c(1,3),mar=c(4,4,1,1)) ## divide plot devise into 3 columns
persp(x,y,z,theta=30,phi=30,col="lightblue",shade=.3) 
image(x,y,z)
contour(x,y,z,xlab="x",ylab="y")
\end{lstlisting}
\eps{-90}{.5}{3d.eps} 
The \lstinline+persp+ plot is self-explanatory, but it is very difficult to read the actual function values from it. Note that \lstinline+phi+ and \lstinline+theta+ control the orientation of the plot, while \lstinline+shade+ creates shadowing to aid visualization. The \lstinline+image+ plot shows the function as a colour map -- you can choose the colour scheme, but in this case red is high and white is low. The best option for actually displaying the surface numerically is a \lstinline+contour+ plot. This plots contours of equal value as continuous labelled curves. The plot can be read like a topographic map, and the value of the $z$ variable at any $x,y$ can be read approximately from the plot. A nice option is to overlay a contour plot on an image plot, using the \lstinline+add=TRUE+ argument to \lstinline+contour+. Then you get nice visualization and hard quantitative information on the same plot.


{\tt ggplot} also offers contouring and image plotting functions. Oddly the contour plot does not allow contours to be labelled, rendering the contours alone rather useless. However a combined plot is more useful.
\begin{lstlisting}
ggplot(xy,aes(x=x,y=y)) + geom_raster(aes(fill=z)) + geom_contour(aes(z=z))
\end{lstlisting}
\eps{0}{.5}{gg-contour.eps} 
We need to move on, but hopefully you have now a good enough grasp of base R and {\tt ggplot} graphics to be able to explore further independently.
%\makeindex
\printindex

\end{document}
