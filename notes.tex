\documentclass[10pt] {article}
\usepackage{epsf}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage{makeidx,epsfig,lscape}
\usepackage{natbib}
\usepackage{rotating}
\usepackage{floatpag}
\rotfloatpagestyle{empty}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{epsf}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{comment}
\usepackage{bm}
\usepackage{times}
\setlength{\textheight}{22.5cm}
\setlength{\textwidth}{16cm}
\setlength{\oddsidemargin}{-5mm}
\setlength{\topmargin}{-0.8cm}
\setlength{\evensidemargin}{-5mm}
\usepackage{listings}
\newcommand{\grad}{\nabla}
\newcommand{\tr}[1]{\text{tr}(#1)}
\newcommand{\bp}{{\vm \beta}}
\newcommand{\X}{{\vf X}}
\newcommand{\E}{E}
\newcommand{\vf}{\bf} %% vector type-setting
\newcommand{\vm}{\bm} %% vector type-setting
\newcommand{\ts}{^{\rm T}}
\newcommand{\its}{^{-\rm T}}
\newcommand{\bmat}[1]{\left [ \begin{array}{#1}}
\newcommand{\emat}{\end{array}\right ]}
\newcommand{\eps}[3]
{{\begin{center}
 \rotatebox{#1}{\scalebox{#2}{\includegraphics{#3}}}
 \end{center}}
}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax} %% original {arg\,max} to space arg max
\lstset{language=R,
        basicstyle={\ttfamily\small},
        keywordstyle=,
        showstringspaces=false,
        columns=flexible}

%% Definitions
\newcommand {\hide}[1] {\typeout{ #1 }}
%Comment out to print all
%\newcommand {\hide}[1] {{\it #1 }}
%Comment out to hide some
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\dif}[2]{\frac{{\rm d} #1}{{\rm d} #2}}
\newcommand{\ildif}[2]{{\rm d} #1/{{\rm d} #2 }}
\newcommand{\ilpdif}[2]{\partial #1/{\partial #2 }}
\newcommand{\pdif}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pddif}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\ilpddif}[3]{\partial^2 #1/{\partial #2 \partial #3}}
\newcommand{\comb}[2]{\left (\begin{array}{c}{#1}\\{#2}\end{array}\right )}
\newcommand{\gfrac}[2]{\mbox{$ { \textstyle{ \frac{#1}{#2} }\displaystyle}$}}
\newcommand{\R}{{\sf R }}
\theoremstyle{definition}
  \newtheorem*{definition}{Definition}
  \newtheorem*{example}{Example}
%\newcommand{\vm}{\bm}
% comment out next line unless double spacing needed
%\renewcommand{\baselinestretch}{2}

\newtheorem{theorem}{Theorem}

\makeindex

\begin {document}

\centerline{\huge \bf Statistical Programming}

\tableofcontents


\section{Software Requirements: R, git, JAGS etc \label{sec:software}}

To complete this course you will need to get yourself a free github account and install the following software on your computer: 
\begin{itemize}
\item R or Rstudio, git, pandoc and JAGS.
\item R packages: ggplot2, rjags, rmarkdown, debug.
\item Only if you do not already have latex installed, install tinytex. 
\end{itemize}
It is assumed that you have the basic computer skills to do this. If not, you will need to spend some time online acquiring them (this course is about programming, not basic computer skills). If you have difficulty, you can post questions on piazza for other students to answer. Please answer other student's questions on installation on piazza. 

\index{git!installation}\index{github!account}\index{R!instalation}\index{JAGS!installation}\index{Rstudio!installation}
\index{CRAN}
In more detail.
\begin{itemize}
\item {\tt R} is the free statistical programming language and environment that we will use. You can get a copy from the Comprehensive R Archive Network (CRAN)\\
\lstinline+https://cran.r-project.org/+\\
just follow the links under `Download and Install R'. 
\item {\tt Rstudio} is an alternative front end for R, which many people prefer to use. It provides an R code editor, R session window, R graphics window and other information, all in a convenient `integrated environment'. If you prefer this to R as available from CRAN, you can get it from\\
\lstinline+https://www.rstudio.com/+\\
at the foot of the page, under `R studio desktop', or go straight to \\
\lstinline+https://www.rstudio.com/products/rstudio/download/#download+
\item {\tt git} is a version control system. It helps you to write code in teams, as you must do for this course, without breaking each other's work. It also lets you keep a record of the changes you make, so that you can go back to earlier versions, if you need to. To install {\tt git} follow the instructions at\\
\lstinline+https://www.git-scm.com/+\\
On Windows use the default options. {\tt linux} systems often have it installed already and something like \lstinline+sudo apt install git+ will install it if not. 
\item For Mac or linux you also need to install the Git Credential Manager Core (this is automatically installed for Windows). Instructions are at:\\
{\scriptsize \verb+https://docs.github.com/en/get-started/getting-started-with-git/caching-your-github-credentials-in-git+}\\
Linux is slightly fiddly (I used the plain text option 4 for credential storage). 
\item {\tt pandoc} is document conversion software used by the {\tt rmarkdown} package. Installation instructions:\\
\lstinline+https://pandoc.org/installing.html+ 
\item {\tt JAGS} stands for `Just Another Gibbs Sampler'. We will use it for programming Bayesian models and sampling from them. Downloads are available here:\\
\lstinline+https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/+\\
but under linux something like \lstinline+sudo apt install jags+ can be used. Note that if you have a new apple/mac with an apple silicon processor (rather than intel), then you will need to install MacPorts from\\ 
\lstinline+https://www.macports.org/install.php+\\
then install JAGS as described here\\
\lstinline+https://ports.macports.org/port/jags/+
\end{itemize}

Once the above are installed, you will need to install some packages from the R session command line. R/Rstudio can install packages in a local directory/folder for you, but I tend to start R as administrator/super user/root and then install them (e.g. on linux I would start R from the command line as \lstinline+sudo R+). Here are the packages you need:
\begin{itemize}
\item {\tt ggplot2} provides a nice alternative to built in R graphics. Install by typing\\
\lstinline+install.packages("ggplot2")+\\
at the R command line.
\item {\tt rmarkdown} provides a neat way of documenting data analyses using R. Install using\\
 \lstinline+install.packages("rmarkdown")+
\item {\tt rjags} lets you use JAGS directly from R. Install using\\
 \lstinline+install.packages("rjags")+
\item Finally we need a debugging package, to help you to find errors in your code, or understand how code is working by stepping through it. What is built into R and Rstudio is not great, and a much better option is Mark Bravington's debug package. This is not available from CRAN, but only from Mark's repository, so to install, you need to do this:
\begin{lstlisting}
options(repos=c("https://markbravington.github.io/Rmvb-repo",
        getOption( "repos")))
install.packages("debug")
\end{lstlisting}
{\tt debug} requires the {\tt tcltk} package, which is installed by default in R, but has requirements which are not met by default on all mac installations. If installation fails for this reason you need to read the error message carefully - it will probably tell you to install {\tt Xquartz}.

\item {\tt rmarkdown} requires a latex installation in order to allow you to produce pdf documents. So, {\em only if you do not already have latex installed}, install {\tt tinytex} with
\lstinline+install.packages("tinytex")+
then if you are {\em really sure} that you do not already have latex installed run 
\lstinline+tinytex::install_tinytex()+
WARNING: this command may break existing latex installations!
\end{itemize}
Once you have the software installed, you need to do one more thing. Get a free {\tt github} account (if you do not already have one).  \lstinline+https://github.com/join+ is where to get one. 


\subsection{Using a terminal window, choose a text editor}

This course involves programming - that is writing text instructions that get a computer to do something. Anyone who programmes rapidly discovers that writing text to get a computer to do something is often quicker, more convenient and more reproducible than clicking your way through a graphical interface. This is sometimes true even for basic tasks such as file copying, or moving between directories/folders. 

Whatever operating system you are using you will be able to launch a `terminal window' for this purpose. Make sure that for your operating system you know how to do this, how to list the names of the files in a directory/folder, how to change from one directory/folder to another, how to delete a file, and how to close the terminal window. 

You will also need to edit text files containing the computer code you write. If you choose to use {\tt Rstudio}, there is a built in editor, and you might choose to just use that. If you use plain {\tt R} then you will need an external editor. {\tt Word} is not suitable, as it will insert hidden characters in your code that will cause problems, so you instead should use something simpler (e.g. {\tt wordpad} or {\tt notepad} on windows). Make sure you know what is available on your computer and use that. I tend to use {\tt emacs}, but this has more features than you actually need for this course. 

\section{git and github}

{\tt git} is a system that lets you track changes in code files and to manage those files when several people are working on them. In particular it lets you maintain a central {\em repository} of your code, which serves as the backed up master copy. The repository is just a folder/directory containing the master copy of your code. Several people can make working local copies of the repository (on their own computers) and work on the code, only merging those changes into the master copy when they are ready. {\tt git} provides the tools to allow management of the master copy in a way that avoids or resolves conflicts. A conflict is when two people made contradictory changes to the code. \index{git}

\subsection{Setting up a repo on {\tt github}}

An easy way to set up a central repository is to use {\tt github}.\index{github!repository}  
\begin{itemize}
\item Login to your {\tt github} account on {\tt github.com}, click on {\tt Repositories} and then click on {\tt New} to set up a new repository.
\item Follow the instructions. You probably want your repository to be private if it is for working on assessed coursework.
\end{itemize}
For group homework, one person sets up the repo and invites their team mates to collaborate on it. 
\begin{itemize}
\item Log on to {\tt github}, nagivate to your repo, and click on {\tt Settings}.
\item Click on {\tt Manage Access} from the left hand menu, and then on {\tt Invite a collaborator} - you will need their github user name. 
\item Note that collaborators have to accept the invitation before they can save work (push) to the github repo -- this applies whether the repo is private or public. 
\end{itemize}
You can add files directly to your repo, via the web interface, but we will shortly cover how to add files to the local copy of your repo, and then {\em push} them to github. 

\subsection{Using {\tt git}}

Mostly, you will interact with your {\tt github} repo via the {\tt git} software, and your local copy of your {\tt github} repo. You will use {\tt git} by typing commands in a terminal window. To check everything is working before using {\tt git} for the first time you might open a terminal window and check the {\tt git} version, a follows:\index{git}
\begin{lstlisting}
git --version
\end{lstlisting}
This gives the answer \lstinline+git version 2.25.1+ for me. Here we will cover only the most basic use of {\tt git} and {\tt github}: the minimum needed to collaborate on projects and keep your work backed up. You can find much more at:
\begin{itemize}
\item \lstinline+https://git-scm.com/docs/gittutorial+
\item \lstinline+https://guides.github.com/+
\item \lstinline+https://training.github.com/+
\end{itemize}
Note that {\tt Rstudio} offers built in facilities for using {\tt github}, which you may find useful. However before using them it is better to first learn how {\tt git} and {\tt github} work by using them in the way described here, which is transferable to any project, not just R programming with {\tt Rstudio}.  

Before moving on, if you are using a Mac or linux you probably want to issue the command:
\begin{lstlisting}
git config --global core.autocrlf input
\end{lstlisting}
which deals with the fact that line ends are dealt with differently by different operating systems and handles this gracefully (Windows is set to do this by default).\index{git!line endings}

\subsubsection{Making a local working copy of the repo}

After creating your github repo you need to make a local copy of it on your computer. 
\begin{itemize}
\item From your {\tt github} repo page (at {\tt github.com}) click on {\tt Code} select {\tt Clone} and copy the {\tt htpps} address for the repository that appears. For example, for my repo containing these notes, the address is \lstinline+https://github.com/simonnwood/sp-notes.git+
\item In the terminal window on your local machine change directory ({\tt cd}) to the folder where you would like the local repo to be located. Then type {\tt git clone} followed by the address you copied. e.g. 
\begin{lstlisting}
git clone https://github.com/simonnwood/sp-notes.git
\end{lstlisting}
\end{itemize}
A local copy of the repo is then created on your computer. \index{git!local copy} \index{git!clone}

\subsection{Modifying work and synchronising with the github repo}

The local copy of your repo is just a directory/folder containing the files in your repo, and a hidden {\tt .git} subdirectory that {\tt git} uses to maintain an index of the files that it should keep track of, and the history of changes made. {\tt git} knows about all the files in the {\tt github} repo you cloned, but will only start keeping track of other files that you may add to your local repo when you tell it to do so (with \lstinline+git add+). 

Similarly {\tt git} does not keep a record of all the changes you make to your code as you work on it. Rather, it takes and stores a `snapshot' of the state of the files it is tracking only when you tell it to, using a {\tt git commit} command. \index{git!commit}

Neither does {\tt git} automatically save all these snapshots to your master {\tt github} repo. That only happens when you tell it to using {\tt git push}. This is quite useful, you can keep a detailed record of the changes you make while working on code locally, without modifying the {\tt github} master repo until you are satisfied that the changes are complete and working.   \index{git!push}

Note that you can access the help pages for the main {\tt git} commands by typing {\tt git help} followed by the command name. For example, if you want to know more about {\tt git add} type \index{git!help}
\begin{lstlisting}
git help add
\end{lstlisting}

\subsubsection{Simple work cycle}

Suppose we want to make some code changes in a file \lstinline+foo.r+ that already exists in the master repo. A typical work sequence would be as follows.
\begin{enumerate}
\item  Change directory ({\tt cd}) to the local repo.\index{git!pull} 
\item If collaborating with others, you might want to make sure that you local repo is up to date with the master copy on {\tt github}, using \lstinline+git pull+ to get (`pull') any changes from {\tt github}. (\lstinline+git diff @{upstream}+ can be used to view the changes first, if you prefer.) 
\item Work on {\tt foo.r} until you are happy with the changes made, and have tested them. 
\item Have {\tt git} take a snapshot of the changes made using something like 
\begin{lstlisting}
git commit -a -m"added a wibbleblaster to foo.r"
\end{lstlisting}  \index{git!commit}
The \lstinline+-a+ option tells {\tt git} to take a snapshot of every file that it is tracking that you have changed. If you omit \lstinline+-a+ then you must tell {\tt git} which files you want it to snapshot explicitly, for example using \lstinline+git add foo.r+, before committing. The option \lstinline+-m+ adds a message describing the changes. If you omit the \lstinline"-m" then {\tt git} will open an editor, in which you enter the message - this can be useful if you want to include a longer set of comments. These comments can be viewed on the {\tt github} repo, providing a useful record of what the individual code changes were for. \index{git!add} 
\item Now `push' the changes to {\tt github}. 
\begin{lstlisting}
git push
\end{lstlisting}\index{git!add}
If only you are working on the code, then you will be done at this point, but if others are working on it at the same time, then the {\tt push} command may fail because your new code conflicts with the code someone else has pushed to {\tt github} since you last pulled the repo. How to fix this is described next. 
\end{enumerate} 

Note that \lstinline+git log+ lets you view the commit history of your project (to review what has been done and why).
\index{git!log}

\subsubsection{Simple conflict resolution}

If your {\tt git push} command fails (and it will tell you if it has!) then you need to resolve the conflict. This is not so hard. 

\begin{enumerate}\index{git!resolve conflict}
\item After your {\tt git push} command has failed, issue the command
\begin{lstlisting}
git pull
\end{lstlisting}
This will get the latest versions of the files from the {\tt github} repo, compare them to your local copies, and attempt to resolve conflicts automatically where possible. Where auto-fixing is not possible, it will modify your local copies so that whenever there is a conflict your local files contain both your code and the conflicting code from the {\tt github} master, with clear marking of which is which. 
\item You check the conflicting files, changing the code to resolve any remaining flagged conflicts (often just selecting one or other of the alternative). 
\item Now redo the commit and push steps
\begin{lstlisting}
git commit -a -m"resolved conflict in favour of small end cracking"
git push
\end{lstlisting} \index{git!commit}\index{git!push}
If this fails because a teammate has meanwhile made another change, you really need to sort out your team communication - {\tt git} can't do that for you. 
\end{enumerate}

\subsubsection{Adding and deleting files with {\tt git}}

You can add as many files as you like to your local copy of the repo, but {\tt git} will take no notice of them until you tell it to. For example, suppose you created a file {\tt bar.r} which should be treated as part of the project, tracked and included in the master repo on {\tt qithub}
\begin{lstlisting}
git add bar.r
git commit -a -m"added file bar.r containing the gribbler code"
git push
\end{lstlisting}\index{git!add}
(You could omit the \lstinline+-a+ in this case, as the preceding \lstinline+add+ command will ensure \lstinline+bar.r+ is included in the next commit.)

You might also want to remove files, of course. 
\begin{lstlisting}
git rm bar.r
\end{lstlisting}
deletes {\tt bar.r} from the local copy, and will cause it to be removed from the master repo at the next {\tt commit} and {\tt push}. \index{git!delete file}

\subsection{More advanced use}

The above covers the most basic use of {\tt git}. It is sufficient for working on small projects in small teams on this course, and for understanding the basic principles of version control systems. We have not covered some key components of  
{\tt git} and {\tt github} that are extremely useful for larger projects. In particular we have not covered project {\em branches}. {\tt git} allows you to simultaneously have several versions of your project ({\em  branches}) in addition to the master version. These versions can all be tracked and backed up, just as the main master branch is. 

A typical use of branches is to code and work on complicated modifications - tracking and backing up the changes in those modifications, while not modifying the main project until is is clear that the modified code is really working and ready to be merged into the main project. 

To give a concrete example, suppose you maintain a large R package, which relies on code written in C and Fortran, and you decide that it would be advantageous to replace all the Fortran code with C code. This is a major undertaking, and you would not want to simply start work on the stable working code for the main package, since this will almost certainly break it initially. That would be a real nuisance if you then needed to deal with a minor bug in the original package, but had only the half completed unstable modified code available. Much better to create a branch of the project on which to develop the new C based code, only merging into the master branch, once everything in the revision is fully working.  
\index{git!branch}


\subsection{A simple {\tt git}/{\tt github} exercise}

At this stage, it is {\em essential} that you try out {\tt git} and {\tt github}. So before reading on try the following exercise. If you are not sure how to do any part, then refer back to the material above that you have just read.
\begin{enumerate}
\item Using a web browser, set yourself up a new repo on {\tt github}, and edit the default {\tt README.md} file on {\tt github} to say something interesting (making sure to save and commit the change).
\item Clone your {\tt github} repo to your local machine.
\item Add a file to your local repo, edit it and add it to the files tracked by {\tt git}.
\item Commit your edits and push the repo to {\tt github}.
\item Check that the repo on {\tt github} now contains your newly added file. 
\item As soon as you know someone else on the course to work with, try out sharing repositories, and resolving conflicts.
\end{enumerate}


\section{Programming for statistical data analysis}

Programming is the process of writing instructions to make a computer perform some task. The instructions have to be written in standard way that both the programmer and the computer can interpret. The rules and key words defining such a standard way of communicating define a computer language. There are many alternative languages designed for different classes of task. Here we will concentrate on the R language (and environment) for programming with data, which is widely used for the statistical analysis of data. A huge amount of statistical analysis software is written in R and is freely available. \index{R}

{\em Statistical} analysis of data is concerned with the analysis of data that are in some sense a random {\em sample} from a larger {\em population}. We want to learn about the population from the sample, without being misled by particular features of the sample that arose by chance as part of the random sampling process. The random sampling approach is powerful because: \index{statistical analysis}
\begin{enumerate}
\item it allows reliable conclusions with known levels of accuracy to be drawn without having to gather data from the whole population, which may be impossible, or prohibitively expensive.       
\item random sampling eliminates the unknowably large bias that occurs if we try to learn about the population from a non-random sample, replacing that bias with a random uncertainty of known magnitude.   
\end{enumerate}
Note that while `population' might mean something concrete like `population of people in the UK' it might be much more abstract like `the population of all experimental results that this experiment could have produced when replicated under the same conditions'. \index{random sample}\index{population}

To understand the difference between statistical and non-statistical data analysis consider data on reported cases of Covid-19 each day. Many charts of these data are produced, and analyses are performed, such as producing running averages or looking for trends in the data. None of these analyses are statistical, as no attempt is made to consider what population the case data might be viewed as a random sample of. They are certainly not a random sample of the people who have Covid-19 on a given day, and neither is it remotely clear how the number of cases relates to the number of people with Covid on a particular day.  The government and media present these data {\em as if} they were a random sample from the population of Covid cases, but this is simply misleading. Any decent applied statistician should be able to list several sources of bias likely to occur by treating them as such. 

In contrast, each week the UK Office for National Statistics publishes the results of testing a randomly selected sample of the UK population for Covid. The analysis includes estimates of trends and uncertainties, from a proper statistical analysis. Unsurprisingly the ONS analysis often appears to contradict the naive interpretations of the case data given by the media and government. More surprisingly the media and government seem to give more weight to the non-statistical analyses of case data than to the statistical analyses of the ONS data, despite the latter being essentially unbiased and of known accuracy, while the former have bias of unknown magnitude. Even more surprisingly, the UK appears to be unique in even conducting unbiased sampling to establish Covid prevalence.

The Covid cases example illustrates why statistical analysis software is dangerous. The easier software is to use, the easier it is to produce an analysis of data that resembles a statistical analysis in every superficial respect, but not in the key one that the data are in some sense a random sample from a population of interest.  Beware!

\subsection*{Exercise}

The following link is to an article in a national newspaper by two professors of statistics that appeared on 19th April 2020.
{\small \lstinline+https://www.theguardian.com/commentisfree/2020/apr/19/coronavirus-deaths-data-uk+} 
There is an astonishing statement in the second paragraph. Identify the statement and what is wrong with it.


\section{Getting started with R}

In this course we will concentrate on learning programming skills that as far as possible are transferable to other programming languages in addition to R. For this reason we will largely stick to programming using what is available with R itself, and we will avoid over-reliance on any particular set of add on packages. There is an add on package providing functions for almost any simple task you might want to accomplish in R. The fact that we will here examine how to programme tasks for which it would be simpler to find an add on package, is not through a desire to re-invent the wheel. The point is not to learn the quickest way to perform the particular task, but rather to illustrate how to programme. 

A word of warning. In much university work, getting something 80\% right is a first class performance. Unfortunately programming is not like that. 80\% right means 20\% wrong, and 20\% wrong computer code will likely result in your programme doing 0\% of what it is supposed to do. This fact calls for a more careful approach to working than is necessary for other topics.  

\subsection{A first R session \label{firstR}}

\index{R!basics}
To get started, let's do some trivial things in R. Start {\tt R} or {\tt Rstudio}, go to the terminal window and type
\begin{lstlisting}
a <- 2
\end{lstlisting}
You just created an object {\tt a} that contains the number 2. \lstinline+<-+ is the assignment operator. If you assign something to an object that does not yet exist, it is created. If you type the name of an object at the R terminal and nothing else, then the contents of the object gets printed. Exactly how this is done depends on the class of the object - more later. For example (\lstinline+>+ is just the R prompt, not something to type): \index{assignment operator}
\begin{lstlisting}
> a
[1] 2
\end{lstlisting}
We can make objects from other objects of course. For example
\begin{lstlisting}
> b <- 1/a
> b
[1] 0.5
\end{lstlisting}

R is a functional programming language: it is structured around functions that take objects as arguments and produce other objects as results. Many functions are built in to base R and even basic operators like \lstinline^+^ and \lstinline+/+ are actually implemented as functions. Suppose we want to create a function to take arguments $x$ and $y$ and return the value of $x \cos(y - k)$ where $k$ is a constant usually taking the value 0.5. Here is the code to define such a function object, named {\tt foo} in this case. \index{functions}
\begin{lstlisting}
foo <- function(x,y,k=0.5) {
  x * cos(y - k)
}
\end{lstlisting}
Curly brackets, \lstinline+{}+, enclose the R code defining how the function arguments are turned into its result. There can be as many lines of code as you like, but what ever is produced on the last line is taken as the object to be returned as the function result. \lstinline+k=0.5+ is used to indicate that if no value of {\tt k} is supplied, then it should take the default value of 0.5. This default system is used extensively by R and its add on packages. Let's try out {\tt foo}. \index{functions!argument}\index{functions!default argument}
\begin{lstlisting}
> foo(3,2)   ## using default k=0.5
[1] 0.2122116
> foo(3,2,0) ## setting k=0
[1] -1.248441
\end{lstlisting}

R is an interpreted language. Code\footnote{I'll use `code' to mean programmes and commands written in R (or indeed other computer languages).} is interpreted and executed line by line as it is encountered. This contrasts with compiled languages, where code is converted to binary instructions en masse, and this binary `machine code' (the native language of the computers processor) is then executed separately. R evaluates lines of code when they appear complete, and a line end has been encountered. If you want to split code over several lines then you need to be careful that the line does not appear complete before you meant it to be. e.g. suppose we want to evaluate $a=1+2+3+4+5$\index{interpreted language}
\begin{lstlisting}
> a <- 1 + 2 + 3 ## split line but it looks complete to R!
> + 4 + 5 
[1] 9
> a 
[1] 6  ## oops
> a <- 1 + 2 + 3 + ## split line that does not look complete
+ 4 + 5
> a
[1] 15 ## success
\end{lstlisting}
Several complete statements can be included on the same line by separating them with a `{\tt ;}'. e.g.
\begin{lstlisting}
a <- 2; b <- 1/a; d <- log(b)
\end{lstlisting}
Now leave R with the \lstinline+q()+ command. By default you will be asked if you want to save your workspace - usually you do not. You can avoid being asked by typing \lstinline+q("no")+. \index{quiting R}

\subsection{Dissecting a simple programming example}

Let us continue with an example of a simple data manipulation task in R. Suppose that we have a vector of 1 or 2 digit numbers and want to create a new vector of the individual digits, in order. For example if the original vector is $[12,5,23,2]$ the new vector should be $[1,2,5,2,3,2]$. Before trying this we need to write down {\em how} we are going to do it. For example:
\begin{enumerate}
\item Identify the number and locations of the double digit numbers in the original vector (and hence the length of the new vector).
\item Work out the locations of the `tens' digits in the new vector, compute and insert them.
\item Work out the locations of the `units' digits in the new vector and insert them. 
\end{enumerate}
The following is R code for one way of implementing this. It could be typed, line by line, into the R console, but it is better to type it into a file and then copy and paste to the R console, or {\tt source} the file into R, or run it in Rstudio. \index{which}\index{length}\index{rep}\index{integer division}\index{mod (integer remainder)}
\begin{lstlisting}
x <- c(10,2,7,89,43,1) ## an example vector to try out
ii <- which(x%/%10 > 0) ## indices of the double digits in x?
xs <- rep(0,length(ii)+length(x)) ## vector to store the single digits
iis <- ii+1:length(ii)-1 ## where should 10s digits go in xs?
xs[iis] <- x[ii]%/%10 ## insert 10s digits
xs[-iis] <- x%%10  ## insert the rest (units)
\end{lstlisting}  
Anything after \lstinline+#+ on a line is ignored by R, so is a comment. It is a good idea to use lots of these. Here is what the code does in detail, line by line.\index{concatenate}\index{TRUE}\index{FALSE}\index{index vector}\index{code!comments}
\begin{enumerate}
\item \lstinline+x <- c(10,2,7,89,43,1)+ creates an example vector to work on. The {\tt c} function takes the individual numbers supplied to it, and concatenates them into a single vector (it can also concatenate vectors). The results are stored in vector \lstinline+x+ using the assignment operator \lstinline+<-+. Notice how {\tt x} is created automatically by this assignment. Unlike in many computer languages, we do not have to declare {\tt x} first. 
\item \lstinline+ii <- which(x%/%10 > 0)+ creates a vector, {\tt ii}, of the indices of the double digit numbers in \lstinline+x+. It does this by computing the result of integer division by 10, for each element of {\tt x}, using the \lstinline+%/%+ operator, and then testing whether this result is greater than 0. The result of \lstinline+x%/%10 > 0+ will be a vector of {\tt TRUE} or {\tt FALSE} values of the same length as {\tt x}. For the given example it is $({\tt TRUE},{\tt FALSE},{\tt FALSE},{\tt TRUE},{\tt TRUE},{\tt FALSE})$. This vector is supplied directly to the {\tt which} function, which returns the indices for which the vector is {\tt TRUE} (1,4 and 5 for the example given). 
\item \lstinline^xs <- rep(0,length(ii)+length(x))^ creates a vector of zeroes into which the individual digits will be inserted, using the {\tt rep} function. {\tt rep} simply repeats its first argument the number of times specified in its second argument. The {\tt length} function returns the number of elements in an existing vector. We need {\tt xs} to be the length of {\tt x} plus an extra element for each double digit number.
\item \lstinline^iis <- ii+1:length(ii)-1^ creates a vector, {\tt iis}, containing the indices (locations) of the 10s digits in {\tt xs}. We have to account for the fact that each time we insert a digit, all the digits after move along one place, relative to where they were. So, the first 10s digit will occupy the same slot in {\tt x} and {\tt xs}, but the next 10s digit will be one element later in {\tt xs} than in {\tt x}, the next 2 elements later, and so on. \lstinline+1:length(ii)-1+ creates a sequence 0,1,2\ldots to add to {\tt ii} to achieve this. It uses the \lstinline+:+ operator --- if {\tt a} and {\tt b} are integers (and $a<=b$) \lstinline+a:b+ generates the sequence $a,a+1,a+2,\ldots,b$.\index{sequence}\index{vector!indexing}
\item \lstinline+xs[iis] <- x[ii]%/%10+ computes the 10s digits of the double digit numbers, indexed by {\tt ii}, using \lstinline+x[ii]%/%10+, and assigns them to the elements of {\tt xs} indexed by {\tt iis}.
\item \lstinline+xs[-iis] <- x%%10+ computes the units digit for all the numbers in {\tt x} using \lstinline+x%%10+ where \lstinline+%%+ is the operator computing the remainder after integer division. These digits are stored in the elements of {\tt xs} {\em not} reserved for 10s digits. That is, \lstinline+xs[-iis]+, all the elements {\em except} those indexed by {\tt iis}. 
\end{enumerate}
Make sure you really understand what this code is doing. Run it one line at a time in {\tt R}, and examine the result created by each line to make sure you understand exactly what is happening. To see what is in an R object, just type its name at the console, and it will be printed, by default. For example:
\begin{lstlisting}
> xs  ## > just denotes the R prompt here
[1] 1 0 2 7 8 9 4 3 1
\end{lstlisting}

The preceding example has quite a bit of R packed in, and we used quite a few R functions and operators on the way. R provides a large number of functions for a variety of tasks, and the available add on packages vastly more. You can't hope to learn them all, so it is essential to learn how to use the R help system. This is easy. For any operator of function you know the name of then just type \lstinline+?+ followed by the function name, or the operator in quotes. For example
\begin{lstlisting}
?which ## get the help for the 'which' function
?"<-"  ## get help for the assignment operator
help.start() ## launch html help in a browser
\end{lstlisting}  
\ldots the last option is often best if you are not sure what you are looking for. \index{R!help}

\subsection{A second simple example: data are not always numbers}

R vectors are not restricted to containing numbers. Character strings are another common data type that we can hold in a vector. For example \lstinline+x <- c("jane","bill","sue")+ creates a 3-vector, containing the 3 given character strings. Functions are provided for manipulating such character string data in various ways. For the moment suppose we want to achieve the same task as in the previous example, but for the case in which the numbers are supplied as character strings, and we want the separated digits as character strings too\footnote{As the point is to provide illustration of string handling, I'll resist the temptation to use {\tt as.numeric} to convert the character data to numbers, run the previous code, and then use {\tt as.character} to convert back.}.  \index{string}\index{character data}

We can use more or less the same approach as in the last section, but with one slight modification to the logic. For numbers it was easy to separate out 10s and units, with only the 2 digit numbers having a 10s digit. When the numbers are represented as character strings it is easier to separate out first and second digits, with only the two digit numbers having second digits. This means that rather than finding the indices of 10s digits, we'll find the indices of second digits. Here is the modified code: \index{nchar}\index{substr}\index{character string!substr}

\begin{lstlisting}
x <- c("10","2","7","89","43","1") ## example vector
ii <- which(nchar(x)>1) ## which elements of x are double digit?
xs <- rep("",length(ii)+length(x)) ## vector to store the single digits
iis <- ii+1:length(ii) ## where should second digit go in xs?
xs[iis] <- substr(x[ii],2,2) ## insert 2nd digits
xs[-iis] <- substr(x,1,1)    ## insert 1st digits
\end{lstlisting}
Line by line, here is what it does:
\begin{enumerate}
\item \lstinline+x <- c("10","2","7","89","43","1")+ example vector, as before, but now of character type.
\item \lstinline+ii <- which(nchar(x)>1)+ uses the {\tt nchar} function to count the characters in each element of {\tt x}.\\ \lstinline+which(nchar(x)>1)+ returns the indices for which the corresponding element of {\tt x} has $>1$ character.
\item  \lstinline^xs <- rep("",length(ii)+length(x))^ creates {\tt xs} as before, but this time it is a character vector.
\item \lstinline^iis <- ii+1:length(ii)^ computes the locations for second digits in {\tt xs}. Same idea as before, but second digits are all located one place after the 10s digits.
\item \lstinline+xs[iis] <- substr(x[ii],2,2)+. Function \lstinline+substr+ is used to obtain the 2nd character from each 2 digit element of {\tt x} ({\tt ii} indexes the 2 digit elements). \lstinline+substr(x[ii],2,2)+ extracts the characters between characters 2 and 2, from the elements of {\tt x[ii]} and returns them in a vector, which is copied into the appropriate locations in {\tt xs}.
\item \lstinline+xs[-iis] <- substr(x,1,1)+ the first digits are then inserted in the locations not reserved for second digits. 
\end{enumerate}

\subsubsection{Another simple text processing task}

The above example is a bit artificial. Let's consider a slightly more realistic task to undertake on character data. Suppose we have a string containing  some `poetry', and we want to count the number of words, tabulate the number of letters per word, count the number of words containing at least one `e' and mark all words containing an `a' and an `e' with a `*'. Here is R code to do this.\index{character string!nchar}\index{character string!strsplit}\index{strsplit}\index{character string!grep}\index{grep}\index{character string!paste}\index{paste}\index{character string!split}\index{character string!join}\index{character string!search}\index{tabulate}
\begin{lstlisting}
poem <- paste("Inside me is a skeleton, of this I have no doubt,",
        "now it's got my flesh on, but it's waiting to get out.")
pow <- strsplit(poem," ")[[1]] ## vector of poem words
n.words <- length(pow) ## number of words
freq <- tabulate(nchar(pow)) ## count frequency of n-letter words
ie <- grep("e",pow,fixed=TRUE) ## find `e' words
n.e <- length(ie)   ## number of `e' words
ia <- grep("a",pow,fixed=TRUE) ## find `a' words
iea <- ia[ia %in% ie] ## find words with `e' and `a'
pow[iea] <- paste(pow[iea],"*",sep="") ## mark `e' `a' words
paste(pow,collapse=" ") ## and put words back in one string.
\end{lstlisting}
Line by line it works like this:
\begin{enumerate}
\item The first line just creates a text string, {\tt poem}, containing the given text. The {\tt paste} function joins the two given strings into one string. The only reason to use it here was to split the string nicely across lines for these notes - we could just as well have written everything in one string to start with.
\item \lstinline+pow <- strsplit(poem," ")[[1]]+ splits {\tt poem} into a vector of its individual words, using\\ \lstinline+strsplit(poem," ")+, which splits the string in {\tt poem} at the breaks given by spaces, \lstinline+" "+. {\tt strsplit} can take a vector of strings as its first argument, and returns a {\em list} of vectors containing the split strings. In our case the list only has one element, which is what the \lstinline+[[1]]+ part of the code accesses. 
\item \lstinline+n.words <- length(pow)+ counts the words in {\tt poem}, since there is one element of {\tt pow} per word.
\item \lstinline+freq <- tabulate(nchar(pow))+ counts how many 1 letter, 2 letter, 3 letter, etc. words are in {\tt poem}. First function {\tt nchar} counts the letters in each word and then {\tt tabulate} tallies them up. Really we should have stripped out punctuation marks first - {\tt gsub} could be used to do this.
\item \lstinline+ie <- grep("e",pow,fixed=TRUE)+ finds the indices of the words containing an `e' using the {\tt grep} function. By default {\tt grep} can do much more complicated pattern matching using `regular expressions (see {\tt ?regex}). For the moment this is turned off using \lstinline+fixed=TRUE+ (otherwise characters like \lstinline+.+ and \lstinline+*+ are not matched as you might expect, but treated differently).
\item \lstinline+n.e <- length(ie)+ is the count of `e' words.
\item \lstinline+ia <- grep("a",pow,fixed=TRUE)+ finds the indices of the words containing an `a'.
\item \lstinline+iea <- ia[ia %in% ie]+ finds the indices of words containing an `a' and an `e'. \lstinline+ia %in% ie+ gives a {\tt TRUE} for each element of {\tt ia} that occurs in {\tt ie} and a {\tt FALSE} for each element of {\tt ia} that doesn't. Hence {\tt iea} will contain the indices of the words containing both letters.
\item \lstinline+pow[iea] <- paste(pow[iea],"*",sep="")+ adds a `*' to each word in {\tt pow} containing an `e' an an `a'. The {\tt paste} function is used for this, with \lstinline+sep=""+ indicating that no space is wanted between a word and `*'.
\item \lstinline+paste(pow,collapse=" ")+ is finally used to put the words in {\tt pow} back into a single string. It is the setting of {\tt collapse} to something non-NULL (here a space) that signals to {\tt paste} that this should happen.
\end{enumerate}

\subsection*{Exercises}

\begin{enumerate}
\item Look up the help page for {\tt gsub} function, and use {\tt gsub} to remove the commas and full stops from the poem before tabulating the length of words. 
\item For the original {\tt poem}, write R code to insert a line end character, \lstinline+"\n"+, after each comma or full stop, and print out the result using the {\tt cat} function.
\item Run the code \lstinline+set.seed(0);y <- rt(100,df=4)+. Use the {\tt hist} function to visualize {\tt y}. Now using the {\tt mean} and {\tt sd} functions to find the mean and standard deviation of {\tt y}, write code to remove any points in {\tt y} that are further than 2 standard deviations from the mean, and calculate the mean of the remainder.
\item Using your code from the previous section to write a function that will take any vector {\tt y} and compute its mean after removal of points more than {\tt k} standard deviations from the original mean of {\tt y}. Set the default value of {\tt k} to 2.

\end{enumerate}



\section{A slightly more systematic look at R}

When you start the {\tt R} programme, two important things are created. The first is an {\tt R} terminal, into which you can type commands written in the R programming language - this is visible. The second is an {\em environment}, known as the user workspace or global environment which will hold the objects created by your commands - this is invisible, but is there as an extendable piece of computer memory. In R an environment consists of a set of symbols used as the names of objects along with the data defining those objects (known together as a {\em frame}) and a pointer to an enclosing `parent' environment. R makes extensive use of sets of nested environments, but we don't need to go into too much detail on this aspect at the moment.

Like any computer language, the R language defines basic data structures, key words used to control programme flow, operators and functions, plus a set of rules about how these things are used and combined. This section introduces these. 

\subsection{Objects, classes and attributes}

Everything in R is an object living in an environment, including R commands themselves. Objects have {\em classes} which R can use to determine how the object should be handled (for example which version of the print function is appropriate for it).  Objects can also be given {\em attributes}: these are basically other objects that have been `stuck onto' the object and are carried around with it. A bit like a set of virtual post-it notes. Attributes are useful for storing information about an object.  For example, matrices in R have class \lstinline+"matrix"+ and a \lstinline+dim+ attribute. The \lstinline+dim+ attribute stores the number of rows and columns in the matrix. \index{R!objects} \index{matrix} \index{attributes}

\subsection{Data structures: vectors, arrays, lists and data frames}  

Let's look at the basic data structures you {\em must} know about to programme in R. When reading through this, try out the code yourself in R, and also try modifying it to check your understanding. 

\subsubsection{Vectors and recycling}

The most basic type of data structure in R is a vector (a one dimensional array). \lstinline+x[i]+ accesses element {\tt i} of vector {\tt x} ({\tt i} is a positive integer). Even scalars are just vectors of length 1 (so e.g. \lstinline+3[1]+ is perfectly valid and evaluates to 3, of course). As we have already seen, vectors can store data of different {\em types}: integer or real numbers (type `double'), character strings, logical variables. The class of vectors is simply determined by the type of thing they contain. Here are some basic examples. \index{R!class}\index{typeof}\index{data types}\index{vectors}
\begin{lstlisting}
> a3d <- c(TRUE,FALSE,FALSE) ## create an example logical vector
> class(a3d)     ## its class
[1] "logical"
> typeof(a3d)    ## its type
[1] "logical"
> a3d[2:3]       ## print its 2nd and 3rd elements 
[1] FALSE FALSE
> a3d[2] <- TRUE ## change the 2nd element
> a3d            ## print the resulting vector
[1]  TRUE  TRUE FALSE
> 
> bb <- 1:10  ## create numeric example
> class(bb)   ## check class
[1] "integer"
> bb[c(1,4,9)] <- c(.1,-3,2.2) ## change selected elements
> class(bb)   ## note automatic change of class 
[1] "numeric"
> typeof(bb)  ## how it is actually being stored
[1] "double"
> bb[3:8]     ## print elements 3:8 
[1]  3 -3  5  6  7  8
\end{lstlisting}
Whatever the type of a vector, some of its elements can always be set to {\tt NA} (not available), if required. Also numbers can take the values \verb+Inf+, \verb+-Inf+ and \verb+NaN+ (not a number e.g. $log(-1)$). \index{NA}\index{NaN}


Since vectors are the basic data structure, operators and most functions are also vectorized - they operate on whole vectors. For example given two vectors, {\tt a} and {\tt b} of the same length then \lstinline+c <- sin(a) * b+ actually forms \lstinline+c[i] <- sin(a[i]) * b[i]+ for all {\tt i} from 1 to the length of the vector. Similarly \lstinline^c <- a * b + 2^ actually forms \lstinline^c[i] <- a[i] * b[i] + 2^ for the same set of {\tt i} values. \index{vectorized code}

Notice how the scalar {\tt 2} got reused for each {\tt i} in that last example. So is a scalar more than just a length one vector after all? Actually no, {\tt 2} is being treated like any other vector --- R has a {\bf recycling rule} for vector arithmetic. Any operator that combines two vectors will recycle the values in the shorter vector as many times as required to match the length of the longer vector. So if a vector contains only one value, that one value is just recycled as often as needed. Here are a couple of examples  \index{recycling rule} \index{vectors}
\begin{lstlisting}
> a <- 1:4 # a 4-vector 
> b <- 5:6 # a 2-vector
> a*b      # multiplication with recycling
[1]  5 12 15 24
> b <- 5:7 # a 3 vector
> a*b      # multiplication with recycling
[1]  5 12 21 20
Warning message:
In a * b : longer object length is not a multiple of shorter object length
\end{lstlisting}

Vectors can be accessed and subsetted using logical vectors in place of indices. The vector should be the same length as the vector being accessed, and will be recycled if not. The elements of vectors can also be named, and can be accessed by name as well. Here are some illustrations (obviously names are not necessary for logical access):
\begin{lstlisting}
> x <- 1:5; names(x) <- c("fred","sue","bill","eve","bob") ## named vector
> x
fred  sue bill  eve  bob 
   1    2    3    4    5 
> x[c(TRUE,FALSE,TRUE,FALSE,FALSE)] ## subset using logical indexing vector
fred bill 
   1    3 
> x[c(TRUE,FALSE)] ## ... logical indexing vector recycled!
fred bill  bob 
   1    3    5 
> x[x>3] ## common case where logical vector generated in situ by a condition 
eve bob 
  4   5 
> x[c("sue","eve")] ## access by name
sue eve 
  2   4 
\end{lstlisting}

\subsubsection{Matrices and arrays}

While vectors are one dimensional arrays data, matrices are 2 dimensional arrays, and in general an array can have as many dimensions as we find useful. We can create an array with the {\tt array function}. For example:\index{array}
\begin{lstlisting}
a <- array(1:24,c(3,2,4))
\end{lstlisting}
creates a $3 \times 2 \times 4$ array, filling it with the numbers given in the first argument. To access the array we just give the dimension indices for the elements required. Leaving an index blank implies that we require all elements for that dimension. For example.
\begin{verbatim}
> a[3,2,3] ## element 3,2,3
[1] 18
> a[1:2,1,] 
     [,1] [,2] [,3] [,4]
[1,]    1    7   13   19
[2,]    2    8   14   20
\end{verbatim}  
notice how the second example accesses all elements for which the first index is 1 or 2, and the second index is 1.

Arrays are actually stored as vectors, with class \lstinline+"array"+ and a {\tt dim} attribute. The {\tt dim} attribute is a vector containing the length of each dimension (so $3,2,4$ above). Since the underlying storage is vector, we can also access it as such, we just need to know that the data are stored in the vector in `column major order'. For example if $d$ is the {\tt dim} attribute of a 3 dimensional array, $a$, then $a[i,j,k]$ is equivalent to $a[i + (j-1)d_1 + (k-1)d_1d_2]$. For example \index{array!vector access}
\begin{lstlisting}
> d <- dim(a) ## get the 'dim' attribute
> a[3+1*d[1]+2*d[1]*d[2]] ## vector access to a[3,2,3]
[1] 18
\end{lstlisting}
Notice that this is quite useful if you need to fill in or access several scattered individual elements of an array at once.

Two dimensional arrays, {\bf matrices}, play a central role in statistics. Data properly arranged for analysis are usually in matrix form with columns as variables and rows as observations (often referred to as `tidy data'), while many statistical methods rely heavily on matrix computations. Hence matrices are treated as a special class of array, with their own \lstinline+"matrix"+ class, a {\tt matrix} function used to create them, special operators for matrix multiplication and other matrix products, and functions implementing many matrix decomposition and matrix equation solving tasks. We will cover this in more detail later, but here is a quick example, which uses the matrix multiplication operator, \lstinline+%*%+.\index{matrix}\index{matrix!multiplication}\index{tidy data}
\begin{verbatim}
B <- matrix(1:6,2,3); B ## create a matrix (filled by col)
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6
> B[1,2] <- -1            ## change element 1,2
> a <- c(.3,-1.2,2.3)    ## a 3-vector
> B %*% a                 ## matrix multiplication
     [,1]
[1,] 13.0
[2,]  9.6
> B*a                     ## element wise multiplication with recycling!!
     [,1] [,2] [,3]
[1,]  0.3 -2.3 -6.0
[2,] -2.4  1.2 13.8
\end{verbatim}   
Do make sure you {\em really} understand the difference between the last two commands! It {\em really} matters.

\subsubsection*{Exercise}

Run the code 
\begin{lstlisting}
set.seed(5); n <- 2000
w <- runif(n)
A <- matrix(runif(n*n),n,n)
system.time(B <- diag(w) %*% A )
\end{lstlisting}
noting how long the last line takes to run. \lstinline+diag(w)+ forms the diagonal matrix with leading diagonal elements given by the elements of {\tt w} (try an $n=5$ example if that's not clear). By considering what actually happens when a diagonal matrix multiplies another matrix, and how matrices are stored in R, find a way of calculating {\tt B} using the recycling rule without using {\tt diag} or \lstinline+%*%+. Time this new code, and check that it gives the same answer as the old code (the {\tt range} function might be useful).


\subsubsection{Lists}

Lists are the basic building blocks of all sorts of complicated objects in R. Each item of a list can be any sort of R object, including another list, and each item can be named. Suppose {\tt a} is a list. Individual items can be accessed by number using \lstinline+a[[i]]+ where {\tt i} is an integer. If an item has a name, \lstinline+"foo"+ for example, it can also be retrieved that way using \lstinline+a[["foo"]]+ or \lstinline+a$foo+. We can also access sublists, also by name or number. e.g. \lstinline+a[c(2,4)]+ or \lstinline+a[c("foo","bar")]+ both produce 2 item lists. Note the difference: \lstinline+[[]]+ retrieves the item, \lstinline+[]+ retrieves a list of the required items (even if there is only one). Here is a simple example, which uses function {\tt list} to create an example list.\index{list}
\begin{lstlisting}
> stuff <- list(a=1:6,txt="furin cleavage site",l2 = function(x) log(x^2),
+          more = list(a="er",b=42))
> stuff[[1]]
[1] 1 2 3 4 5 6
> stuff[["l2"]]
function(x) log(x^2)
> stuff$a
[1] 1 2 3 4 5 6
> stuff[c(1,2)]
$a
[1] 1 2 3 4 5 6
$txt
[1] "furin cleavage site"
\end{lstlisting}  

\subsubsection{Data frames and factors -- statistical data structures}

R provides two types of data structure that are particularly useful for statistics: {\bf factor} variables are a special class of variables especially useful for data that consist of labels that serve to classify other data; {\bf data frames} are 2 dimensional arrays of data where the columns are variables, {\em which can be of different types}, and the rows are observations. 

{\bf Factors} are vectors of labels. For example a clinical trial might record the {\tt sex}, {\tt nationality} and {\tt treatment} received for each subject. All three are labels that categorize the subject. The different values that the label can take are known as {\em levels} of the factor (although they usually have no ordering, and it they do it is generally ignored). For example in a multi-centre vaccine trial {\tt nationality} might have levels {\tt "GB"}, {\tt "Brazil"}, {\tt "USA"}. In R factors are of class \lstinline+"factor"+ and the {\tt levels} function can be used to access their \lstinline+"levels"+ attribute. In fact the actual labels of a factor variable are only stored in the \lstinline+"levels"+ attribute, the variable itself is a set of integers indexing the levels. Why all this fuss? Because factor variables are very useful in statistical modelling, and setting them up this way makes them easy for modelling functions to handle. Here is a simple example.\index{data frame} \index{factor}
\begin{lstlisting}
> fac <- factor(c("fred","sue","sue","bill","fred"))
> class(fac)
[1] "factor"
> fac         ## default printing for class factor 
[1] fred sue  sue  bill fred
Levels: bill fred sue
> levels(fac) ## extract the levels attribute
[1] "bill" "fred" "sue" 
> as.numeric(fac)  ## look at the underlying coding
[1] 2 3 3 1 2
\end{lstlisting}

{\bf Data Frames} are basically matrices, in which the columns have names and can have different types (numeric, logical, factor, character, etc). They can be accessed like matrices, or like lists. They provide the best way of organising data for many types of statistical analysis. Here is a basic example 
\begin{lstlisting}
> dat <- data.frame(y = c(.3,.7,1.2),x = 1:3,fac = factor(c("a","b","a")))
> dat      ## a data.frame
    y x fac
1 0.3 1   a
2 0.7 2   b
3 1.2 3   a
> dim(dat) ## like a matrix
[1] 3 3
> dat$fac  ## and like a list!
[1] a b a
Levels: a b
\end{lstlisting}

\noindent {\bf Exercise:} Given the importance of factor variables in statistical modelling, it is worth some effort in understanding exactly how they are represented, as some basic tasks with factors are much easier if you understand this. First set up a simulated factor variable:
\begin{lstlisting}
a <- factor(sample(c("fred","george","sue","ann"),20,replace=TRUE)); a
\end{lstlisting}
You'll see that the levels of the factor have been ordered alphabetically. Since factors are just groups, and don't usually have a natural order that may be fine. However, plot functions, for example, will often use the level order to decide plotting order, and you might want to change that. Similarly statistical models often treat the first level differently from the others, so again you may want to control which it is. If you want the levels to be ordered differently then you can supply an order like this:\index{factor!levels}
\begin{lstlisting}
b <- factor(a, levels = c("ann","sue","fred","george")); b
\end{lstlisting}
Examine the underlying numeric values stored in {\tt a} and {\tt b} to make sure you understand exactly what has happened here. Finally, extract the levels of {\tt a} to a vector {\tt al} using the {\tt levels} function, and the numeric representation of {\tt a} to another vector, {\tt an}. Now use just {\tt al} and {\tt an} to reproduce a vector of the level labels for each entry in {\tt a}.
 

 


\subsection{Attributes}

As already mentioned, attributes are simply R objects attached to other objects, and carried around with them (a bit like a set of `post it' notes). They can be useful for storing things that are somehow properties of an object. We can find out about an objects attributes using the \lstinline+attributes+ function. For example
\begin{lstlisting}
> attributes(PlantGrowth) ## PlantGrowth is an R data set
$names
[1] "weight" "group" 

$class
[1] "data.frame"

$row.names
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
[26] 26 27 28 29 30

> A <- matrix(1:6,2,3) ## create a matrix
> attributes(A)        ## check its attributes
$dim
[1] 2 3
\end{lstlisting}
Single attributes can be queried, reset or created using the \lstinline+attr+ function. For example.
\begin{lstlisting}
> attr(A,"dim") ## query attibute
[1] 2 3
> attr(A,"foo") <- list(1:3,"fred") ## create or reset attribute
\end{lstlisting}
Attributes can be quite useful when your programming task requires an object that modifies a standard object (e.g. a matrix) by adding some extra information to it, but you still want all the standard functions that work with the object to function as for the standard object (e.g. matrix decompositions).  \index{attributes}\index{attr}  

\subsubsection{str and object structure}

Before moving on from data structures it is worth mentioning one useful function: {\tt str} prints a summary of the structure of any R object. This is a good way of seeing what an object actually looks like, since many classes of object have their own {\tt print} function, so that only rather limited information is printed when you simply type the object name at the command line. For example \index{str, structure}
\begin{lstlisting}
> str(dat)
'data.frame':	3 obs. of  3 variables:
 $ y  : num  0.3 0.7 1.2
 $ x  : int  1 2 3
 $ fac: Factor w/ 2 levels "a","b": 1 2 1
\end{lstlisting}

\subsection{Operators} 

Here is a boring table of some operators in R.

\begin{tabular}{cl|cl|cl}
Operator & Description& Operator & Description & Operator & Description\\ \hline
\verb+&+ & logical AND & \verb+|+ & logical OR & \verb+<+ & less than\\
\verb+<=+ & $\le$ &\verb+>+ & greater than & \verb+>=+ & $\ge$\\
\verb+==+ & testing equality & != & not equal & \verb+!+ & logical negation\\
\verb+<-+& assignment & \verb+=+ & assignment & \verb^+^& addition \\
\verb+-+ & subtraction & \verb+*+ & multiplication & \verb+/+ & division\\
\verb+^+ & raise to power & \verb+%/%+ & integer division & \verb+%%+& integer remainder (mod)\\
\verb+%*%+ & matrix multiplication & \verb+%x%+ & Kronecker product & \verb+%in%+ &logical match\\
\hline
\end{tabular}

\bigskip

\noindent These operators act element-wise on vectors, except for the final row, which are matrix operators, or in the case of \verb+%in%+ operates on vectors of different lengths (see \verb+?"%in%"+)  In addition \verb+||+ and \verb+&&+ are logical OR and AND operators that only evaluate the first element of their arguments, returning a single {\tt TRUE} or {\tt FALSE}. 

For example suppose we want to set all elements of vector {\tt x} to zero, for which $x[i]$ is between 1 and 2, or less than -2. \lstinline+x[(x < 2 & x > 1) | x < -2] <- 0+ does it. 

\noindent {\bf Exercise:} Suppose you have vectors \lstinline+x <- rnorm(20); z <- rnorm(20); y <- rnorm(20)+. Write (2 lines of) code to replace $x_i$ by $x_i^2$ if $z_i \ne 0$ and the following condition holds: $z_i$ is less than one or $y_i/z_i$ is negative. Note that R evaluates from left to right and stops evaluating logical expressions once their state is known unambiguously (i.e. if the statement is {\tt TRUE} or {\tt FALSE} irrespective of what comes next). Why might that matter here?

\subsection{Loops and conditional execution \label{sec:loops}}

Many programming tasks require that some operation is repeated many times for different values of some index, or require that the choice of which operation to carry out should depend on some conditions.

Let's start with conditional execution of code. Often this is coded in an implicit vectorized way. For example \lstinline+x[x<0] <- 0+ finds all the elements of \lstinline+x+ that are less than 0 and sets them to zero. i.e. we have set \lstinline+x[i]+ to zero only if originally \lstinline+x[i]+ was less than zero. But sometimes we need a more explicit way of doing this: {\tt if} and {\tt else} are used to achieve this. The basic form is \index{if, else} 
\begin{lstlisting}
if (logical condition) {
  ## one version of code
} else {
  ## another version of code
}
\end{lstlisting}  
Leaving out the {\tt else} means that nothing is done if the {\tt condition} is {\tt FALSE}. For example we could simulate a coin toss:\index{runif}
\begin{lstlisting}
if (runif(1)>.5) cat("heads\n") else cat("tails\n")
\end{lstlisting}
{\tt runif} generates uniform random numbers on $(0,1)$ and {\tt cat} is a simple function for printing (\lstinline+"\n"+ produces a line end). In R you will often see code with the following sort of structure\index{cat}\index{print}
\begin{lstlisting}
a <- if (a<0) 0 else a + 1
\end{lstlisting}
i.e. what is assigned to an object depends on a logical condition. {\tt if} statements can also be chained together of course. Here is a pointless example:
\begin{lstlisting}
if (runif(1)>.5) {
  cat("heads\n")
} else if (runif(1)>.7) {
  cat("tails\n")
} else cat("also tails\n")
\end{lstlisting}



Let's move on to looping. The {\tt for} loop is the most commonly used example. It repeats a set of R commands once for each element of some vector. The basic syntax is \index{for loop}
\begin{lstlisting}
for (a in vec) {
  ## some R code goes here
}
\end{lstlisting} 
which repeats the code between the brackets\footnote{you can drop the brackets if there is only one statement to execute at each iteration.}, for {\tt a} set to each value in {\tt vec} in turn. Here's a simple example\footnote{for which a loop is in no way needed: you should be able to replace this with a single call to {\tt cat}.}
\begin{lstlisting}
> vec <- c("I","am","bored")
> for (a in vec) cat(a," ")
I  am  bored
\end{lstlisting}
Perhaps the most common use of a {\tt for} loop is to loop over all integers between some limits. For example \lstinline+for (i in 1:10) {...}+ evaluates all the commands in \lstinline+{...}+ for $i=1,2,\ldots,10$. (Take care if you have programmed in other languages -- in R \lstinline+for (i in 1:0) {...}+ {\em will} execute the loop for 1 and then for 0). Here is an example iterating a chaotic map:
\begin{lstlisting}
n <- 100;p <- rep(NA,n); p[1] <- 0.1
for (i in 2:n) p[i] <- 3.7 * p[i-1] * (1 - p[i-1]) 
plot(1:n,p,type="l",xlab="i")
\end{lstlisting}
\eps{-90}{.6}{chaos.eps}

\noindent What if I wanted to stop the above loop if \lstinline+p[i]+ exceeded 0.92? I could do that by using the {\tt break} command inside the loop, to break out if the condition was met. i.e. \index{for loop!break}
\begin{lstlisting}
for (i in 2:n) { 
  p[i] <- 3.7 * p[i-1] * (1 - p[i-1])
  if (p[i]>.92) break
}   
\end{lstlisting}
Occasionally you might want to {\tt repeat} indefinitely until a condition is met that causes you to {\tt break}. The basic structure is this \index{repeat loop}
\begin{lstlisting}
repeat {
  some code
  if (condition) break
}
\end{lstlisting}
Another possibility is a {\tt while} loop, something like \index{while loop}
\begin{lstlisting}
while (condition) {
  R code
}
\end{lstlisting}
but {\tt for} loops are the most commonly used.

Note that for a vector oriented language like R, these sorts of tasks can often be accomplished efficiently by exploiting the fact that vectorized operations loop over vectors automatically, while vectors can also be subsetted so that we act only on the parts meeting some condition. Vectorized operations are usually much faster than explicitly coded loops, so generally it is a good idea to only write explicit loops when vectorization is not possible, or if the operations to be conducted at each loop iteration are expensive enough that the overhead of explicit looping is unimportant. For example, you should never write \lstinline+for (i in 1:length(x)) y[i] <- x[i]+ since \lstinline+y <- x+ does the same thing more efficiently, with less code:
\begin{lstlisting}
> n <- 10000000; x <- runif(n) ## simulate 10 million random numbers 
> system.time(y <- x)   ## time a vector copy
   user  system elapsed 
  0.000   0.000   0.001 
> system.time(for (i in 1:n) y[i] <- x[i]) ## time the equivalent copy loop
   user  system elapsed 
  0.538   0.028   0.567
\end{lstlisting}
In an interpreted language like R, there is more interpreting what to do than actual doing in the loop case. 
\index{vectorized code}

\noindent {\bf Exercise:} Suppose you want to find the solution to the equation $f(x)=0$ where $f(x) = \log(x)-1/x$. A reasonable approach is bisection search. Start with an interval $[x_0,x_1]$ such that $f(x_0)<0$ and $f(x_1)>0$.\footnote{These inequalities would be reversed for a decreasing function, of course} Defining $x_t = (x_1+x_0)/2$, set $x_1 = x_t$ if $f(x_t)>0$ and $x_0=x_t$ otherwise. This process can be repeated until $f(x_t)$ is close enough to zero (or until $x_1-x_0$ is small enough). Write code to find the solution to $f(x)=0$ by bisection search, given that the solution (root) is somewhere in $[.1,10]$ . Function {\tt abs} finds the absolute  
value of a function, by the way.\index{bisection search}\index{root finding}\index{absolute value}

\subsection{Functions \label{sec:function}}

Functions were introduced in Section \ref{firstR}, but some more detail is required to write them effectively.  Formally a function consists of an argument list, a body (the code defining what it does), and an environment (which is the environment where it was created). Generally, functions take objects as arguments and manipulate them to produce an object, which is returned. There are two caveats to this general principle. \index{functions}
\begin{enumerate}
\item A function may have side effects, such as printing some output to the console or producing a plot. Indeed a function may only produce a side effect, and no return object. Generally side effects that modify objects that are external to the function are to be avoided, if code is to be clean and easy to debug. 
\item A function may make use of objects not in its argument list: if R encounters a symbol not in the function argument list and not previously created within the function, then it searches for it, first in the environment in which the function was {\em defined}\footnote{This is known as `lexical scoping', because the parent environment of the \index{R!lexical scoping} function is where it was written down.} (which is not necessarily the environment from which it was called). If that fails it looks in the environments returned by function {\lstinline+search()+}. A benign use of this mechanism is to call other functions not in a function's argument list, or to access constants such as those stored in {\lstinline+.Machine+}. Using this mechanism to provide a function with other objects that you have created is generally bad practice, because it makes for complex hard-to-debug code. Generally all objects that a function needs should be provided as its arguments. If this gets unwieldy, then group the arguments into a smaller number of list arguments.   
\end{enumerate}

Here is an example of a function definition. It generalises one-to-one real functions with power series representations to symmetric matrices using the following idea. The eigen-decomposition of symmetric matrix ${\bf A}$ is ${\bf A} = {\bf U}{\bm \Lambda}{\bf U}\ts$ where the columns of ${\bf U}$ are eigenvectors of $\bf A$ and ${\bm \Lambda}$ is the diagonal matrix of eigenvalues. The generalization of a function, $f$ is then $f({\bf A}) = {\bf U} f({\bm \Lambda}){\bf U}\ts$, where $f$ is applied element wise to $\bm \Lambda$'s diagonal. \index{functions!definition}\index{functions!argument}\index{eigen}\index{matrix!multiplication}
\begin{lstlisting}
mat.fun <- function(A,fun=I) {
  ea <- eigen(A,symmetric=TRUE)
  ea$vectors %*% (fun(ea$values)*t(ea$vectors)) ## note use of re-cycling rule!
}
\end{lstlisting}\index{recycling rule}
`{\lstinline+function(A,fun=I)+}' indicates that a function is to be created with arguments {\lstinline+A+} and {\lstinline+fun+}. In this case the function created is given the name {\lstinline+mat.fun+}, but functions are sometimes used without being given a name (for example, in the arguments to other functions). The argument list gives the names by which the function arguments will be referred to within the function body. Arguments may be given default values to be used in the event that the function is called without providing a value for that argument. This is done using \lstinline$name = default$ in the argument list. \lstinline+fun=I+ is an example of this, setting the default value of {\lstinline+fun+} to the identity function. \index{R!functions!argument list}

Next comes the body of the function given by the R expressions within the curly brackets \lstinline+{ ... }+ (if the function body consists of a single expression, then the brackets are not needed). The function body can contain any valid R expressions. The object created on the last line of the function body is the object returned by the function. Alternatively the object can be returned explicitly using the {\lstinline+return+} function. For \lstinline+mat.fun+, the eigen decomposition of the first argument is obtained and then used to produce the generalised version of {\lstinline+fun+}. \index{R!functions!body}

Now let us use the function, with a random matrix. First a sanity check that the identity function is correct:
\begin{lstlisting}
> set.seed(1)
> m <- 3; B <- crossprod(matrix(runif(m*m),m,m)) ## example matrix
> range(B - mat.fun(B)) ## check input matches output
[1] -2.220446e-16  6.661338e-16
\end{lstlisting}
This confirms that the output matches the first argument when the default identity function is used.

An aside: \index{precision!finite} why was the difference between the input and output not exactly 0? Because real numbers can only be stored in a computer to a finite precision, here equivalent to about 16 places of decimals. This inevitably means that arithmetic computations on real numbers are not exact --- rounding errors accumulate as calculations are performed. \index{rounding errors} A great deal of mathematical work has gone into minimising these numerical errors in matrix computations, but they can not be eliminated. \index{precision!machine} In this case we can say that the input and output matrices are identical {\em to machine precision}. You can get an idea of the size of number that counts as indistinguishable from zero by typing \lstinline+.Machine$double.eps+. Note, however that what counts as machine zero is relative to the size of numbers involved in a calculation. For example, here is what happens if {\tt B} is multiplied by $10^{10}$
\begin{lstlisting}
> B <- B * 1e10
> range(B - mat.fun(B))
[1] -3.814697e-06  3.814697e-06
\end{lstlisting}

Back to functions! What actually happened when the function was called (by \lstinline+mat.fun(B)+). R first matches the arguments of the function to those actually supplied, adopting a rather permissive approach to so doing. First it matches on the basis of exact matches to argument names (`{\lstinline+A+}' and `{\lstinline+fun+}' in the example). This does not mean that R is looking for {\lstinline+B+} to be called {\lstinline+A+} in the example; rather it is looking for statements of the form \lstinline+A=B+, specifying unambiguously that object {\lstinline+B+} is to be taken as argument `{\lstinline+A+}' of {\lstinline+mat.fun+}. After exact matching, R next tries partial matching of names on the remaining arguments; for example \lstinline+mat.fun(B,fu=sqrt)+ would cause the {\lstinline+sqrt+} function to be taken as the object to be used as argument {\lstinline+fun+}. After matching by name, the remaining arguments are matched by position in the argument list: this is how R has actually matched {\lstinline+B+} to {\lstinline+A+} earlier. Any unmatched argument is matched to its default value. \index{functions!argument matching}\index{functions!partial matching}

R next creates an {\em evaluation frame}: an extendible piece of memory in which to store copies of the function arguments used in the function, as well as the other objects created in the function. This evaluation frame has the environment of the function as its parent (which is the environment where the function was defined, remember). \index{functions!evaluation frame}

Having matched the arguments, R does not actually evaluate them immediately, but waits until they are needed to evaluate something in the function body: this is known as {\em lazy evaluation}. Evaluation of arguments takes place in the environment from which the function was called, except for arguments matched to their default values, which are evaluated in the function's own evaluation frame. \index{evaluation!lazy}

Preliminaries over, R then evaluates the commands in the function body, and returns a result. 

Notice that arguments are effectively copied into the function's evaluation frame, so nothing that is done to a function argument within the function has any effect on the object that supplied that argument `outside' the function. Within the body of {\lstinline+mat.mod+} argument {\lstinline+A+} could have been replaced by some poetry, but the matrix {\lstinline+B+} would have remained unaltered.

Here is an example of calling {\lstinline+mat.mod+} to find a matrix inverse:
\begin{lstlisting}
> mat.fun(A = B, fun = function(x) 1/x)
          [,1]      [,2]      [,3]
[1,] 10.108591 -2.164337 -3.070143
[2,] -2.164337  4.192241 -2.707381
[3,] -3.070143 -2.707381  4.548381
\end{lstlisting}
In this case both arguments were supplied by their full name, and a function definition was used to supply argument {\lstinline+fun+}. 

\subsection{The `{\tt \ldots}' argument}

Functions can also have a special argument `\lstinline$...$', which is used to create functions that can have variable numbers of arguments. It is also used to pass arguments to a function that may in turn be passed on to other functions, without those arguments having to be declared as arguments of the calling function: this is useful for passing arguments that control settings of plotting functions, for example. \index{functions!variable number of arguments}
\index{functions!\ldots argument}

Any arguments supplied in the call to a function, that are not in the argument list in the function definition, are matched to its `\lstinline$...$' argument, if it has one.\footnote{This has the slightly unfortunate side effect that mistyped argument names do not generate obvious warnings.} The elements of `\lstinline$...$' can be extracted into a list, to work with, but here we will just consider how to use `\lstinline$...$' to pass arguments to a function called inside another function {\em without having to know the names of the arguments in advance}. 

To make the problem concrete, suppose that we want to use our \lstinline$mat.fun$ function with the function argument defined by 
\begin{lstlisting}
foo <- function(x,a,b) x/a + b
\end{lstlisting}
Obviously {\tt foo} can not be used directly with \lstinline$mat.fun$ as the code assumes that whatever is passed in as {\tt fun} only has one argument. We certainly don't want to write a new version of \lstinline$mat.fun$ just for 3 argument functions with arguments called {\tt a} and {\tt b}. That's where `\lstinline$...$' comes to the rescue: we'll use it to pass {\tt a} and {\tt b}, or any other extra arguments (or none) to {\tt fun}, as follows
\begin{lstlisting}
mat.fun <- function(A,fun=I,...) { ## ... allows passing of extra arguments
  ea <- eigen(A,symmetric=TRUE)
  ea$vectors %*% (fun(ea$values,...)*t(ea$vectors)) ## to be passed to fun
}
\end{lstlisting}
And it works\ldots
\begin{lstlisting}
mat.fun(B,fun=foo,a=2,b=3)
           [,1]       [,2]       [,3]
[1,] 2685660117 4154166277 4285541075
[2,] 4154166277 8363105089 7782109902
[3,] 4285541075 7782109902 8624247833
\end{lstlisting}

One irritation is worth being aware of. 
\begin{lstlisting}
ff <- function(res=1,...) res;f(r=2)
\end{lstlisting}
will return the answer 2 as a result of partial matching of argument names, even if you meant \lstinline+r+ to be part of the `\lstinline+...+' argument. It is easy to be caught out by this. If you want `\lstinline+...+' to be matched first, then it has to precede the arguments it might be confused with. So the following gives the answer 1:
\begin{lstlisting}
ff <- function(...,res=1) res;f(r=2)
\end{lstlisting}

\noindent {\bf Exercise:} turn your code from the section \ref{sec:loops} exercise into a general root finding function that takes an initial interval, a function (which may depend on some extra constants) the root of which is to be found (that is the value of its argument at which the function is zero). Try your function out with the function from the previous exercise, and with $g(x) = x^b - e^{ax} + 2$ where $b \ge 1$ and $a>0$. Make sure that your function checks that the initial supplied interval brackets the solution, and can deal with increasing or decreasing functions. Function {\tt stop} can be used to jump out of a function and print an error message if a problem is detected. \index{error}\index{stop} If your function does not work as expected, try stepping through it using the {\tt mtrace} from the {\tt debug} package to figure out where the error is.   

\subsection{Planning and coding example: plotting an empirical CDF}

The cumulative distribution function, $F$, of a random variable $X$ is defined as $F(x) = \text{Pr}(X\le x)$. Hence if we have a random sample $x_1, x_2, \ldots, x_n$ of observations of $X$ we can define an {\em empirical} cumulative distribution function
$$
\hat F(x) = \frac{1}{n}\sum_{i=1}^n \mathbb{I} (x_i \le x)
$$
where $\mathbb{I}$ is the indicator function. Suppose we want to write a function to take a sample of data in a vector $\bf x$ and plot its CDF. There are many ways to do this, so the first thing we have to do is to write down a plan. Usually this will involve jotting down ideas and maybe sketches on a piece of paper. Here are my notes for this task\ldots\index{CDF}\index{planning}

\eps{0}{.7}{cdf-plan.pdf}

The sketch of the CDF  is me trying to get my head around what the definition of $\hat F$ really means, and whether the $x_i$ values are at the start of a step or just before it (the former). As a result of this figuring out, I get to a plan that is a bit better than just evaluating the formula for $\hat F$ for a finely spaced sequence of $x$ values. Obviously more complicated tasks may involve several sheets of paper, false starts, and the need to summarize the design at the end. Code that involves no sheets of paper and none of this planning process is often poorly designed, error prone and slow to write. 

To implement the design I used built in functions, {\tt sort}, {\tt plot} and {\tt lines}. Look up the help pages for these. {\tt plot} is a high level function for producing scatter plots, while {\tt lines} is a lower level function for adding lines to an existing plot ({\tt points} is an equivalent for adding points). For a little extra flourish I used {\tt expression} for defining the y axis label --- see {\tt ?plotmath} for how that works. Here is the code -- makes sure you understand each line. \index{plot}\index{sort}\index{lines}\index{points}
\begin{lstlisting}
ecdf <- function(x) {
## function to plot empirical cdf of sample x
  n <- length(x)
  p <- 1:n/n ## create cumulative probability vector 
  xs <- sort(x) ## sort the x values
  ## plot the x, p points
  plot(xs,p,pch=19,cex=.5,xlab="x",ylab=expression(P(X<=x)))
  ## now add in the lines of constant probability between x values
  lines(c(2*xs[1]-xs[n],xs[1]),c(0,0))
  for (i in 1:(n-1)) lines(c(xs[i],xs[i+1]),c(p[i],p[i]))
  lines(c(2*xs[n]-xs[1],xs[n]),c(1,1))
} ## ecdf

## test it
set.seed(8);
x <- rgamma(50,shape=3,scale=2) ## test data
ecdf(x)
\end{lstlisting}
\eps{-90}{.4}{ecdf.eps}
\subsubsection{Avoiding the loop and vectorization}
You will notice that {\tt ecdf} uses a {\tt for} loop for plotting each line segment in turn, but actually if you check the help page {\tt ?lines} you will see that {\tt lines} has a mechanism for plotting multiple separate line segments all at once - we just separate the co-ordinates defining the lines with an {\tt NA} (not available) to indicate that the points either side of the {\tt NA} should not be joined by a line. Since vectorized code is usually faster than loop code, lets think about how to uses this feature.
\begin{enumerate}
\item For $n$ data we have $n+1$ line segments, each defined by 2 $x,y$ points, and between each line segment we will need {\tt NA} values ($n$ in total). 
\item So let's create vectors, {\tt p1} and {\tt x1} each of length $3n+2$ to contain the line segment definitions and breaks, and fill them in.
\end{enumerate}
Here is the code that could replace everything after \lstinline+## plot the x, p points+ in {\tt ecdf}.
\begin{lstlisting}
p1 <- x1 <- rep(NA,3*n+2) ## create p1 and x1 with NAs everywhere
p1[1:n*3+2] <- p1[1:n*3+1] <- p ## fill in the step heights
x1[1:n*3+1] <- xs ## step start
x1[1:n*3+2] <- c(xs[-1],2*xs[n]-xs[1]) ## step ends
p1[1:2] <- 0 ## initial prob is zero
x1[1:2] <- c(2*xs[1]-xs[n],xs[1]) ## x co-ords for zero prob
plot(xs,p,pch=19,cex=.5,xlab="x",ylab=expression(P(X<=x)))
lines(x1,p1)
\end{lstlisting}\index{vectorized code}
Notice how this code is entirely vectorized --- no repeating of code $n$ times in a loop. It is therefore faster to run. But also notice that it is slower to read and understand. Also, with 500 data points the vectorized version of {\tt ecdf} is about 50 times faster than the original version. But it still takes only 0.7 seconds to run on my machine. So was it worth the effort of speeding up? That really depends on how often the function will be used and on what size data set. These are common trade-offs, which mean that you should beware of making a fetish out of vectorizing your code. Always vectorize if it is easy to write and understand the vectorized code, but otherwise think about whether the computer time savings will justify the effort.  

\noindent {\bf Exercise:} One approach to getting a smoother approximation to the CDF is to recognize that we are effectively using the sorted data to approximate the quantiles of the distribution, and that there is uncertainty in these quantile estimates - the quantiles could just as well have been  a bit higher or lower. This suggests that we could generate many replicate datasets in which we add noise to the original data, combine these simulated datasets and plot their CDF, to get a smoothed CDF. Modify the (vectorized) {\tt ecdf} function to do this, by generating 100 replicated datasets, each obtained by adding normal noise ({\tt rnorm}) to the original data, with standard deviation given by $h$ (default 1), an argument of the modified function. You probably want to change the default plot character to \lstinline+"."+.   


\subsection{Useful built-in functions}

R has a huge number of useful built in functions. This section is about how to find them.  Recall that R's extensive help system can be accessed by typing {\lstinline+help.start()+} at the command prompt, to obtain help in navigable HTML form, or by typing {\lstinline+?foo+} at the command line, where {\lstinline+foo+} is the function or other topic of interest. \index{functions!built in to R}

\begin{center}
\begin{tabular}{ll}
{Help topic} & {Subject covered} \\ \hline
{\tt ?Arithmetic} & Standard arithmetic operators \\
{\tt ?Logic} & Standard logical operators \\
{\tt ?sqrt} & Square root and absolute value functions \\
{\tt ?Trig} & Trigonometric functions ({\lstinline+sin+}, {\lstinline+cos+}, etc.) \\
{\tt ?Hyperbolic} & Hyperbolic functions ({\lstinline+tanh+}, etc.) \\
{\tt ?Special} & Special mathematical functions ($\Gamma$ function, etc.)\\
{\tt ?pgamma} & Partial gamma function\\
{\tt ?Bessel} & Bessel functions\\
{\tt ?log} & Logarithmic functions\\
{\tt ?max} & Maximum, minimum and vectorised versions\\
{\tt ?round} & Rounding, truncating, etc.\\
{\tt ?distributions} & Statistical distributions built into R\\ \hline 
\end{tabular} 
\end{center}

The {\lstinline+?distributions+} topic requires some more explanation. R has built-in functions for the {\lstinline+beta+}, {\lstinline+binomial+}, {\lstinline+cauchy+}, {\lstinline+chisq+}uared, {\lstinline+exp+}onential, {\lstinline+f+}, {\lstinline+gamma+}, {\lstinline+geom+}etric, {\lstinline+hyper+}geometric, {\lstinline+lnorm+}al (log-normal), {\lstinline+multinom+}ial, {\lstinline+nbinomial+} (negative binomial), {\lstinline+norm+}al, {\lstinline+pois+}son, {\lstinline+t+}, {\lstinline+unif+}orm and {\lstinline+weibull+} distributions. The R identifying names for these are shown in {\lstinline+courier+} font in this list. \index{R!distributions}\index{R!random numbers}

For each distribution, with name {\lstinline+dist+}, say, there are four functions:\index{random deviates}
\begin{enumerate}
\item {\lstinline+ddist+} is the probability (density) function of {\lstinline+dist+}.
\item {\lstinline+pdist+} is the cumulative distribution functions of {\lstinline+dist+}.
\item {\lstinline+qdist+} is the quantile function of {\lstinline+dist+}.
\item {\lstinline+rdist+} generates independent pseudorandom deviates from {\lstinline+dist+}. 
\end{enumerate}

\section{Simulations I: stochastic models and simulation studies}

One major class of statistical programming task involves {\em stochastic simulation}. Simulating processes that involve random numbers. Strictly speaking {\em pseudorandom} numbers. There are several main sorts of simulation that we might need to undertake.
\begin{enumerate}
\item Simulate from a stochastic model. e.g. a model of disease spread, or a model of football teams competing, or a model of how shoppers will move around a store.
\item Simulate the process of sampling data from a population, and analysing it using a statistical method, in order to assess how well the statistical method is working. 
\item Simulate the process of sampling data directly, as part of a statistical analysis method.
\item Simulate from the posterior distribution of the parameters in a Bayesian data analysis.
\end{enumerate}  
This section will take a look at examples of the first two. 

\subsection{Random sampling building blocks}

There are a couple of sorts of random sampling that we might want to do:
\begin{enumerate}
\item Sample observations from a set of data. 
\item Sample random deviates from some particular probability distribution. 
\end{enumerate}

\subsubsection{Sampling data}
Suppose {\tt x} is a vector and we would like to sample {\tt n} of its elements, at random, without replacement. {\tt sample} is a function for doing this. For example, let's draw a random sample of size 10 from the integers $1,2,\ldots,100$.
\begin{lstlisting}
> set.seed(0) ## just so you can reproduce what I got
> sample(1:100,10)
 [1]  14  68  39   1  34  87  43 100  82  59
\end{lstlisting}
This sampled each number with equal probability, {\em without replacement}. So each observation had the same chance of occurring in the sample, but could do so only once. This is the sort of sampling that you get if you put numbered balls in a bag, shake them up and then draw out a sample from the bag without looking. \index{sample}

Notice the use of \lstinline+set.seed()+. We can not generate truly random numbers, but only sequences of numbers that appear indistinguishable from random using any statistical test you might apply. What ever sort of random number we eventually want, R starts by generating sequences that appear to be uniform random deviates from $U(0,1)$, and then transforming these. These uniform sequences are generated using a {\em random number} generator, and are in fact completely deterministic. If we repeatedly start the sequence off from the same state, it will generate the same sequence every time. Setting the random number generator to a particular state is known as {\em setting the seed}. To fix ideas, here is how it works for the basic uniform generator
\begin{lstlisting}
> set.seed(3);runif(5) ## 5 random numbers after setting seed
[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007
> runif(5) ## another 5 - all different
[1] 0.6043941 0.1246334 0.2946009 0.5776099 0.6309793
> set.seed(3);runif(5) ## and after setting the seed to 3 again - same as first time
[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007
\end{lstlisting}

Let's get back to sampling. We might also want to sample {\em with replacement}. This is as if we sampled numbered balls from a bag, as before, but this time after noting the number we return the ball to the bag and give the bag a shake, before drawing the next ball.
\begin{lstlisting}
> sample(1:100,10,replace=TRUE)
 [1] 51 97 85 21 54 74  7 73 79 85
\end{lstlisting}
Notice that 85 occurred twice in this sample. Obviously we can sample any vector in this way, not just a vector of integers, but sampling integers like this is especially convenient if you want to sample whole rows of data from a matrix or data.frame. You sample the row indices, and then use these to extract the sampled rows. For example, suppose we want a random sample of 30 rows of data frame, {\tt dat}:
\begin{lstlisting}
ii <- sample(1:nrow(dat),30) ## 'nrow' returns the number of rows of 'dat'
dat1 <- dat[ii,]             ## 30 rows selected at random from 'dat'
\end{lstlisting}
It is also possible to sample with different probabilities of sampling each observation. For example let's sample the integers $i=1,2,\ldots,100$ each with probability $\propto 1/i$.  
\begin{lstlisting}
> sample(1:100,10,prob=1/1:100)
 [1] 13  1  5 19 24 14 12 17 20 63 
\end{lstlisting}
Note that we are sampling with probability proportional to {\tt prob} -- {\tt sample} will normalise the probabilities so that they sum to one internally, before using them. In this example we only got one observation $>25$. Does that seem as expected? Let's work out the probability of sampling an observation $<25$ as a sanity check\ldots 
\begin{lstlisting}
> sum(1/1:24)/sum(1/1:100)
[1] 0.7279127 ## seems fine!
\end{lstlisting}

\subsubsection*{Example: the birthday problem}

A standard toy problem in probability is the birthday problem. What is the probability of at least 2 people in a group of 30 sharing the same birthday? Let's investigate this by simulation. As usual we start by planning. For each group we want to sample 30 birthdays at random from the numbers $1 \ldots 366$, with one of the numbers occurring only in leap years and therefore having 1/4 of the probability of the others. For each group we can then find the unique birthdays, and see how often there are fewer than 30 of them. So  
\begin{enumerate}
\item Randomly sample the $n $ lots of 30 birthdays, and put them in an $n \times 30 $ matrix.
\item For each matrix row, count the unique birthdays.
\item Find the proportion for which that count is $<30$ 
\end{enumerate} 
Here is an implementation, it uses the {\tt apply} functions to apply a function counting the unique birthdays to each row of the birthday matrix. The second argument of {\tt apply} gives the dimensions of the first argument over which the function should be applied. Here the first argument is a matrix, so I give one as the second argument, to apply over rows.
\begin{lstlisting}
n <- 1000000 ## number of groups
bd <- matrix(sample(1:366,30*n,replace=TRUE,prob=c(rep(4,365),1)),n,30) ## birthdays
p <- mean(apply(bd,1,function(x) length(unique(x)))!=30)
\end{lstlisting}\index{apply}
It gives the answer p=0.706. Obviously there is some stochastic variability in that answer - we could work out the standard error of the answer using the variance of a Bernoulli random variable: \lstinline+(p*(1-p)/n)^.5+ is about 0.0005. Or if we are lazy we could just run the simulation repeatedly to assess the variability of the answer. 

There are two rather appealing features of this way of answering the question. Firstly, it was very easy, in a way that most people do not find the probability calculation based answer. More importantly, it is very easy to generalize to situations for which the exact answer would be very difficult to work out. For example, births are not spread out evenly throughout the year, so the probabilities of the different birthdays are not all equal. Given appropriate data it would be easy to adjust the probabilities in the sampling step, and get an adjusted probability for that case too.  

\noindent {\bf Exercise:} {\em Bootstrapping} is the process of resampling with replacement from a set of data, in order to simulate the process of sampling the data from the population, in order to assess the sampling variability in quantities calculated from the data (statistics). In R \lstinline+morley$Speed+ contains the measured speed of light in km/s above 299000 km/s, in 100 runs of the Michaelson Morley experiment. Sample these data with replacement to generate 1000 bootstrap replicate datasets. Find the mean of each replicate, and then use the {\tt quantile} function to find the range of the middle 95\% of the bootstrap means. This range is sometimes known as a {\em bootstrap percentile confidence interval}. You may find {\tt colMeans} or {\tt rowMeans} useful. \index{rowMeans}\index{colMeans} \index{bootstrap}

\subsubsection{Sampling from distributions}

R has functions for simulating random variables from a wide variety of basic distributions: see {\tt ?distributions} for an overview. The functions all start with an {\tt r} followed by the distribution name. Their first argument is the number of values to simulate, and remaining arguments are vectors of distribution parameters. For example, if I would like 3 normal random deviates, with means 1, 3.2 and -0.5 and standard deviations 1, 1 and 2.5 then the following produces them\index{random numbers}
\begin{lstlisting}
> rnorm(3,mean=c(1,3.2,-.5),sd=c(1,1,2.5))
[1] 0.4268462 2.1288700 2.3638236 ## you should get different answers here!
\end{lstlisting}
An immediate use for simulation is to quickly get approximate probabilities that may be awkward to compute otherwise. 

For example the time from infection to onset of Covid-19 symptoms follows a log normal distribution with log scale mean and sd of 1.63 and 0.50. Meanwhile one estimate has the distribution of time from symptom onset to death estimated to be a gamma distribution with shape parameter 4 and scale parameter 4.2. Assuming independence of the 2 durations, what does the distribution of time from infection to death look like, and what is the probability that it is less than one week? 
\begin{lstlisting}
> n <- 100000
> d <- rlnorm(n,1.63,.5) + rgamma(n,shape=4.45,scale=4)
> mean(d<7)
[1] 0.00389
> hist(d,breaks=0:100) ## plots a histogram with bar breaks as given
\end{lstlisting}
\eps{-90}{.5}{i2d.eps}\index{histogram}

\noindent {\bf Exercise:} The {\em Central Limit Theorem} states that the distribution of the sum or mean of a set of $n$ independent identically distributed finite variance random variables tends to a normal distribution as $n \to \infty$. Produce an illustration of this by simulating random variables $X_n = n^{-1} \sum_{i}^n U_i$ where the $U_i$ are i.i.d. exponential random variables, with rate parameter 1 (see {\tt ?rexp}). For $n=2^1,,2^2,\ldots,2^9$ produce histograms illustrating the distribution of 10000 simulated values of each $X_n$. \lstinline+par(mfrow=c(3,3))+ will set up a graphics window to show all the histograms on the same page.   



\subsection{Simulation from a simple epidemic model}

Let's put a few things together and build a slightly more complicated simulation. SEIR (Susceptible-Exposed-Infective-Recovered\footnote{often a euphemism for recovered or dead}) models are commonly used in disease modelling. In the simplest version almost everyone starts out susceptible, and then move to the exposed class according to their daily probability of being infected by someone in the infected class. This daily probability is assumed proportional to the number of people in the infected class, with constant of proportionality $\beta$. People move from the exposed to infected class with a constant probability, $\gamma$ , each day, and move from the infected to the R class with a different constant probability, $\delta$. 

Suppose that we want to simulate from this stochastic SEIR model, maintaining a record of the populations in the different classes over time. And suppose in particular that we are interested in investigating scenarios in which people's daily probability of catching the disease is variable from person to person, so that each has a different $\beta$ value. In fact let's assume that each person has their own $\beta_i$ drawn from a gamma distribution. As usual we first need to think about code design. 
\begin{enumerate}
\item Since the code is for investigating multiple scenarios, we should produce a function that takes parameter values and control constants as inputs, and outputs vectors of the S, E, I and R populations over time.
\item We could maintain vectors for the different stages, and move individuals between them, but this seems clumsy. It is probably better to have a single vector maintaining an integer state indicating each individual's current status 0 for S, 1 for E, 2 for I and 3 for R.
\item The individual states are then updated daily according to the model, and the numbers in each state summed. 
\end{enumerate}    
Here is an implementation. 
\begin{lstlisting}
seir <- function(n=10000,ni=10,nt=100,gamma=1/3,delta=1/5,bmu=5e-5,bsc=1e-5) {
## SEIR stochastic simulation model.
## n = population size; ni = initially infective; nt = number of days
## gamma = daily prob E -> I; delta = daily prob I -> R;
## bmu = mean beta; bsc = var(beta) = bmu * bsc
  x <- rep(0,n) ## initialize to susceptible state
  beta <- rgamma(n,shape=bmu/bsc,scale=bsc) ## individual infection rates
  x[1:ni] <- 2 ## create some infectives
  S <- E <- I <- R <- rep(0,nt) ## set up storage for pop in each state
  S[1] <- n-ni;I[1] <- ni ## initialize
  for (i in 2:nt) { ## loop over days
    u <- runif(n) ## uniform random deviates
    x[x==2&u<delta] <- 3 ## I -> R with prob delta
    x[x==1&u<gamma] <- 2 ## E -> I with prob gamma
    x[x==0&u<beta*I[i-1]] <- 1 ## S -> E with prob beta*I[i-1]
    S[i] <- sum(x==0); E[i] <- sum(x==1)
    I[i] <- sum(x==2); R[i] <- sum(x==3)
  }
  list(S=S,E=E,I=I,R=R,beta=beta)
} ## seir
\end{lstlisting}
Hopefully by now most of this should be relatively easy to understand. The lines like 
\begin{lstlisting}
x[x==2&u<delta] <- 3 
\end{lstlisting}
are doing most of the real work, so these are especially important to understand. First note that elements of the logical vector \lstinline+u<delta+ will be {\tt TRUE} with probability {\tt delta} and {\tt FALSE} otherwise. This is because the elements of {\tt u} are $U(0,1)$ deviates and hence have probability {\tt delta} of being less than {\tt delta}. Elements of \lstinline+x==2&u<delta+ are then {\tt TRUE} only when the corresponding elements of \lstinline+u<delta+ {\bf and} \lstinline+x==2+ are {\tt TRUE}. As a result  \lstinline+x[x==2&u<delta] <- 3+ takes each element of {\tt x} in state 2 and with probability {\tt delta} sets it to state 3. The other lines act similarly, with the probability for the state 0 to 1 transition varying between elements of {\tt x}. 

The fixed $\beta$ version of this basic epidemic model is close to 100 years old. A great deal of the modelling work on Covid uses variations on this structure, perhaps surprisingly, given the simplistic way that the infection process is modelled. Even more surprisingly most of the models ignore the fact that there is wide variability in the number of contacts people have with other people each day, and in their chance of becoming infected at each contact -- i.e $\beta$ is not treated as random. We can use our simple implementation to investigate whether this matters. In particular consider a simulation with almost no variability in $\beta$, with {\tt bmu} and {\tt bsc} set to {\tt 7e-5} and {\tt 1e-7}. Then compare that to a simulation in which $\beta$ varies more widely (\lstinline+bsc=7e-5+). Finally consider a universal social distancing simulation, in which the mean and variability in $\beta$ are reduced (\lstinline+bmu=5e-5+, \lstinline+bsc=2e-6+).  
\begin{lstlisting}
par(mfcol=c(2,3),mar=c(4,4,1,1)) ## set plot window up for multiple plots
epi <- seir(bmu=7e-5,bsc=1e-7)   ## run simulation
hist(epi$beta,xlab="beta",main="") ## beta distribution
plot(epi$S,ylim=c(0,max(epi$S)),xlab="day",ylab="N") ## S black
points(epi$E,col=4);points(epi$I,col=2) ## E (blue) and I (red)
...
\end{lstlisting}
\eps{-90}{.6}{seir.eps}
Comparing the left two columns we see that neglecting the variability in $\beta$ means that the severity of the epidemic will be overestimated by the model. Comparing the right two columns we see that social distancing slows the epidemic down, but can make it larger in the long run, by suppressing the variability in $\beta$ (as everyone has to follow the same rules). Perhaps most surprising of all, scientists who tried to point out the problems with neglecting variability in $\beta$ had extreme difficulty getting published, and were often vilified on social media. Some models used for policy did represent the differences in contact rates between different age groups. But the main survey used to do this was based on one day diaries kept by survey participants of whom 7 were over 75 and none over 80. This is quite limited data for the age groups that make up 3/4 of Covid deaths to date. 

\noindent {\bf Exercise:} Try stepping through the {\tt seir} code line-by line using the {\tt debug} package (\lstinline+library(debug)+ loads it). \lstinline+mtrace(seir)+ indicates that you want to step into this function. Then call {\tt seir} as usual, and you will execute it in the debugger. Pressing the return key steps forward. {\tt bp} is used to set breakpoints. {\tt go()} runs until completion, or the next breakpoint. 
\lstinline+https://www.maths.ed.ac.uk/~swood34/RCdebug/RCdebug.html+ has more information on debugging. \lstinline+mtrace(seir, FALSE)+ to stop debugging {\tt seir}. Or \lstinline+mtrace.off()+ to stop all debugging.\index{debug}\index{debug!mtrace}\index{debug!go}\index{debug!bp}\index{breakpoint}

\subsection{Simple simulation study: testing an approximate confidence interval}

Suppose we observe a binomial random variable $X \sim \text{binom}(n,p)$ (number of successes in $n$ independent trials, each with probability of success $p$). $\hat p = X/n$ is an obvious estimator, of $p$, and a simple plug in estimate of its variance is then $\hat \sigma^2_p = \hat p (1-\hat p)/n$, suggesting approximate 95\% confidence interval $\hat p \pm 1.96 \hat \sigma_p$. The central limit theorem implies that this interval will have the correct coverage probability in the $n \to \infty$ limit, but how will it behave when $n=20$, for example? Let's investigate this question for the case $p=.2$. 
  
The basic idea, is that we should generate a large number of observations $x$ and from each one compute a 95\% confidence interval for $p$. We then count up what proportion of those intervals include the true value of $p$ we used in the simulation. If the intervals were exact then that proportion would be very close to 0.95.

\begin{lstlisting}
n.rep <- 10000 ## number of CIs to compute 
n.ci <- 0 ## counter for number that include the truth 
n <- 20;p <- .2
for (i in 1:n.rep) { 
  x <- rbinom(1,n,p) ## generate x
  p.hat <- x/n       ## compute the estimate of p
  sig.p <- sqrt(p.hat*(1-p.hat)/n) ## and its standard error
  if (p.hat - 1.96*sig.p <p && p.hat+1.96*sig.p > p) n.ci <- n.ci + 1 ## truth included?
}
n.ci/n.rep ## proportion of intervals including the truth
\end{lstlisting}
The result is about 0.92 --- so when $n=20$, 8\% of intervals fail to include the true value, instead of the 5\% that should. Many statistical methods rely on infinite sample size approximations, and this sort of simulation study is an essential way of checking out their performance at finite sample sizes.

\noindent {\bf Exercise:} write vectorized code to do the preceding calculation without a loop, and investigate how the coverage probability changes as you increase the sample size to 200, 2000 etc.


\section{Reading and storing data in files}

Since statistics is about understanding what data are telling us about the world, a statistical programmer needs to know how to read data into computer memory. Recall that computer memory (or `random access memory') is the fast access temporary working memory of a computer. What is in it vanishes when you turn the computer off. It should not be confused with the long term storage provided by a hard disk, or solid state storage. The latter persists even when the computer is turned off, and is where `computer files' are stored, organised in a hierarchy of folders/directories. Persistent storage is also sometimes accessed remotely over the internet, but we will start with reading in files stored locally on your computer. 

\subsection{working directories}

R maintains a record of the current `working directory'. This is the directory on your computer's file system where it will assume that a file is stored if you don't tell it otherwise. You can find out what the working directory currently is as follows. \index{getwd}\index{setwd}\index{working directory}
\begin{lstlisting}
> getwd()
[1] "/home/sw283" ## my working directory
\end{lstlisting}
The exact format depends on your operating system, but note that R uses \verb+/+ to separate subdirectories from parent directories, even on Windows. You can change the working directory as follows:
\begin{lstlisting}
> setwd("/home/sw283/lnotes")
\end{lstlisting}
making sure that you specify a valid path for your filesystem. For example, on windows the path includes a letter identifying the disk or other storage device where the directory is located. e.g. \lstinline+setwd("c:/foo/bar")+ for subdirectory `bar' of directory `foo' on \verb+c:+. Note the use of forward slashes in place of the windows default back slashes.

\subsection{Reading in code: source()}
\index{source}
Before reading in data, consider reading in code. The \lstinline+source+ function reads in code and runs it from a file. This is particularly useful for reading in function definitions from a file. For example:
\begin{lstlisting}
source("mycode.r")
\end{lstlisting}
reads in the contents of {\tt mycode.r}, first checking that it is valid R code and then running it. Running R code from a file like this is not exactly the same as typing it, or pasting it, into the R console. One main difference is that if you type the name of an object, and nothing else, at the console, then the object will be printed. If a line of sourced code contains the name of an object and nothing else, then the line causes nothing to happen. You would need to explicitly print the object if you want it to be printed. See {\tt ?source} for full details.

A useful function related to {\tt source} is {\tt dump}, which writes a text representation of some R objects to a file as R code, in a way that can later be read back in to R using {\tt source}. For example suppose we have objects {\tt a}, {\tt alice} and {Ba} to save. All we have to do is supply {\tt dump} with a text vector of the object names, and the name of the file to write them to: \index{dump}
\begin{lstlisting}
dump(c("a","alice","Ba"),file="stuff.R")
\end{lstlisting}

\subsection{Reading from and writing to text files of data}

Text files have information stored in human readable form. You can open the file in an editor and understand it. Text files are a very portable way of storing data, but the files are quite a bit larger than is strictly necessary to store the information in the file. R has a number of functions for reading data from text files. All have many arguments to let you adapt to the way that the data has been stored. \index{file!text}

As a simple example consider a file called {\tt data.txt} containing 
\begin{verbatim}
"name" "age" "height"
"sue"  21    1.6
"fred" 19    1.7
\end{verbatim}

{\tt scan} reads data from a file into a vector. For example:\index{scan}
\begin{lstlisting}
> a <- scan("data.txt",what="c");a
Read 9 items
[1] "name"   "age"    "height" "sue"    "21"     "1.6"    "fred"   "19"  "1.7"
\end{lstlisting}
The type of the {\tt what} argument tells {\tt scan} what type of items it will be reading in. Character in this case, given \verb+"c"+ is of type character. {\tt scan} treats white space as the separating items to read, but sometimes the separator is something else, for example a comma. In that case you can specify the separator using the {\tt sep} argument. e.g. \lstinline+sep=","+.

{\tt scan} can also read in data in which different columns of data have different types, returning the different columns as different items of a list. In that case {\tt what} is supplied as a list, with the type of the list items giving the types of the items to be read and stored. Here is an example in which the first line of the file is skipped using the {\tt skip} argument. 
\begin{lstlisting}
> b <- scan("data.txt",what=list("a",1,1),skip=1); b
Read 2 records
[[1]]
[1] "sue"  "fred"

[[2]]
[1] 21 19

[[3]]
[1] 1.6 1.7
\end{lstlisting} 
Actually, the file does not strictly need to have the data arranged in columns, simply in a repeated pattern.

Often the list form of the data is not as useful as having it in a data frame. So let's name the items in the list according to the first row of {\tt data.txt} and convert to a data frame.
\begin{verbatim}
> names(b) <- scan("data.txt",what="c",n=3)
> d <- data.frame(b); d
  name age height
1  sue  21    1.6
2 fred  19    1.7
\end{verbatim}

Reading columns of data into a data frame in R is such a common task that a number of functions are provided for this purpose. The main one is {\tt read.table}. Take a look at {\tt ?read.table} for the variations. An easier way of reading {\tt data.txt} directly into {\tt d} would be \index{data frame!read in}\index{read.table}
\begin{lstlisting}
d <- read.table("data.txt",header=TRUE) 
\end{lstlisting}
where \lstinline+header=TRUE+ simply indicates that the first line gives the variable names. {\tt sep} and {\tt skip} can also be used with {\tt read.table}. Unsurprisingly, the opposite task of writing a data frame out to a text file has its own function {\tt write.table}. For example:\index{write.table}
\begin{lstlisting}
write.table(d,file="data1.txt") 
\end{lstlisting}

On occasion it is necessary to read lines of text into a character vector, for further processing. The {\tt readLines} function will do this, and there is an equivalent {\tt writeLines} function as well.\index{readLines}\index{writeLines}

To write data other than a data frame or character vector to a text file you can use the {\tt cat} or {\tt write} functions: both take a {\tt file} argument specifying the file name to write to. {\tt dump} is another option covered in the previous section.\index{file!write text}

\noindent {\bf Exercise:} create a version of text file {\tt data.txt} outside R yourself, and read it into R as described above.

\subsection{Reading and writing binary files}

To save large R objects or data frames to file, it is more efficient to use a binary format and compression. {\tt save} can do this. It takes a sequence of R objects, and writes them to the file specified in its {\tt file} argument. {\tt  load} will read the objects back in from the file. Here is a simple example:
\begin{lstlisting}
A <- matrix(1:6,3,2); y <- rnorm(100) ## create some objects
save(A,y,file="stuff.Rd")  ## save them to a binary file
rm(y,A)          ## delete the original objects
load("stuff.Rd") ## load them back in again from the file
\end{lstlisting}
{\tt save} does allow you to choose to save the objects as text as well, but this requires larger files: for the above example twice as large, but sometimes {\em much} larger.\index{save}\index{load}\index{file!binary}

\subsection{Reading data from other sources}

Rather than reading standard files from storage on your local computer, you might want to read data from a file subject to some standard compression algorithm, or over a local network, or from a URL. Most of the functions for reading and writing data can use one of these other types of {\em connection} as their file argument. See {\tt ?connection} for details. Here is a single common example of reading data from a web address
\begin{lstlisting}
raw <- readLines("https://michaelgastner.com/DAVisR_data/homicides.txt") 
\end{lstlisting}
in this case lines of data are read into a character vector. \index{readLines} \index{connections}

\section{Data re-arrangement and tidy data}

In raw form many data sets are messy and have to be tidied up for statistical analysis. By tidying is meant that the data are re-arranged conveniently for analysis, not that the information in the data is in anyway modified unless an actual error is identified (no removal of outliers, for example). Since `data tidying' or `data re-arrangement' sound too boring and clerical for many enthusiasts for Modern Data Science, you will often hear terms like `data wrangling' and `data munging\footnote{a term that started out as the derogatory `mash until no good'.}' used instead. These sound suitably rugged, as if the types engaged in them return home aching and covered in dust and mud after a day engaged in the honest toil of these muscular pursuits.

By tidy data is meant that data are arranged in a matrix so that:\index{tidy data}\index{data frame}
\begin{enumerate}
\item Each column is a variable.
\item Each row is an observation (a unique combination of variables).
\end{enumerate}
and it is recognised that what counts as `an observation' can depend on the subset of variables of interest. For example,
this data frame is in tidy form if interest is in student's grades each week. 
\begin{verbatim}
name    d.o.b.  nationality week grade
George  11/2/01 GB          1    B
George  11/2/01 GB          2    C
George  11/2/01 GB          3    A
Anna    15/6/00 GB          1    A
Anna    15/6/00 GB          2    B
Anna    15/6/00 GB          3    A
.       .       .           .    .
\end{verbatim}
But if we are only interested in birth dates and nationality of students, then we would really want to simplify to
\begin{verbatim}
name    d.o.b.  nationality 
George  11/2/01 GB          
Anna    15/6/00 GB          
.       .       .           
\end{verbatim}
It is easy to get the second data frame form the first (\lstinline+marks+, say) in base R using
\begin{lstlisting}
student <- unique(marks[,c("name","d.o.b.","nationality")])
\end{lstlisting}
which has selected the named columns of interest, and then found the unique rows. If you want to refer each row in \lstinline+marks+ to its row in \lstinline+student+ then you can use 
\begin{lstlisting}
library(mgcv)
student <- uniquecombs(marks[,c("name","d.o.b.","nationality")])
ind <- attr(student,"index") 
## ... ind[i] is row of 'student' corresponding to row i of 'marks'
\end{lstlisting}

At this point, you are hopefully wondering about replicate data, and the definition of `observation' as `unique combination of variables'? If I repeat an experiment under identical conditions, could I not occasionally get identical results, and hence two identical rows in a data frame that are different observations? Well yes, but only if you do not include a variable recording the `replicate number'. If you don't record the replicate number then you do not really have tidy data, as it is not possible to distinguish the common error of accidental replication of some recorded data, from genuine replicates. 

The R add on packages collectively known as the {\em tidyverse} provide a large number of functions for data tidying -- we won't cover these here, as it takes us too far from fundamentals of statistical programming: but see the {\em Research Skills} course next semester.
 
\subsection{Baltimore homicides: data tidying and regular expressions}

Let's look at re-arranging a fairly messy data set on homicides in Baltimore. The raw data can be obtained using
\begin{lstlisting}
raw <- readLines("https://michaelgastner.com/DAVisR_data/homicides.txt")
raw[1] ## examine first record
[1] "39.311024, -76.674227, iconHomicideShooting, 'p2', '<dl><dt>Leon Nelson</dt>
<dd class=\"address\">3400 Clifton Ave.<br />Baltimore, MD 21216</dd><dd>black male,
 17 years old</dd><dd>Found on January 1, 2007</dd><dd>Victim died at Shock Trauma
 </dd><dd>Cause: shooting</dd></dl>'"
\end{lstlisting}
The subsequent entries in {\tt raw} look similar, but they are not completely standardized, as we will see.
For analysis, we need to extract the information that has been read into {\tt raw} in standardized form. For example, we might want to extract the longitude and latitude location information (the first 2 numbers), the victim name, the race information (black, white, hispanic etc), age etc. To do this requires some effort, first trawling through the data to check how the information is stored, and whether it is consistent across records.   

To start consider a relatively easy task - the location information. This is always first, with latitude and longitude separated by a comma. So we could simply split up each record wherever there is a comma, and read off the first two elements from each resulting split record. \lstinline+strsplit+ can be used to do this, as follows: 
\begin{lstlisting}
rl <- strsplit(raw,",")
lat <- as.numeric(sapply(rl,function(x) x[1]))
lon <- as.numeric(sapply(rl,function(x) x[2]))
\end{lstlisting}
\begin{itemize}
\item \lstinline+strsplit(raw,",")+ splits each character string in text vector {\tt raw} into a vector of character strings, with the splits occurring wherever there is a comma (the commas are dropped). {\tt strsplit} returns a list of character vectors, with an element for each element of {\tt raw}: this is stored in {\tt rl}.
\item Now we just need to extract the first and second element of each character vector in {\tt rl}. {\tt sapply} applies a function to each element of a list, returning the result as a vector. So \lstinline+sapply(rl,function(x) x[1])+ applies a function that simply extracts the first element of a vector to {\tt rl}. The result is a vector of latitudes, as text, which are then converted to numeric and stored in {\tt lat}. 
\item Longitudes are extracted in the same way.  
\end{itemize}

Extracting names is not so easy. A consistent pattern is that the name record starts \lstinline+<dt>+ and ends \lstinline+</dt>+, although the record may also include a web address pointing to a further record. I therefore first tried to split on the occurrence of \lstinline+dt+, but even after trying to clean out the web addresses one of the resulting names was 86 characters long, and still contained a web address. The problem turned out to be that the victims names was Nancy Schmi{\bf dt}. 

So it is better to use \lstinline+<dt>+ and \lstinline+</dt>+ in some way. Indeed if you look at record 1 above, you'll see that I ought to be able to snip out the name very neatly using these. However the data turn out to be messier than that. e.g.
\begin{lstlisting}
> raw[300]
[1] "39.28957200000, -76.57039400000, icon_homicide_shooting, 'p489', '<dl><dt><a 
href=\"http://essentials.baltimoresun.com/micro_sun/homicides/victim/489/alton-alston
\">Alton Alston</a></dt><dd class=\"address\">200 S. Clinton St.<br />Baltimore,
 MD 21224</dd><dd>Race: Unknown<br />Gender: male<br />Age: 18 years old</dd><dd>
 Found on December  1, 2008</dd><dd>Victim died at Unknown</dd><dd>Cause: Shooting
 </dd><dd><a href=\"http://www.baltimoresun.com/news/local/baltimore_city/bal-
 eastside1201,0,5413416.story\">Read the article</a></dd></dl>'"
\end{lstlisting}
The web address in the name record needs stripping out. This sort of problem needs a bit more than the simple splitting on a pattern met so far: it needs {\em regular expressions}. 

\subsubsection{Regular Expressions}

Regular expressions provide a way of matching patterns in text that are more flexibly defined than simple fixed strings. For example, suppose I wanted to find all words in a which at some point an `i' occurs two characters after a `c' (as in `clip' or `skittle'), or I want to find all words starting with `f' and ending with `k' (as in `fork' or `flunk'). We can accomplish such tasks by constructing strings which contain special characters interpreted as matching particular patterns. Functions such as {\tt grep} and {\tt gsub} can understand these {\em regular expressions} (unless we turn off this ability, with argument \lstinline+fixed=TRUE+). You can get a full description of the available regular expressions by typing \lstinline+?regex+ in R. \index{regular expression}\index{regex} Note the slight irritation that control characters referred to as e.g. \verb+\b+ in the help file actually have to be used as \verb+\\b+ in regular expressions.

Within regular expressions the characters  \verb-. \ | ( ) [ { ^ $ * + ?- have special meanings. If you want them treated  as regular characters instead, you have to precede them by \verb+\\+ (or if you don't need any of them to have their special meaning, use argument \lstinline+fixed=TRUE+). Here we will use a couple of these: `\verb+.+' can be used to represent any single character. This could accomplish the first task above:
\begin{lstlisting}
> txt <- 
"He scribbled a note on his clipboard and clicked his heels as he clinched the deal."
> grep("c.i",strsplit(txt," ")[[1]])
[1]  7  9 14
\end{lstlisting}
For the second example we want to allow an arbitrary number of letters between the letters at the start and end of the word. `\verb+?+'  `\verb+*+' and `\verb-+-' all modify the character preceding them: `\verb+?+' indicates that it's optional, and will be matched at most once; `\verb+*+' indicates that it should be matched zero of more times, and `\verb-+-' that it should be matched 1 or more times. We want to match any number of characters between `f' and `k', and `\verb+.*+' is how to do that:
\begin{lstlisting}
> txt1 <- 
"To flunk this exam would be disaster, thought Phil, picking up his fork to eat "
> grep("f.*k",strsplit(txt1," ")[[1]])
[1]  2 13
\end{lstlisting}
But actually this is not general enough. Consider 
\begin{lstlisting}
> txt2 <- 
"Flunk this exam and I'll be toast, thought Farouk, forking beans into his mouth."
> grep("f.*k",strsplit(txt2," ")[[1]])
[1] 10
\end{lstlisting}
\ldots the wrong answer - the two words we wanted have been missed, and `forking' has been wrongly matched. Consulting {\tt ?regex} again, it turns out that we can specify sets of characters to match, by enclosing them in square brackets, for example \verb+[Ff]+. We can also specify the end of a word by \verb+\\b+. Putting these together:
\begin{lstlisting}
> grep("\\b[Ff].*k\\b",strsplit(txt2," ")[[1]])
[1] 1 9 ## success
\end{lstlisting}

\subsubsection*{Back to the homicide data}

With regular expressions it is now possible to extract the names automatically, as follows
\begin{lstlisting}
get.name <- function(x) {
  x1 <- gsub(".*<dt>","",x) ## strip out everything to <dt>
  x1 <- gsub("</dt>.*","",x1) ## strip out everything after </dt>
  x1 <- gsub("<a.*\">","",x1) ## strip out any web addresses
  sub("</.*","",x1) ## strip out any further tags
}
name <- sapply(raw,get.name,USE.NAMES=FALSE)
\end{lstlisting}
\begin{enumerate}
\item Notice how I have broken the task down into parts in \lstinline+get.name+, with each intermediate result stored before it is updated. This `defensive coding' facilitates debugging if something goes wrong. I can then use a debugger to step through \lstinline+get.name+ line by line, checking that each intermediate step is what I expected. If I had written the whole operation as a set of nested function calls, it would have been much harder to debug. Styles of programming that involve `piping' the output of one function directly to the input of another, suffer from the same code maintenance and debugging problems.  \index{defensive coding} \index{piping}   
\item {\tt sapply } applies the functions specified as its second argument to each element of its first argument, returning the result in a vector. The \lstinline+USE.NAMES=FALSE+ argument avoids {\tt sapply} attaching names to the vector it creates: by default it uses the entire original string in \verb+raw[i]+ as the label for \verb+name[i]+. This would be inconvenient.
\end{enumerate}

Finally let's extract race. More data inspection reveals that the information comes immediately after the first \verb+<dd>+ tag, but there are two forms: either the race qualifier is the first word in a longer phrase, or it comes after the text `Race:'. So one way of extracting something appropriate is 
\begin{lstlisting}
rl <- strsplit(raw,"<dd>") ## split so race info second element of each record
race <- sapply(rl,function(x) sub("<.*","",x[2]) ) ## strip trailing tags
ii <- grep("Race:",race) ## locate 'Race:' cases
race[ii] <- gsub("Race: ","",race[ii]) ## extract race info for 'Race:' cases
race[-ii] <- gsub(" .*","",race[-ii]) ## drop rest of text in other cases
\end{lstlisting}

Putting the information extracted into a data frame gives
{\small \begin{verbatim}
> dat <- data.frame(lon=lon,lat=lat,name=name,race=tolower(race))
> head(dat)
        lon      lat               name  race
1 -76.67423 39.31102        Leon Nelson black
2 -76.69895 39.31264         Eddie Golf black
3 -76.64988 39.30978   Nelsene Burnette black
.    .        .                        .    .
\end{verbatim}}
We might immediately want to plot the locations of the homicides, coded by race. The following does this for main racial groups of black (64\% of population of Baltimore), white (28\%) and hispanic (4\%), other races and unknown are lumped together (the vast majority of these are unknown or not recorded). 
\begin{lstlisting}
with(dat,plot(lon,lat,pch=19,cex=.5))
ii <- dat$race=="white"
points(dat$lon[ii],dat$lat[ii],col="pink",pch=19,cex=.6)
ii <- dat$race=="hispanic"
points(dat$lon[ii],dat$lat[ii],col="brown",pch=19,cex=.6)
ii <- !(dat$race=="black"|dat$race=="white"|dat$race=="hispanic")
points(dat$lon[ii],dat$lat[ii],pch=19,cex=.5,col="green")
\end{lstlisting}

\eps{-90}{.5}{baltimore.eps}

\noindent {\bf Exercise:} Write code to extract the cause of death. In this case you'll see that most records have a cause field starting \lstinline+<dd>Cause:+. Use {\tt grep} to create an index of the entries in {\tt raw} containing this sequence, and hence check the entries that do not have this field to see if cause is coded differently there, or is simply missing. Then write code to extract the cause data, doing something sensible about the records without the \lstinline+<dd>Cause:+ entry. Find the proportion of homicides that are by shooting.

\section{Statistical Modelling: linear models \label{sec:lm}}

So far we have looked at programming data manipulation tasks, and at some simulation from stochastic models. What about statistical modelling? That is the process of using models of the data generating process to help us learn about the population from which the data were generated (or, if you prefer, the {\em system} that generated the data). {\em Linear models} are a good place to start, since such models introduce many of the concepts used in statistical modelling more generally. 

The general setup is that we have observations of a random {\em response} variable, $y_i$, accompanied by (fixed or random) {\em predictor} variables (a.k.a. {\em covariates}), $x_{ij}$. Here observations are indexed by $i = 1,\ldots,n $ and predictor variables by $j=1, \ldots, p$, and $p \le n$. Predictor variables come in two basic varieties:
\begin{enumerate}
\item {\em Metric variables} are numerical measurements of something. e.g. height, weight, litres of fuel used per 100km, temperature etc. \index{predictor!metric}
\item {\em Factor variables} are labels indicating membership of some group (the groups are known as {\em levels} of the factor, although they are not treated as having any particular ordering). For example, nationality, sex, variety of wheat, species of bird.  \index{predictor!factor} \index{factor}
\end{enumerate}  
Obviously on occasion factors and metric variables may play rather similar roles - for example when we categorize people into age groups.\index{linear model}

Considering all metric predictors first, the generic linear model structure is 
$$
y_i = \beta_0 + x_{i1} \beta_1 + x_{i2} \beta_2 + \cdots + x_{ip} \beta_p + \epsilon_i 
$$
where the $\beta_j$ are unknown parameters to be estimated from data, and the $\epsilon_i$ are independent random variables, with zero expected value, and variance $\sigma^2$. For some purposes, we will also assume that they are normally distributed. Notice that if we take expectations of both sides of the model we get $E(y_i) =  \beta_0 + x_{i1} \beta_1 + x_{i2} \beta_2 + \cdots + x_{ip}\beta_p $. 

The model is {\em linear} in the parameters, $\beta_j$, and noise, $\epsilon_i$. It can be non-linear in the predictors. For example:
$$
y_i = \beta_0 + x_{i1} \beta_1 + x_{i1}^2 \beta_2 + x_{i2} \beta_3 + x_{i1} x_{i2} \beta_4 \cdots + \epsilon_i 
$$
In a way it is obvious that this must be possible: we put no restrictions on the predictors, so $x_{i1}^2$ or $x_{i1} x_{i2}$ must be as valid as predictors as $x_{i1}$ and $x_{i2}$ themselves.  

This observation leads us on to linear models involving factor variables. The key idea is best seen by example. Suppose $x_{i1} $ is a factor variable that can take values {\tt a}, {\tt b} or {\tt c}. The idea is then that a parameter will be added to the model depending on which level the factor takes. Mathematically we have a model something like:
$$
y_i = \beta_0 + \mathbb{I}(x_{i1}={\tt a}) \beta_1 + \mathbb{I}(x_{i1}={\tt b})\beta_2 + \mathbb{I}(x_{i1}={\tt c})\beta_3 + x_{i2} \beta_4 + \cdots + \epsilon_i 
$$
where $\mathbb{I}()$ is the indicator function, taking the value 1 when its argument is true and 0 otherwise. There is a catch, however. In the preceding model we could add any constant $k$ to $\beta_0$ while subtracting the same constant from $\beta_1$, $\beta_2$ and $\beta_3$ and the numerical values produced by the right hand side of the model would be unchanged. Clearly the model has redundancy - an infinite set of parameter values can produce any given value of the right hand side. Hence the model parameters can not be uniquely estimated from data: the model is not {\em identifiable}. \index{identifiability}\index{linear model!identifiability}\index{identifiability!constraint}

Happily, this lack of identifiability can be rectified by simply dropping one of the parameters from the redundant set. We could drop $\beta_0$, but then we get the same problem again is we add a second factor variable to the model, so a better option is to drop the parameter associated with the first level of each factor. This is what R does automatically. In the preceding case we then get:
\beq
y_i = \beta_0  + \mathbb{I}(x_{i1}={\tt b})\beta_1 + \mathbb{I}(x_{i1}={\tt c})\beta_2 + x_{i2} \beta_3 + \cdots + \epsilon_i \label{factor.mod}
\eeq
(note the closing up of $\beta$ indices). The new version can predict anything that the old version could, but the redundancy/lack of identifiability has gone. The only price paid is that the interpretation of $\beta_0$ and the parameters for the factor effect now changes: $\beta_0$ is now the `intercept' parameter that applies when ${x}_{i1}$ has value {\tt a}.  $\beta_1$ and $\beta_2$ are now the {\em differences} between the effect of level {\tt b} and level {\tt a} and between level {\tt c} and level {\tt a}, respectively. 

Note that writing models out with indicator functions can get very clumsy, especially for factors with many levels, so alternative ways of expressing the same model can be useful. One approach uses multiple indices, with an index for each level of each factor variable in the model. For example 
$$
y_{ij} = \beta_0 + \gamma_j + z_{ij} \beta_1 + \epsilon_{ij} \text{  if }x_{ij}\text{ takes its }j^\text{th}\text{ level}
$$
is a structure exactly like that of (\ref{factor.mod}), but to express it clearly, multiple Greek letters have been used for parameters, and multiple Roman letters for variables. Identifiability in this case is imposed with a restriction, such as $\gamma_1=0$. Multiple indices like this can also get messy, so it can be more elegant to deal with factors by simply indexing the associated parameters by levels of a factor. For example if $x_i$ is a factor (perhaps taking levels {\tt a}, {\tt b} or {\tt c}) then a model might be expressed as
$$
y_i = \beta_0 + \gamma_{x_i} + z_i \beta_1 + \epsilon_i
$$ 
\ldots the idea being that there is a different $\gamma$ parameter for each level of the factor (e.g. $\gamma_{\tt a}$, $\gamma_{\tt b}$, $\gamma_{\tt c}$, or whatever). Again identifiability constraints are written as restrictions on the parameters, such as $\gamma_{\tt a}=0$. 

The point here is to recognise that there are different ways to express exactly the same model structure - which is most convenient depends on the context. 


\subsection{Statistical interactions}

Clearly linear models allow us to include the effects of several predictors of mixed type (metric and/or factor) in an additive way --- with the effects of the variables simply added to each other. But what about the case in which the effects of variables do not simply add up, but rather the effect of one variable is to modify the effect of another variable. In statistical modelling this is known as an {\bf interaction}. It is one of those simple key concepts that it is essential to understand, so read the following very carefully, even if you already know about interactions.

What does {\em  the effect of one predictor modifies the effect of another predictor} mean in concrete model terms? Consider an example. Suppose a model contains the effect of predictor $x_i$ in a simple linear way:
$$
y_i = \beta_0 + x_i \beta_1 + \epsilon_i
$$    
i.e. $E(y_i)$ has a straight line relationship to $x_i$, with slope parameter $\beta_1$. But now suppose that the parameters of this straight line relationship themselves change according to a second predictor variable, $z_i$. How? Perhaps the effect of $z_i$ is also linear, so that $\beta_0 = \gamma_0 + z_i \gamma_1$ and $\beta_1 = \gamma_2 + z_i \gamma_3$. Substituting into the original model we get\index{interaction}\index{linear model!interaction}
$$
y_i = \gamma_0 + z_i \gamma_1 + x_i \gamma_2  +  x_i z_i \gamma_3 + \epsilon_i
$$  
We are not limited to producing interactions of effects that are linear in the covariates. Interactions of any sort of effect can be produced in the same way. Suppose, for example, that the effect of $x_i$ is quadratic so that 
$$
y_i = \beta_0 + x_i \beta_1 +  x_i^2 \beta_2  + \epsilon_i
$$
and that this effect should vary linearly with $z_i$. Then $\beta_0$ and $\beta_1$ are as before, while $\beta_2 = \gamma_4 + z_i \gamma_5$, so
$$
y_i = \gamma_0 + z_i \gamma_1 + x_i \gamma_2  +  x_i z_i \gamma_3 + x_i^2 \gamma_4 + x_i^2 z_i \gamma_5 + \epsilon_i.
$$
Notice how in both the preceding examples the model includes some terms that would have been there if the effects had been purely additive: these are known as the {\bf main effects}, while the extra terms are referred to as {\bf interaction terms}. For the last model the main effects are $z_i \gamma_1$ and $ x_i \gamma_2+ x_i^2 \gamma_4$, while the interaction term is $x_i z_i \gamma_3 + x_i^2 z_i \gamma_5$.

Exactly the same idea applies to factor variables. For example we might want the effects of $x_{i1}$ and $x_{i2}$ from model (\ref{factor.mod}) to interact. Whether we allow the parameters for the  $x_{i1}$ effect to change linearly with $x_{i2}$, just like the main effect of  $x_{i2}$, or we allow the parameter for the $x_{i2}$ effect to depend on $x_{i1}$ according to the $x_{i1}$ model, we end up with the same model\ldots
$$
y_i = \beta_0  + \mathbb{I}(x_{i1}={\tt b})\beta_1 + \mathbb{I}(x_{i1}={\tt c})\beta_2 + x_{i2} \beta_3 
+ \mathbb{I}(x_{i1}={\tt b})x_{i2} \beta_4 + \mathbb{I}(x_{i1}={\tt c})x_{i2}  \beta_5 +
 \cdots + \epsilon_i
$$  
The pattern is similar for interactions between factor variable effects. For the above example, suppose $x_{i2}$ was actually a factor variable, with levels \verb+g1+ and \verb+g2+. The first level, \verb+g1+, would get dropped of course, to ensure identifiability. Allowing the parameters for the  $x_{i1}$ main effect to vary according to the $x_{i2}$ main effect (or vice-versa) we get (ignoring any other model components)
$$
y_i = \beta_0  + \mathbb{I}(x_{i1}={\tt b})\beta_1 + \mathbb{I}(x_{i1}={\tt c})\beta_2 + \mathbb{I}(x_{i2}={\tt g2}) \beta_3 
+ \mathbb{I}(x_{i1}={\tt b})\mathbb{I}(x_{i2}={\tt g2}) \beta_4 + \mathbb{I}(x_{i1}={\tt c})\mathbb{I}(x_{i2}={\tt g2})  \beta_5 + \epsilon_i
$$ 
Given the model identifiability problems when first introducing factors, is it necessary to worry about identifiability in interactions? Not if the main effects are made identifiable first, and the interaction terms are constructed from these identifiable main effects: the identifiability is `inherited' from the main effects. \index{identifiability!interactions}  

Pay careful attention to what a statistical interaction is not. It {\em is not} about the relationship of one predictor to another. It {\em is} about their combined effect on the response.\index{factor!interaction}


\subsection{Computing with linear models: model matrices and model formulae \label{sec:model.matrix}}

To compute with linear models it is very helpful to write the model in the general matrix vector form
$$
{\bf y} = {\bf X}{\bm \beta} + {\bm \epsilon}
$$
where ${\bf y}$ is an $n$ vector of response variable values, $\bf X$, the {\bf model matrix}, is an $n \times p$ matrix determined by the model structure and predictor variables, $\bm \beta$ is a parameter vector and ${\bm \epsilon}$ is a vector of independent zero mean, constant variance random variables (possibly normally distributed). How do the models we have seen so far fit into this form?

This is easiest to see by example, so consider model (\ref{factor.mod}) again, and assume it only has the 4 parameters shown. As written there is an index $i$ which runs from 1 to $n$, so the model is really the system of equations
\begin{align*}
y_1 &= \beta_0  + \mathbb{I}(x_{11}={\tt b})\beta_1 + \mathbb{I}(x_{11}={\tt c})\beta_2 + x_{12} \beta_3 + \epsilon_1\\
y_2 &= \beta_0  + \mathbb{I}(x_{21}={\tt b})\beta_1 + \mathbb{I}(x_{21}={\tt c})\beta_2 + x_{22} \beta_3 + \epsilon_2\\
. ~ & ~~.  ~~~~~~~~~~~~ .\\
. ~ & ~~.  ~~~~~~~~~~~~ .\\
y_n &= \beta_0  + \mathbb{I}(x_{n1}={\tt b})\beta_1 + \mathbb{I}(x_{n1}={\tt c})\beta_2 + x_{n2} \beta_3 + \epsilon_n
\end{align*}
Obviously this can be written as 
$$
\bmat{c} y_1 \\ y_2 \\ \cdot \\ \cdot \\ y_n \emat = 
\bmat{cccc}
1 & \mathbb{I}(x_{11}={\tt b}) &  \mathbb{I}(x_{11}={\tt c}) & x_{12} \\
1 & \mathbb{I}(x_{21}={\tt b}) &  \mathbb{I}(x_{21}={\tt c}) & x_{22} \\
. & . & . & . \\. & . & . & . \\
1 & \mathbb{I}(x_{n1}={\tt b}) &  \mathbb{I}(x_{n1}={\tt c}) & x_{n2}
\emat \bmat{c} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \emat + 
\bmat{c} \epsilon_1 \\ \epsilon_2 \\ \cdot \\ \cdot \\ \epsilon_n \emat,
$$
which is clearly in the form $ {\bf y} = {\bf X}{\bm \beta} + {\bm \epsilon} $.\footnote{Obviously we do not require that every linear model we write down has parameters called $\beta_0,\beta_1,\beta_2$ etc. The point is simply that  whatever parameters the model has, they can be stacked up into one parameter vector, here called $\bp$. } To make this more concrete still, suppose that $x_{\cdot1} = ({\tt c},{\tt b},{\tt b},{\tt c}, \ldots, {\tt b})\ts$, and $x_{\cdot2} = (0.3,-2,27,3.4, \ldots, 10 )\ts $. Then the model is 
$$
\bmat{c} y_1 \\ y_2 \\ y_3 \\ y_4 \\ \cdot \\ \cdot \\ y_n \emat = 
\bmat{cccc}
1 & 0 &  1 & 0.3 \\
1 & 1 &  0 & -2 \\
1 & 1 &  0 & 27 \\
1 & 0 &  1 & 3.4 \\
. & . & . & . \\. & . & . & . \\
1 & 1 & 0  & 10
\emat \bmat{c} \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \emat + 
\bmat{c} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \cdot \\ \cdot \\ \epsilon_n \emat.
$$
To understand linear models it is essential to understand how model matrices relate to the models as they are usually written down, especially how factor variables expand to several columns of binary indicators. The other key concepts to understand to the point that they are second nature are: factors, identifiability and interactions. 

So how do we actually compute with these modelling concepts? R has functionality designed to make the setting up of model matrices very straightforward, based on the notion of a {\bf model formula}. \index{model formula}\index{linear model!formula} An R model formula is a compact description of a model structure in terms of the names of the variables in the model. For example:
$$
\verb^y ~ x + z^
$$
specifies a model $y_i = \beta_0 + x_i\beta_1 + z_i \beta_2 + \epsilon_i$, if $x$ and $z$ are metric variables (an intercept is included by default). Whatever is to the left of \verb+~+ is the response variable, while what is to the right specifies how the model depends on predictor variables. \verb^x + z^ implies that we want a main effect of {\tt x} and a main effect of {\tt z}. The exact form of these main effects depends on whether each predictor is a factor or a metric variable. Notice how \verb-+- has a special meaning in the formula. It basically means {\em and}. It does not mean addition. 

Operators \verb-+-, \verb+-+, \verb+^+, \verb+*+ and \verb+:+ all have special meanings when used in a formula, {\em except} when they are using in arguments of a function inside a formula. So while 
$
\verb^y ~ x + z^
$
indicates to include effects of {\tt x} and of {\tt z}, not include and effect of $x+z$, the formula
$$
\verb^y ~ log(x + z)^
$$
does imply the addition of $x$ and $z$ before the log is taken. This means that it is easy to restore the usual arithmetic meaning of operators, by including the terms as arguments of the identity function \verb+I()+ in model formulae. For example, to implement $y_i = \beta_0 + x_i\beta_1 + x_i^2 \beta_2 + \epsilon_i$, use
$$
\verb%y ~ x + I(x^2)%
$$
Model formula operators {\tt *} and {\tt :} implement interactions. \verb+a*b+ indicates that the main effect terms and the terms for the interaction of {\tt a} and {\tt b} are to be included in a model. \verb+a:b+ just includes the interaction terms, but not the main effects. So  \verb+a*b+ and \verb@a + b + a:b@ are equivalent. Higher order interaction operate the same way. For example \verb+a*b*c+ is equivalent to \verb@a + b + c + a:b + a:c + b:c + a:b:c@

Of course you might want to include interactions of several variables, only up to a certain order. This is where the \verb+^+ operator comes in. Suppose we want the ineractions of {\tt a}, {\tt b}, {\tt c} and {\tt d} up to order 2 (and not up to order 4 as \verb+a*b*c*d+ would generate). \verb%(a+b+c+d)^2% will do just that, being equivalent to 
$$
\verb&a + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d&    
$$
Similarly \verb%(a+b+c+d)^3% will generate all interactions up to order 3. Notice how there is no interaction of a variable with itself. You could argue that there should be, but actually if you try and make a factor variable interact with itself you get back the original factor, so it is cleaner to simply define metric self-interactions in the same way, which is what is done. 

It is also helpful to be able to specify that some terms otherwise included should be excluded and the \verb+-+ operator allows this to happen. For example, suppose be wanted to exclude the last two interactions in the previous example. \verb%(a+b+c+d)^2-c:d-b:d% would do this. But actually the most common use of \verb+-+ is to exclude the intercept term, by including `\verb+-1+' in the formula.

R function {\tt model.matrix}\index{model.matrix} takes a model formula and a data frame containing the variables referred to by the formula, and constructs the specified model matrix. Here it is in action. First consider the {\tt PlantGrowth} data frame in R, which contains {\tt weight}s of plants grown under 3 alternative treatment {\tt group}s. Suppose we want a simple linear model in which {\tt weight} depends on {\tt group}.
\begin{lstlisting}
> X <- model.matrix(weight~group,PlantGrowth)
> X
   (Intercept) grouptrt1 grouptrt2
1            1         0         0
2            1         0         0
.            .         .         .
10           1         0         0
11           1         1         0
12           1         1         0
.            .         .         .
20           1         1         0
21           1         0         1
22           1         0         1
.            .         .         .
\end{lstlisting}
As a further example, consider the {\tt mpg} data frame provided with {\tt ggplot2}. Actually {\tt mpg} is provided as a modified data frame given the obscure name of a `tibble'\footnote{the documentation for tibbles says that they help with  coding expressively, how inventing new meaningless names helps that is unclear.}. The features of a tibble are not any help here, so we may as well convert back to a regular data frame. Then let's consider a linear model which attempts to model fuel efficiency ({\tt cty} miles per US gallon) using predictors {\tt trans} and {\tt displ}. {\tt trans} is the type of transmission - manual or automatic + other information such as number of gears. For now let's strip out the other information and just consider the two levels . {\tt displ} is total cylinder volume in litres.  
{\small \begin{verbatim}
> mpg1 <- data.frame(mpg) ## convert to regular data frame
> head(mpg1) ## fl is fuel, but levels undocumented, so unusable
  manufacturer model displ year cyl      trans drv cty hwy fl   class
1         audi    a4   1.8 1999   4   auto(l5)   f  18  29  p compact
2         audi    a4   1.8 1999   4 manual(m5)   f  21  29  p compact
3         audi    a4   2.0 2008   4 manual(m6)   f  20  31  p compact
4         audi    a4   2.0 2008   4   auto(av)   f  21  30  p compact
5         audi    a4   2.8 1999   6   auto(l5)   f  16  26  p compact
6         audi    a4   2.8 1999   6 manual(m5)   f  18  26  p compact
> mpg1$trans <- factor(gsub("\\(.*\\)","",mpg1$trans)) ## convert to 2 level
> head(model.matrix(cty~trans+displ,mpg1)) ## get model matrix
  (Intercept) transmanual displ
1           1           0   1.8
2           1           1   1.8
3           1           1   2.0
4           1           0   2.0
5           1           0   2.8
6           1           1   2.8
\end{verbatim}}
\noindent \ldots make sure that this is what you were expecting!

As another example, suppose we want an interaction between {\tt displ} and {\tt trans}
{\small \begin{verbatim}
> head(model.matrix(cty~trans*displ,mpg1))
  (Intercept) transmanual displ transmanual:displ
1           1           0   1.8               0.0
2           1           1   1.8               1.8
3           1           1   2.0               2.0
4           1           0   2.0               0.0
5           1           0   2.8               0.0
6           1           1   2.8               2.8
\end{verbatim}}
\noindent \ldots make sure you can interpret this model. There is a straight line relationship between {\tt cty} and {\tt displ} for automatic transmission cars, given by columns 1 and 3. There is then a second straight line dependence on {\tt displ} for the difference in city miles per gallon between manual and automatic cars. 

\noindent {\bf Exercises:}
\begin{enumerate}
\item Suppose you have a response variable $y_i$ a metric predictor variable ${\bf x} = (0.1, 1.3, 0.4, 1.4, 2.0 , 1.6)\ts$ and a factor variable ${\bf z} = ({\tt a}, {\tt a},{\tt fred},{\tt a},{\tt c},{\tt c})$. Write out the model matrix for the model $y_i = \beta_0 + x_i \beta_1 + \gamma_{z_i}$, making sure that it is identifiable.
\item Consider factor variables ${\bf x} = ({\tt a}, {\tt a}, {\tt b},{\tt a}, {\tt b},{\tt a})\ts$ and ${\bf z} = ({\tt ctrl},{\tt trt},{\tt trt},{\tt trt},{\tt ctrl},{\tt ctrl})\ts$. Write out an identifiable model matrix for the linear model which includes main effects and an interaction of {\bf x} and {\bf z}. 
\item Now suppose that {\bf x} is as in the previous exercise, but ${\bf z}=(1,2,1,3,4,2)$. Write out the model matrix for a model containing main effects of ${\bf x} $ and ${\bf z}$ and their interaction. 
\end{enumerate}

\subsection{Fitting linear models}

The parameters of linear models are estimated by {\em least squares}. Defining $\mu_i = E(y_i)$ (so vector ${\bm \mu} = \X\bp$), we seek parameter estimates $\hat \bp$ that minimise 
$$
\sum_{i=1}^n (y_i -\mu_i)^2 = \|{\bf y} - \X \bp\|^2.
$$  
How that is done mathematically will be covered as an example of matrix computation. For the moment, consider the built in R functions for linear modelling. The main function is {\tt lm}. Here it is in use fitting a basic model for {\tt cty} miles per gallon, in which $E({\tt cty})$ depends linearly on {\tt trans} and {\tt displ}.
\begin{lstlisting}
> m1 <- lm(cty ~ trans + displ, mpg1)
> m1

Call:
lm(formula = cty ~ trans + displ, data = mpg1)

Coefficients:
(Intercept)  transmanual        displ  
    25.4609       0.7842      -2.5520 
\end{lstlisting}    
{\tt lm} takes a model formula, sets up the model matrix and estimates the parameters. Typing the names of the returned model object, causes it to be printed, giving the {\tt lm} function call and reporting the fitted parameter values. Notice how the parameters are identified by the name of the predictor variable they relate to. For factor variables, where there may be several parameters, parameters are referred to by the name of the factor, and the relevant level. 

The first thing to do with a linear model is to check its assumptions. These are essentially that the $\epsilon_i$ are independent with zero mean and constant variance, and less importantly, that they are normally distributed. The usual way to check this is with residual plots. The {\em residuals} are define as 
$$
\hat \epsilon_i = y_i - \hat \mu_i
$$
where $\hat {\bm \mu} = \X \hat \bp$ - they are effectively the estimates of the $\epsilon_i$ . They are plotted in ways that should show up problems with the assumptions. In fact simply calling the {\tt plot} function with a linear model object as argument generates four default residual plots. 
\begin{lstlisting}
> plot(m1)
\end{lstlisting}
\eps{-90}{.5}{mpg-check1.eps}  
The top left plot immediately shows a problem here - the residuals do not look like zero mean independent random deviates. There is a clear pattern in the mean of the residuals (the red curve is a running smoother, helping to visualize that trend). Interpreting the remaining plots is pointless given this problem, but here is what they show, anyway. The top right plot is a normal quantile-quantile plot - sorted residuals are plotted against quantiles of a standard normal distribution: close to a straight line is expected for normal data (but normality is not the most important assumption). The lower left plot standardizes the residuals so that any trend in the mean indicates violation of the constant variance assumption. The final plot indicates whether some points have a combination of high residual and unusual predictor values that makes them particularly influential on the whole model fit --- see the generalized regression models course for more on these. 

What might cause the systematic pattern in the residuals? One possibility is that the model for the relationship between {\tt cty} and {\tt displ} is wrong. To check, plot the residuals against displacement.
\begin{lstlisting}
> plot(mpg1$displ,residuals(m1))
\end{lstlisting}
\eps{-90}{.5}{mpg-resid1.eps}  
\ldots clearly there is a pattern suggesting that the relationship should at least have been quadratic.
\begin{lstlisting}
m2 <- lm(cty~trans+displ+I(displ^2),mpg1)
\end{lstlisting}
\ldots results in a much less problematic set of residual plots, so we can interrogate the model further. For example the {\tt summary} function can be used to report the parameter estimates, their standard deviations (standard errors), and the p-values associated with the hypothesis test that each true parameter value in turn is zero (the `t values' are the test statistic used for this).  
{\small
\begin{verbatim}
> summary(m2)

Call:
lm(formula = cty ~ trans + displ + I(displ^2), data = mpg1)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.5988 -1.4620 -0.0174  1.1067 12.4922 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 34.98583    1.27874  27.360   <2e-16 ***
transmanual  0.45431    0.32911   1.380    0.169    
displ       -8.23552    0.71766 -11.475   <2e-16 ***
I(displ^2)   0.75213    0.09366   8.031    5e-14 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.257 on 230 degrees of freedom
Multiple R-squared:  0.7224,	Adjusted R-squared:  0.7188 
F-statistic: 199.5 on 3 and 230 DF,  p-value: < 2.2e-16
\end{verbatim}}
Clearly there is strong evidence for the {\tt displ}acement effect, but no real evidence of an additional effect of {\tt trans}mission. Let's check whether that conclusion might result from being too simplistic about the {\tt trans} effect, by allowing an interaction. And let's then test the null hypothesis that a model with no {\tt trans} effect is correct, against the alternative that the full interaction model is needed, as follows:
{\small
\begin{verbatim}
> m3 <- lm(cty~(trans+displ+I(displ^2))^2,mpg1)
> m0 <- lm(cty~displ+I(displ^2),mpg1)
> anova(m0,m3) ## F ratio test of H_0: m0 is correct vs H_1: we need m3
Analysis of Variance Table

Model 1: cty ~ displ + I(displ^2)
Model 2: cty ~ (trans + displ + I(displ^2))^2
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1    231 1181.1                           
2    227 1147.0  4    34.092 1.6867 0.1539
\end{verbatim}}
So no evidence for a {\tt trans} effect of any sort. But what about this?
{\small
\begin{verbatim}
> summary(lm(cty~trans,mpg1))

Call:
lm(formula = cty ~ trans, data = mpg1)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.6753 -2.9682  0.0318  2.3247 16.3247 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  15.9682     0.3248  49.168  < 2e-16 ***
transmanual   2.7072     0.5662   4.782 3.09e-06 ***
...
\end{verbatim}}
If we don't consider {\tt displ} then there appears to be a strong and significant effect of {\tt trans}. Why? Well, on examination of the data, it appears that manual cars tend to have smaller engines than automatic ones. And because smaller engines have lower fuel consumption, manual cars therefore appear to be more efficient. But if you control for engine size ({\tt displ}) then there is no additional effect of transmission. 

So are manual cars more efficient or not? That's not so easy to answer. For a given engine size it appears not, but for a given performance level the answer might well be yes: the automatics need a larger engine for the same performance, and therefore are less efficient for a given performance level. Clearly the interpretation of statistical model analyses require care. It requires particular care not to interpret effects as casual without alot more evidence than a statistical model of observational data can provide. In this example the {\tt displ} variable that probably drives the apparent {\tt trans} effect is one of the measured predictors. But in many cases it's quite possible that a variable we have not measured is driving both the response variable and the predictor, with no causal relationship between the response and the predictor at all.   

This can really matter. For example there has been a good deal of work attempting to prove that lock down measures, and only lock down measures, are what controlled Covid-19 in various places at various times in the pandemic. But ultimately almost all that these studies show is an {\em association} between lockdown and reduced Covid transmission. It is impossible to rule out that the spontaneous changes in people's behaviour, which included clamouring for lockdowns on social media, were enough both to stop transmission increasing and to push governments into action. Nor to rule out that measures short of full lockdown, which were invariably associated with, but preceded, full lockdown, were what caused infections to start to decline.        

\noindent {\bf Exercise:} Use {\tt lm} to fit the simple linear model for the {\tt PlantGrowth} data in which {\tt weight} depends on experimental treatment {\tt group}. Check the residual plots for the model, and then test whether {\tt weight} depends on {\tt group}. If it does, interpret the model parameters. If the results are not clear cut, consider resetting the levels of the {\tt group} factor to help. 

\section{R classes}

The linear modelling routines provide a good introduction to {\em classes} in R. The {\tt lm} function
returns an object of class \verb+"lm"+, and typing the name of such an object produces the short summary of the model we saw above. What it doesn't do is to print the entire contents of the object. You can check that by looking at \lstinline+str(m1)+, for example: {\tt m1} is packed full of stuff. So what is happening when {\tt m1} gets printed?

R provides a simple form of object orientation, where some functions are defined as {\em generic} functions, with {\em methods} specific to the class of their first argument. So the {\tt print} function has a default method, but also several hundred other methods (type \lstinline+methods(print)+ to see them), including one for objects of class \lstinline+"lm"+, called \lstinline+print.lm()+. It is this latter method that is invoked when you type \lstinline+print(m1)+, or just \lstinline+m1+ at the R command prompt. Similarly \lstinline+plot(m1)+ actually uses the method function \lstinline+plot.lm()+, and \lstinline+summary(m1)+ really calls \lstinline+summary.lm+. The function {\tt residuals} is also generic, and again it is the method function \lstinline+residuals.lm+ that is invoked by \lstinline+residuals(m1)+.  \index{object orientation}

As a simple example, suppose we want to create a class, \lstinline+"foo"+, of objects whose structure is a list, with elements {\tt a} and {\tt sd} representing some estimates and their standard deviations, and we want the print method for such objects to print a simple approximate confidence interval for the estimate and its standard deviation. Here is an appropriate print method function.
\begin{lstlisting}
print.foo <- function(x) {
  cat(x$a-1.96*x$sd,"\n")
  cat(x$a+1.96*x$sd,"\n")
}
\end{lstlisting}
And here is an example of it in action
\begin{lstlisting}
> a <- list(a = 1:3, sd = c(.1,.2,.1))
> class(a) <- "foo"
> print(a)
0.8 1.6 2.8 
1.2 2.4 3.2
\end{lstlisting}
Note that an object can have several classes arranged in a hierarchy, usually when an object somehow extends or generalizes an existing class, and can therefore re-use some of the methods from that original class. This is referred to as {\em inheriting} from an existing class. For example R has a {\tt glm} function for fitting {\em generalized linear models}. It returns objects of class \lstinline+"glm"+ {\em inheriting} from class \lstinline+"lm"+. e.g.\index{glm}
\begin{lstlisting}
> x <- runif(20); y <- rpois(20,exp(2*x))
> m <- glm(y~x,family=poisson)
> class(m)
[1] "glm" "lm"
\end{lstlisting}
When a method function is called with an argument of class \lstinline+"glm"+, R first looks to see if there is a method function for class  \lstinline+"glm"+ objects. If not then it looks for the method function for class \lstinline+"lm"+ objects, only resorting to the default method function if it finds neither of these. So, for example, \lstinline+summary(m)+ would invoke method function \lstinline+summary.glm+, but \lstinline+plot(m1)+ invokes method function \lstinline+plot.lm+, while \lstinline+AIC(m1)+ invokes method function \lstinline+AIC.default+. \index{function!method}\index{function!generic}\index{class}\index{class!inherits}

The fact that objects can inherit from several classes means that you should not test for class membership using code like \lstinline+if (class(a)=="foo")+ but rather use \lstinline+if inherits(a,"foo")+. 

\section{Matrix computation \label{sec:matrix}}

Both the linear model and the notion of tidy data rely on matrices, and in fact matrix computation is fundamental to many statistical modelling and statistical learning methods. You need to know some basics, starting with multiplication. You might think there is not much to say about this, but consider the following example:\index{matrix!efficiency}\index{matrix!multiplication}
\begin{lstlisting}
> n <- 2000; y <- runif(n) ## example vector
> A <- matrix(runif(n*n),n,n) ## example matrix
> B <- matrix(runif(n*n),n,n) ## example matrix
> system.time(a1 <- A %*% B %*% y)
   user  system elapsed 
  1.435   0.003   1.629 
> system.time(a2 <- A %*% (B %*% y))
   user  system elapsed 
  0.018   0.000   0.018 
> range(a1-a2) ## results the same
[1] -2.852175e-09  3.026798e-09
\end{lstlisting}
Why is \lstinline+A %*% B %*% y+ so much slower than \lstinline+A %*% (B %*% y)+? Consider the definition of matrix multiplication\footnote{you should have a mental picture of this, going across the rows of $\bf A$ and down the columns of $\bf B$.  }:
$$
C_{ij} = \sum_{k=1}^n A_{ik}B_{kj}. 
$$
Each element $C_{ij}$ involves $n$ multiplications and $n$ additions, and for the above case there are $n^2$ elements $C_{ij}$. So there are $2n^3$ computations to perform. Often we are not much concerned about constants such as 2, and simply write that the cost is $O(n^3)$ (`order $n^3$'). What about the matrix vector product?
$$
a_i = \sum_{k=1}^n B_{ik} y_k,
$$
so each of the $n$ elements of $\bf a$ also costs $n$ multiplications and $n $ additions. So the cost is now $O(n^2)$. The origin of the difference in timings should be clear. When forming {\tt a1}, R worked from left to right, first forming the product of $\bf A$ and $\bf B$, and only then the product of the result with $\bf y$, so the dominant  cost is $O(n^3)$. When forming {\tt a2} the brackets force the vector ${\bf By}$ to be formed first, and then the product of $\bf A $ with the resulting matrix is formed. The cost of both steps is $O(n^2)$. This accounts for the speed up, but you will notice that the speed up was not by a factor of 2000. That's because of other overheads in the process, which are not affected by the reduction in the operations count.  

In addition to matrix multiplication (\lstinline+%*%+) there are several other basic operations:
\begin{trivlist}
\item \verb+t()+ transposes a matrix. So \verb+t(A)+ forms ${\bf A}\ts$.
\item \verb+crossprod(A)+ forms the crossproduct ${\bf A}\ts{\bf A}$ as efficiently as possible
\footnote{{\tt t(A)\%*\%A} has about twice the computational cost. Why?}.
\item \verb+diag()+ allows the leading diagonal of a matrix to be be extracted or set. e.g. \lstinline+diag(A) <- 1:3+
sets the leading diagonal of $3 \times 3 $ matrix $\bf A$ to the integers 1 to 3. {\tt sdiag} in package {\tt mgcv} does the same for sub- or super- diagonals.
\item Operators such as \verb%+ - * /% operate element wise on matrices. 
\item \verb+drop(x)+ drops dimensions of an array having only one level. e.g. if {\tt x} is an $n \times 1$ matrix, \verb+drop(x)+ is an $n$ vector. 
\end{trivlist}\index{matrix!transpose}\index{matrix!cross product}\index{matrix!diagonal}\index{drop}\index{matrix!trace}

\noindent {\bf Exercise:} The {\em  trace} of a matrix is the sum of its leading diagonal elements. i.e. $\text{tr}({\bf A}) = \sum_i A_{ii}$. Suppose you have two (compatible) matrices $\bf A$ and $\bf B$ and want to evaluated $\text{tr}({\bf AB})$. Clearly forming $\bf AB$ for this purpose is wasteful, as the off diagonal elements of the product contribute nothing. By considering the summation that defines $\text{tr}({\bf AB})$, find a way to evaluate it efficiently using regular multiplication, transposition and summation.  



\subsection{General solution of linear systems}

Suppose we want to solve ${\bf Ab} = {\bf c}$ for $\bf b$, where $\bf A$ is an $n \times n$ full rank matrix, and $\bf c$ is an $n $ vector, or an $n \times m$ matrix ({\bf b} is obviously of the same dimension).
\begin{lstlisting}
c <- solve(A,b)
\end{lstlisting} 
will do this. Formally the solution is ${\bf c}={\bf A}^{-1}{\bf b}$, but we almost never form ${\bf A}^{-1}$ explicitly, as it is less efficient computationally, and the resulting {\bf c} typically has higher numerical error. However, given a routine for solving linear systems, it is obviously possible to use it to solve ${\bf AA}^{-1} = {\bf I} $ for ${\bf A}^{-1}$, and \lstinline+Ai <- solve(A)+ will do just that. For most statistical work {\tt solve} is suboptimal in various ways, so let's consider the matrix methods that do tend to be more useful. \index{solve}\index{linear equations} \index{matrix!inverse}

\subsection{Cholesky decomposition}

A symmetric $n \times n$ matrix, $\bf A$, is {\em positive definite} if ${\bf x}\ts {\bf Ax} > 0$ for any non-zero $\bf x$. This is exactly equivalent to it having only positive eigenvalues. {\bf A} is {\em positive semi-definite} if  ${\bf x}\ts {\bf Ax} \ge 0$ for any non-zero $\bf x$ (equivalently all eigenvalues are $\ge 0$). Positive (semi) definite matrices are important in statistics because all covariance matrices are in this class. Recall that a covariance matrix, $\Sigma$, of a random vector $\bf X$ is the matrix containing the covariance of $X_i$ and $X_j$ as its element $\Sigma_{ij}$. That is $\Sigma_{ij} = E[\{X_i - E(X_i)\}\{X_j - E(X_j)\}]$, or equivalently \index{matrix!positive definite}\index{covariance}\index{covariance!matrix}
$$
{\bm \Sigma} = E[\{\X - E(\X)\}\{\X-E(\X)\}\ts].
$$
Various tasks are especially easy and efficient with positive definite matrices. This is largely because a positive definite matrix can be decomposed into the crossproduct of an upper triangular matrix with itself. That is we can write ${\bm \Sigma} = {\bf R}\ts{\bf R}$, where $\bf R$ is upper triangular, meaning that $R_{ij}=0$ if $i>j$. It's easy to see how this works on a small example\index{Cholesky decomposition}\index{matrix!Cholesky decomposition}
$$
\bmat{ccc} 
R_{11} & 0 & 0 \\ R_{12} & R_{22} & 0 \\ R_{13} & R_{23} & R_{33}  
\emat \bmat{ccc} 
R_{11} & R_{12} & R_{13} \\
0 & R_{22} & R_{23} \\
0 & 0 & R_{33}
\emat = \bmat{ccc} 
\Sigma_{11} & \Sigma_{12} & \Sigma_{13} \\
\Sigma_{12} & \Sigma_{22} & \Sigma_{23} \\
\Sigma_{13} & \Sigma_{23} & \Sigma_{33}
\emat
$$
Starting at top left, and working across the columns and down the rows we have
\begin{align*}
{\bf R}_{11}^2 &= \Sigma_{11}\\
R_{11}{\bf R}_{12} &= \Sigma_{12}\\
R_{11}{\bf R}_{13} &= \Sigma_{13}\\
R_{12}^2 + {\bf R}_{22}^2 &= \Sigma_{22}\\
R_{12} R_{13} + R_{22}{\bf R}_{23} &= \Sigma_{23}\\
R_{13}^2 + R_{23}^2 + {\bf R}_{33}^2 &= \Sigma_{33} 
\end{align*}
where at each row the only unknown is shown in bold. For an $n\times n$ matrix this becomes (defining $\sum_{i=1}^0 x_i \equiv 0$)
$$
R_{ii} = \sqrt{\Sigma_{ii} - \sum_{k=1}^{i-1} R_{ki}^2} \text{    and    }
R_{ij} = \frac{\Sigma_{ij} - \sum_{k=1}^{i-1} R_{ki}R_{kj}}{R_{ii}}.
$$
An immediate use is to compute the determinant 
$$
|{\bf \Sigma}| = |{\bf R}\ts{\bf R}| = |{\bf R}|^2 = \left ( \prod_{i=1}^n R_{ii} \right )^2.
$$
or better the log determinant $\log |{\bf \Sigma}| = 2 \sum_{i=1}^n \log R_{ii}$. \index{determinant}

Solving is also easy once we are dealing with triangular factors. To solve ${\bm \Sigma} {\bf x} = {\bf y}$ for {\bf x}, we just need to solve ${\bf R}\ts {\bf Rx} = {\bf y} $. That is done by solving ${\bf R}\ts {\bf z} = {\bf y}$ for {\bf z} and then ${\bf Rx} = {\bf z}$ for $\bf x$. Again a small example makes it clear how easy this is:
$$
\bmat{ccc} 
R_{11} & 0 & 0 \\ R_{12} & R_{22} & 0 \\ R_{13} & R_{23} & R_{33}  
\emat \bmat{c} z_1 \\ z_2 \\ z_3 \emat = \bmat{c} y_1 \\ y_2 \\ y_3 \emat.
$$ \index{forward solve}
Obviously working from the top we have the following system, with one unknown (in bold) at each line
\begin{align*}
R_{11} {\bf z}_1 &= y_1\\
R_{12} z_1 + R_{22} {\bf z}_2 & = y_2\\
R_{13} z_1 + R_{23} z_2 + R_{33} {\bf z}_3 &= y_3 
\end{align*}
The generalization of this {\em forward solve} is obvious, as is the equivalent {\em  back solve}, where we start from the last line and work back solving for $x_j$\ldots
$$
\bmat{ccc} 
R_{11} & R_{12} & R_{13} \\
0 & R_{22} & R_{23} \\
0 & 0 & R_{33}
\emat \bmat{c} x_1 \\ x_2 \\ x_3 \emat = \bmat{c} z_1 \\ z_2 \\ z_3 \emat.
$$ \index{backward solve} \index{matrix!triangular solve}
In R, function {\tt chol} computes the Cholesky factor, while {\tt backsolve} and {\tt forwardsolve} solve with triangular factors. One advantage of the Cholesky factorization is speed. Solving linear systems of dimension $n$ costs $O(n^3)$ operations whether we use {\tt chol} or {\tt solve}, but the constant of proportionality is smaller for Cholesky. For example.
\begin{lstlisting}
> n <- 2000
> A <- crossprod(matrix(runif(n*n),n,n)) ## example +ve def matrix
> b <- runif(n)  ## example n vector
> system.time(c0 <- solve(A,b)) ## solve 
   user  system elapsed 
  0.577   0.000   0.577 
> system.time({R <- chol(A);    ## solve with cholesky
+ c1 <- backsolve(R,forwardsolve(t(R),b))})
   user  system elapsed 
  0.312   0.000   0.313 
> range(c0-c1)/norm(A) ## confirm same result
[1] -3.673905e-12  3.970016e-12
\end{lstlisting}

As an example, consider evaluating the log of the multivariate normal p.d.f.
$$
- \frac{1}{2}({\bf y} - {\bm \mu})\ts {\bm \Sigma}^{-1} ({\bf y} - {\bm \mu}) - \frac{1}{2} \log |{\bm \Sigma}| - \frac{n}{2}\log(2 \pi)
$$ 
Doing this by using R functions {\tt solve} and {\tt determinant} is much slower than is necessary. In particular {\tt determinant} requires $O(n^3)$ operations, whereas once we have the Cholesky factor, determinant calculation is $O(n)$. Notice also how 
$$
({\bf y} - {\bm \mu})\ts {\bm \Sigma}^{-1} ({\bf y} - {\bm \mu}) = {\bf z}\ts {\bf z} \text{   where  } {\bf z} = {\bf R}\its ({\bf y} - {\bm \mu})
$$
This suggests the following function (recall $|{\bf AB}| = |{\bf A}||{\bf B}|$, so $|{\bm \Sigma}|=|{\bf R}|^2$)\index{determinant}
\begin{lstlisting}
ldmvn <- function(y,m,S) { 
## Cholesky based evaluation of log density of MVN at y. 
## m is mean and S the covariance matrix.
  R <- chol(S) ## Get Cholesky factorization R'R = S
  z <- forwardsolve(t(R),y-m) ## R^{-T}(y-m)
  -sum(z^2)/2 - sum(log(diag(R))) - length(m)*log(2*pi)/2
} ## ldmvn
\end{lstlisting}
Here is a small example using it
\begin{lstlisting}
> y <- c(3,4.5,2); m <- c(.6,.8,1.2)
> S <- matrix(c(1,.5,.3,.5,2,1,.3,1,1.5),3,3)
> ldmvn(y,m,S)
[1] -8.346052
\end{lstlisting}

\noindent {\bf Exercise:} If ${\bf y } = {\bf Ax}$ and $\bf x$ has covariance matrix ${\bm \Sigma}_x$, show that the covariance matrix of $\bf y$ is ${\bm \Sigma}_y = {\bf A}{\bm \Sigma}_x{\bf A}\ts$. This is a basic and fundamental result re-used repeatedly in statistics and data science. \index{covariance matrix!of a linear transform} 

\noindent {\bf Exercise:} A linear transformation of multivariate normal random variables is also multivariate normal. Write a function to generate {\tt n} draws from a multivariate normal distribution with mean {\tt m} and covariance matrix {\tt S}, by transformation of independent $N(0,1)$ deviates, generated by {\tt rnorm}. Also write code to test that it is working (function {\tt cov} might be helpful).  


\subsection{QR decomposition \label{sec:qr}}

An orthogonal matrix $\bf Q$ is one with the property that ${\bf Q}\ts {\bf Q} = {\bf QQ}\ts = {\bf I}$. Any full rank square matrix, $\bf A$, can be decomposed into the product of an orthogonal matrix and an upper triangular matrix: ${\bf A} = {\bf QR}$. So the solution to ${\bf A}\bp = {\bf y}$ is the solution to ${\bf R}\bp = {\bf Q}\ts {\bf y}$ --- it involves multiplication be an orthogonal matrix, and a back-solve. 

But suppose that instead of full rank square matrix $\bf A$ defining the l.h.s. of the system we want to solve, we have $n \times p$, rank $p$, matrix $\X$, where $n > p$. Clearly we can not expect to find a solution to $\X\bp={\bf y}$ in general, since we have more equations to satisfy than there are elements of $\bp$. But what about $\X\bp \approx {\bf y}$? We can solve that provided we are a little more precise about what `$\approx$' means. In particular consider finding $\bp$ to minimize the  mean square difference between $\X\bp$ and $\bf y$. That is we want to minimize the squared Euclidean length of ${\bf y} - \X \bp$, the sum of squares of differences between ${\bf y} $ and $\X \bp$. i.e. we seek
$$
\hat \bp = \underset{\beta}{\text{argmin }} \|{\bf y} - \X \bp \|^2
$$
where $\|{\bf v}\|^2 = {\bf v}\ts{\bf v} = \sum_{i=1}^n v_i^2$ is the squared Euclidean norm/Euclidean length of a vector, $\bf v$. 

Solution of this problem involves the fact that we can also decompose $\bf X$ into the product of an $n \times n$ orthogonal matrix, $\bm{{\cal Q}}$, and an upper triangular matrix, so that
$$
{\bf X} = \bm{{\cal Q}} \bmat{c} {\bf R} \\ {\bf 0} \emat
$$ 
${\bf R}$ is $p \times p$ upper triangular. Defining $\bf Q$ to be the first $p$ columns of $\bm{{\cal Q}}$, we can also write the decomposition as $\X = {\bf QR}$, of course. Now a defining property of orthogonal matrices is that they act as rotations/reflections, that is $\|\bm{{\cal Q}} {\bf v}\|^2 = \|{\bf v}\|^2 $, for any ${\bf v}$. It follows that 
$$
\|{\bf y} - \X \bp \|^2 = \|\bm{{\cal Q}}\ts{\bf y} - \bm{{\cal Q}}\ts\X \bp \|^2 = \left \|\bm{{\cal Q}}\ts{\bf y} -  \bmat{c} {\bf R} \\ {\bf 0} \emat \bp  \right \|^2 = \|{\bf Q}\ts {\bf y} - {\bf R}\bp \|^2 + \|{\bf r}\|^2
$$
where $\bf r$ is the vector containing the final $n-p $ elements of $\bm{{\cal Q}}\ts{\bf y}$. Since $\bf r$ does not depend on $\bp$, then $\hat \bp$ must be the minimizer of  $\|{\bf Q}\ts {\bf y} - {\bf R}\bp \|^2$. But this can be reduced to zero by setting \index{linear model!least squares}\index{QR decomposition}\index{matrix!QR decomposition}
$$
\hat \bp = {\bf R}^{-1} {\bf Q}\ts {\bf y}.
$$
$\hat \bp$ is the least squares estimate of the parameters of a linear model, and the given formula is how least squares estimates are computed in practice (obviously using a triangular solve, not explicit formation of ${\bf R}^{-1}$). This approach has much better numerical stability than using the theoretical formula $\hat \bp = (\X\ts \X)^{-1} \X \ts {\bf y}$ that you may have seen previously. It is easy to show that $\hat \bp$ is unbiased (that is $E(\hat \bp) = \bp$). Further, since the covariance matrix of $\bf y$ is ${\bf I}\sigma^2$, the basic result on the covariance matrix of a linear transform shows that the covariance matrix of $\hat \bp$ is 
$$
{\bf \Sigma}_{\hat \bp} = {{\bf R}^{-1}{\bf R}\its} \sigma^2. 
$$ 
An unbiased estimate of $\sigma^2$ is $\hat \sigma^2 = \|\hat {\bm \epsilon}\|^2/(n-p) = \|{\bf y} - \X \hat \bp \|^2/(n-p) = \|{\bf r}\|^2/(n-p)$.


The R function {\tt qr} computes a QR decomposition. It returns an object of class \verb+"qr"+ containing the decomposition stored in a compact form. The components of the decomposition can be accessed using functions \lstinline+qr.R+ and \lstinline+qr.Q+ where the latter has optional argument \lstinline+complete+ which is set to {\tt TRUE} if the full $n \times n$ matrix $\bm{{\cal Q}}$ is required, rather than the default $n \times p$ matrix {\bf Q}. In fact explicitly extracting $\bm{{\cal Q}}$ or ${\bf Q}$ is computationally very inefficient, if all we need is the product of a vector and the orthogonal factor. In that case it is far more efficient to use functions \lstinline+qr.qy+ and \lstinline+qr.qty+ which take the {\tt qr} object as first argument and a vector, {\tt y}, as second argument, and return respectively $\bm{{\cal Q}}{\bf y}$ or  $\bm{{\cal Q}}\ts {\bf y}$. 

Here are the computations for obtaining least squares estimates, applied to the simple {\tt PlantGrowth} model from section \ref{sec:lm}. It uses the fact that ${\bf Q}\ts {\bf y}$ is the first $p$ elements of $\bm{{\cal Q}}\ts {\bf y}$
\begin{lstlisting}
> X <- model.matrix(~group,PlantGrowth) ## model matrix (note: response not needed!)
> qrx <- qr(X)              ## get QR decomposition of model matrix
> y <- PlantGrowth$weight   ## response
> p <- ncol(X)              ## number of parameters  
> beta <- backsolve(qr.R(qrx),qr.qty(qrx,y)[1:p]) ## R^{-1} Q^T y
> beta                      ## least squares estimates
[1]  5.032 -0.371  0.494
\end{lstlisting}  
This is exactly the computational approach used by {\tt lm}. \index{least squares}

\noindent {\bf Exercise:} Occasionally explicit inverses are needed. Write code that uses {\tt backsolve} to find an explicit inverse of an upper triangular matrix, $\bf R$, and test it.   

\noindent {\bf Exercise:} Using the fact that $|{\bf AB}|=|{\bf A}||{\bf B}|$ and $|{\bf A}| = |{\bf A}\ts|$, show that for an orthogonal matrix, $\bf Q$, $|{\bf Q}|=\pm 1$. Hence run the code \lstinline@set.seed(0);n<-100; A <- matrix(runif(n*n),n,n)@ and use the QR decomposition to evaluate the magnitude\footnote{The sign of the determinant depends on whether $|{\bf Q}|$ is positive or negative. This can also be determined cheaply, but takes us too far afield: basically $\bf Q$ is constructed from the product of $n-1$, rank one orthogonal {\em Householder} matrices, each with determinant -1, so $|{\bf Q}| = (-1)^{n-1}$.} of the determinant of {\tt A}. Compare your answer to \lstinline+determinant(A,log=FALSE)+. Now increase the size of the elements of {\tt A} using \lstinline+A <- A*1000+ and repeat the exercise. This should emphasis the problem with working directly with the determinant, so now use the QR decomposition of {\tt A} to obtain the log of the magnitude of the determinant of {\tt A}. \index{determinant}

\subsection{Pivoting and triangular factorizations}
 
When computing factorizations involving triangular matrices, it turns out that the numerical stability of the factorization is affected by the order in which the rows and/or columns appear in the original matrix. But for all least squares and equation solving tasks the order in which rows and columns are present in the matrix doesn't actually matter - we can always re-order and get the same result (provided we take account of the re-ordering). So rows and/or columns can be re-ordered for maximum numerical stability - that is for minimum numerical error in the final answer. This is known as {\em pivoting}. {\tt chol} and {\tt qr} have options allowing pivoting to be used. Using them requires extra book-keeping, and takes us too far off topic in this course: but for serious computation you should be aware that pivoting is possible, and can be necessary. \index{matrix!pivoting}    
 
\subsection{Symmetric Eigen-decomposition}

Any real symmetric matrix, $\bf A$, has an {\em eigen-decomposition} 
$$
{\bf A} = {\bf U}\bm{\Lambda}{\bf U}\ts
$$
where {\bf U} is an orthogonal matrix, the columns of which are the (normalized) eigenvectors of $\bf A$, and $\bm \Lambda$ is a diagonal matrix of the eigenvalues of $\bf A$, arranged so that $\Lambda_{ii}\ge\Lambda_{i+1,i+1}$. R function {\tt eigen} computes eigen-decompositions, as we saw in section \ref{sec:function}. Rather than provide further simple examples of the use of {\tt eigen}, let's look at a major application in multivariate statistics: principle components analysis. \index{eigen}

\subsubsection{Eigen-decomposition of a covariance matrix: PCA}

The eigen-decomposition of a covariance matrix is often known as Principle Components Analysis (PCA). Consider a set of observations, ${\bf x}_1, {\bf x}_2, \ldots, {\bf x}_n$ of a $p$ dimensional random vector $\bm{{\cal X}}$, and suppose we create an $n \times p$ matrix ${\bf X}$ whose ith row is ${\bf x}_i$. The estimated variance of ${\cal X}_j$ is $\sum_{i=1}^n (X_{ij} - \bar X_j)^2/(n-1)$, and the estimated covariance of ${\cal X}_j$ and ${\cal X}_k$ is $\sum_{i=1}^n (X_{ij} - \bar X_j)(X_{ik} - \bar X_k)/(n-1)$ where $\bar X_j = n^{-1}\sum_{i=1}^n X_{ij}$. Hence if $\tilde {\bf X}$ is $\bf X$ with the column mean subtracted from each column, then the estimated sample covariance matrix for  $\bm{{\cal X}}$ is simply\index{PCA}\index{eigen}
$$
\hat {\bm \Sigma}_{\cal X} = \tilde {\bf X}\ts \tilde {\bf X}/(n-1).
$$ 
We can form an eigen decomposition $\hat {\bm \Sigma}_{\cal X} = {\bf U}{\bm \Lambda}{\bf U}\ts$ (${\bm \Lambda} = \text{diag}({\bm \lambda})$ and $\lambda_i>\lambda_{i+1}$), and use this to define a new random vector ${\bf z} = {\bf U}\ts \bm{{\cal X}}$. Now $\bf U$ is orthogonal, so using the standard result for a linear transformation of a covariance matrix, we have that the estimated covariance matrix for $\bf z$ is
$$
\hat {\bm \Sigma}_z = {\bf U}\ts \hat {\bm \Sigma}_{\cal X} {\bf U} = 
{\bf U}\ts{\bf U}{\bm \Lambda}{\bf U}\ts {\bf U} = {\bm \Lambda}.
$$
i.e. the elements of $\bf z$ are uncorrelated, and the estimated variance of $z_j$ is $\lambda_j$. By the orthogonality of $\bf U$ we can write ${\bf Uz}=\bm{{\cal X}}$. i.e. $\bm{{\cal X}}$ is a weighted sum of the orthogonal columns of $\bf U$, where the weights are uncorrelated random variables with variances given by the $\lambda_j$. Hence the variability of $\bm{{\cal X}}$ has been decomposed additively into orthogonal {\em principle components}, ${\bf U}_{\cdot j}$ with associated variance $\lambda_j$. The principle component with the largest variance, $\lambda_1$, contributes the most variance, the component with the next largest variance $\lambda_2$ is next, and so on.      

We can of course view $z_j$ as the co-ordinate of $\bm{{\cal X}}$ on the axis defined by ${\bf U}_{\cdot j}$, which suggests computing such co-ordinates for each original observations. That is computing ${\bf Z} = {\bf XU}$ where the ith row of $\bf Z$ gives the co-ordinates of observation ${\bf x}_i$ relative to the axes defined by $\bf U$. Now, by construction these co-ordinates have sample variance $\lambda_1, \lambda_2,\dots, \lambda_p$, with the first co-ordinate having most variability, the next having the second most variability and so on. This implies that we can use the co-ordinates for {\em dimension reduction}. Retaining the first $m$ components will capture $\sum_{i=1}^m \lambda_i/\sum_{i=1}^p \lambda_i$ of the variance in the original data. Often this proportion can be very high for $m \ll p$. This dimension reduction is equivalent to replacing the covariance matrix $ \hat {\bm \Sigma}_{\cal X}$ with ${\bf U}_m {\bm \Lambda}_m {\bf U}_m\ts$ where ${\bf U}_m$ denotes the first $m$ columns of ${\bf U}$ and ${\bm \Lambda}_m$ the first $m$ rows and columns of $\bm \Lambda$.   

The classic dataset for illustrating this is the {\tt iris} data, available in R. This gives 4 measurements for each of 50 iris flowers of 3 species (so $n=150$, $p=4$). Plotting the raw data is is not clear to what extent the measurements can really separate the species:\index{iris data}
\begin{lstlisting}
pairs(iris)
\end{lstlisting}
\eps{-90}{0.5}{iris.eps}
Let's see if the principle components do any better. Function {\tt sweep} can be used to column centre the data matrix.
\begin{lstlisting}
X <- sweep(as.matrix(iris[,1:4]),2,colMeans(iris[,1:4])) ## col centred data matrix
ec <- eigen(t(X)%*%X/(nrow(X)-1))    ## eigen decompose the covariance matrix
U <- ec$vectors;lambda <- ec$values  ## exract eigenvectors and values
Z <- X %*% U;                        ## the principle co-ordinated 
plot(Z[,1],Z[,2],col=c(1,2,4)[as.numeric(iris$Species)],main="iris PCA",
     xlab="PCA 1",ylab="PCA 2")      ## plot first two components  
\end{lstlisting}
\eps{-90}{0.5}{iris-pca.eps}
The three species are quite clearly separated now. Notice how the range of the first component is substantially greater than that of the second component --- as expected, since the components are in decreasing order of variance explained. Here is some code to re-iterate the link between the ${\bf Z}$ column variances and ${\bm \lambda}$, and assess what proportion of the variance is explained by (can be attributed to) the first 2 principle components (columns of {\bf Z}).    
\begin{lstlisting}
> apply(Z,2,var);lambda ## compare Z column variances to lambda
[1] 4.22824171 0.24267075 0.07820950 0.02383509
[1] 4.22824171 0.24267075 0.07820950 0.02383509
> sum(lambda[1:2])/sum(lambda) ## proportion variance explained by components 1 and 2
[1] 0.9776852
\end{lstlisting}

\noindent {\bf Exercise:} An obvious alternative is to standardize the original variables to have unit variance and then perform PCA. In that case we are really working with the correlation matrix for the variables, rather than the covariance matrix. Try this and compare the separation of the species now. Examining the eigenvalues, suggest a possible partial explanation for any difference.

\subsection{Singular value decomposition}

PCA is an example of {\em dimension reduction}: finding a lower dimensional representation of high dimensional data. In a sense much of statistics and data-science is about dimension reduction: about finding ways of representing complex data using simpler summaries, without the loss of important information. But for the moment, let's stick with finding lower dimensional representations of matrices. One output of PCA was low rank approximations to covariance matrices - but what about low rank approximations to non-square matrices? {\em Singular value decomposition} gives a way to achieve this. Any $n \times p$ matrix, $\bf X$ can be decomposed
$$
\X = {\bf U}{\bf DV}\ts
$$ 
where $\bf U $ is an $n \times p $ matrix with orthogonal columns, $\bf V$ is an orthogonal matrix, and $\bf D$ is a diagonal matrix of the {\em singular values} of $\bf X$ --- the positive square roots of the eigenvalues of ${\bf X}\ts {\bf X}$. $\bf D$ is arranged so that $D_{ii} \ge D_{i+1,i+1}$ (that is the singular values are in decreasing order on the diagonal). Suppose ${\bf U}_r$ and ${\bf V}_r$ denote the matrices consisting of the first $r$ columns of $\bf U$ and $\bf V$, while ${\bf D}_r$ is the first $r$ rows and columns of $\bf D$. Then
$$
{\bf U}_r{\bf D}_r{\bf V}_r\ts
$$
is the best rank $r$ approximation to $\X$ (in a sense that is beyond our scope here). The {\tt svd} function in R returns the singular value decomposition of a matrix. \index{singular value decomposition}

\noindent {\bf Exercise:} The formal expression for the least squares estimates of a linear model is $\hat \bp = (\X\ts \X)^{-1} \X \ts {\bf y}$. Find a simplified expression for this in terms of the components of the SVD. Use you expression to compute the least squares estimates for the {\tt PlantGrowth} model of section \ref{sec:qr}, one more time.  

\section{Design, Debug, Test and Profile}

Having covered enough R, and a sufficient diversity of statistically oriented examples, that writing code for serious statistical tasks is possible, it is time to consider some topics that are a essential for larger coding projects. 

\subsection{Design}

As mentioned several times already in these notes, you will write better code more quickly if you spend some time and effort thinking about, planning and writing down the {\em design} of your code, before you type any actual code at the keyboard. A major aim of code design is to achieve well {\em structured} code, in which the overall computational task has been broken down into logical components, coded in concise manageable functions, each with well defined {\em testable} properties. Well structured programmes are are easier to code, test, debug, read and maintain. \index{code!design}\index{code!structure}

For complex projects, design may be multi-levelled, with the task broken down into `high level' logical units, with each unit requiring further design work itself. But however complex the task, the same basic approach is useful at each level.
\begin{enumerate}
\item {\em Purpose}. Write down what your code should do. What are the inputs and outputs, and how do you get from one to the other?
\item {\em Design Considerations}. What are the other important issues to bear in mind? e.g. does your code need to be quick? or deal with large data sets? is it for one off use, by you, or for general use by others? etc.
\item  {\em Design and structure}. Decide how best to split the code into functions each completing a well defined manageable and testable part of the overall task. Write down the structure.
\item {\em Review}. Will this structure achieve the purpose and meet the design considerations?  
\end{enumerate} 
Often you will need to iterate and refine the process a few times to achieve a design you are happy will do the job, and be as straightforward as possible to code. And some parts of any design are likely to need more careful specification and detailing than others. 

At the design stage you are acting like the architect and structural engineer for a building. Once you start to code you are acting as the builder. This is a useful analogy. Architects' plans never cover how every brick is to be laid. The builder still has to solve many on the ground problems during construction, but the better the plans, the more smoothly the construction phase tends to go. Just as making up any but the simplest building work as you go along is a recipe for making a mess, so is trying to design as you code - at least sketch out a plan first. But also expect to have to modify your plan as you code, as some realities become apparent that were not anticipated. 

Good planning and structure also help to write readable code, but readability also requires good commenting. If you write reasonable good code then as you write it, everything about your code will usually seem very obvious to you. But two weeks later it will not seem obvious. Neither will it seem obvious to anyone else (as the team-working experience from this course so far will hopefully have already made clear). You should therefore aim to comment your code so that you could give it to another R programmer and they could quickly understand what is supposed to be doing and how it does it, {\em without knowing anything about these things previously}. You can help others and your later self by\index{code!comments}
\begin{enumerate}
\item Using (short) meaningful variable names where possible.
\item Explaining what the code does using frequent \verb+# comments+, including a short overview at the start of each function, and in longer projects an overview of the code's overall purpose. 
\item Laying out the code so that it is clear to read. Remember that white space and blank lines cost nothing.
\item Whenever you work in a team, always ensure that any piece of code and commenting written by one team member is carefully reviewed and checked by another.    
\end{enumerate}  

\subsection{Ridge regression ---  a very short design example}

The parameters estimates, $\hat \bp$, for the linear model ${\bf y} = {\bf X}\bp + {\bm \epsilon}$ can become rather unstable if $p$, the number of parameters, becomes large relative to $n$, the number of data. Indeed if $p>n$ then least squares parameter estimates $\hat \bp $, will not exist. This in turn means that model predictions $\hat \mu = {\bf X} \hat \bp$ may poorly approximate the true ${\bm \mu} = E({\bf y})$. One approach that can help is {\em ridge regression}, which reduced variability in $\hat \bp$ by {\em shrinking} parameters towards zero. Specifically we estimate parameters as 
$$
\hat \bp = \underset{\bp}{\text{argmin}} ~ \|{\bf y} - \X\bp\|^2 + \lambda \|\bp\|^2 = (\X\ts \X + \lambda {\bf I})^{-1} \X\ts {\bf y}
$$
for some {\em smoothing parameter} $\lambda$. The larger we make $\lambda$ the more the elements of  $\hat \bp$ are shrunk towards zero, while $\lambda=0$ returns the least squares estimates\footnote{There is a close link between ridge regression and a Bayesian approach to linear modelling, in which a prior $\beta\sim N({\bf 0}, {\bf I}\lambda^{-1}\sigma^2)$ is placed on the linear model parameters. With such a prior, $\hat \bp$, is the posterior mode for $\bp$.}. $\lambda$ can be selected to optimize an estimate of the error that the model would make in predicting a replicate dataset to which it had not been fitted. One such estimate is the {\em generalized cross validation} score,\index{ridge regression}\index{cross validation!generalized}
$$
V(\lambda) = \frac{n\|{\bf y} - {\bf X}\hat \bp\|^2}{\{n-\text{trace}({\bf A})\}^2},
$$
where ${\bf A} = \X(\X\ts \X + \lambda {\bf I})^{-1} \X\ts$, so that $\hat {\bm \mu} = {\bf Ay}$. The term $\text{trace}({\bf A})$ is the {\em effective} degrees of freedom of the model. It varies smoothly between $p$ and 0 as $\lambda$ is varied between 0 and $\infty$. 

Let's briefly consider the design a simple ridge regression function.

\begin{enumerate}
\item {\em Aim}. Produce a function that takes model matrix, $\bf X$, response vector $\bf y$, a sequence of $\lambda $ values to try, and returns the vector of ridge regression parameter estimates $\hat \bp$ corresponding to the GCV optimal choice of $\lambda$.
\item {\em Considerations}. Reasonable to assume that inputs are correct here. Important to check that the GCV score has a proper minimum. Since ridge regression often used for large $p$, efficiency is of some importance.
\item {\em Outline high level design}: \begin{enumerate}
\item Top level function {\tt ridge(y,X,lsp)} loops through log smoothing parameters in {\tt lsp}, calling function {\tt fit.ridge} for each to evaluate GCV score. Hence find optimal log smoothing parameter. Plots GCV against {\tt lsp} using function {\tt plot.gcv}. Returns optimal $\hat \bp$
\item {\tt fit.ridge(y,X,sp)} fits ridge regression as above using Cholesky\footnote{Actually QR based methods would be more stable, but involve extra maths that is unhelpful for this illustration.} based method to solve for $\hat \bp$. {\tt sp} is the $\lambda $ value to use. Return the GCV score, effective degrees of freedom and $\hat \bp$.
\item {\tt plot.gcv(edf,lsp,edf)} plots the GCV score against effective degrees of freedom, {\tt edf} and log smoothing parameters, {\tt lsp}, marking the minimum.
\end{enumerate}
\end{enumerate}   
So the overall task has been broken down into simpler tasks for which testable functions can be written. 

\bigskip

\noindent {\bf Exercise:} Complete any necessary design details for each function, and implement {\tt ridge}. 

\subsection{Testing}

It is important to test code. You should of course test code as you write it, checking that it does what you expect in an interactive manner, but you should also devote some time to developing tests that can be run repeatedly in a more automated manner. One important approach is known as {\em unit testing}, where you write code implementing tests that check whether the components of a software project do what they are designed to do. In R programming this would usually mean writing code to check that the functions that make up your project each perform as expected, when supplied with arguments covering the range of conditions that the function is designed to cope with (and often also testing that appropriate error message are given when inappropriate inputs are provided). \index{unit tests}\index{code!testing}

In complex projects it is also a good idea to design appropriate tests that the functions are working together in the appropriate way, although this is often somewhat automatic, since projects are often hierarchical, with some functions calling  several other functions, so that tests of the calling function must automatically test the co-ordinated operation of the functions being called.  

As a very simple example, consider unit tests for the simple matrix argument to matrix value function from section \ref{sec:function}, modified slightly to also include some error checking.\index{error checking}\index{stop}
\begin{lstlisting}
mat.fun <- function(A,fun=I) {
  if (!is.matrix(A)) A <- matrix(A) ## coerce to matrix if not already a matrix
  if (ncol(A)!=nrow(A)) stop("matrix not square") ## halt execution in this case
  if(max(abs(A-t(A)))>norm(A)*1e-14) stop("matrix not symmetric")
  ea <- eigen(A,symmetric=TRUE) ## get eigen decomposition
  ## generalize 'fun' to matrix case...
  ea$vectors %*% (fun(ea$values)*t(ea$vectors)) ## note use of re-cycling rule!
}
\end{lstlisting}
We might write tests for the case where \verb+fun+ is the identity, square root and inverse, where in each case the returned value should be easily checked. Another test might check that non-square matrices are correctly handled. 
\begin{lstlisting}
## test fun=I and inverse
n <- 10; A <- matrix(runif(n*n)-.5,n,n)
A <- A + t(A) ## make symmetric

B <- mat.fun(A) ## identity function case
if (max(abs(A-B))>norm(A)*1e-10) warning(" mat.fun I failure")

B <- mat.fun(A,function(x) 1/x) ## inverse function case
if (max(abs(solve(A)-B))>norm(A)*1e-10) warning("mat.fun inv failure")

A <- crossprod(A) ## ensure +ve def for sqrt case
B <- mat.fun(A,sqrt) ## sqrt function case
if (max(abs(A-B%*%B))>norm(A)*1e-10) warning("mat.fun sqrt failure")

## use try to check that an error is produced, as intended for non-square argument
if (!inherits(try(mat.fun(matrix(1:12,4,3)),silent=TRUE),"try-error")) 
              warning("error handling failure")
\end{lstlisting}
Obviously you might choose to bundle these unit tests into a function that can then be called to run the tests, possibly returning a code indicating which error or none occurred, and/or printing warnings as appropriate.  

Writing careful tests that are as comprehensive as possible is good practice, especially for code that will be used by others, and may be subject to future modification. But beware of the complacency that having good tests can bring with it. When modifying code, the mere fact that it still passes all its tests is no proof that the modification is correct. Also don't get in the habit of coding to the unit tests - debugging modifications until the unit tests are all passed and then assuming that all is OK. 

\bigskip

\noindent {\bf Exercise:} add a test for whether non symmetric matrices are being handled as designed.

\subsection{Debugging}

Debugging\footnote{The term comes from the early days of computing, when errors were sometimes caused by moths or other `bugs' getting into the hardware of the computer. It stuck.} is the process of finding and fixing errors in computer code. There are two aspects to debugging. Adopting the right approach, and using an appropriate debugger appropriately. Start with approach. \index{debug}\index{code!debugging}
\begin{itemize}
\item  Ideally we would not introduce bugs in the first place. But humans are error prone, and would probably not be creative if that was not the case. So recognise that you are error prone, by adopting working methods that minimize the chance of errors: design before you code. Make sure the task is broken down in a well structured manner, into functions that can be individually tested. Comment carefully, and test as you go along. Note that the act of describing what code is supposed to do in a useful comment is often a good way of noticing that it doesn't do that. 
\item Be aware that modification of code, for example to add functionality, is often the point at which bugs are introduced. Carefully commenting code to make its original function clear is the first line of defence against this. Being very careful to test modifications is also key. 
\item But any interesting code will have bugs at some point in its development. Finding bugs usually requires that you adopt a scientific approach - gathering data on what might be wrong, forming hypotheses about the what is causing the problems and then designing experimental tests to check this.
\item Key to this approach is checking what your code is {\em actually} doing, and whether it conforms to what you think it {\em ought} to be doing. 
\item While going carefully through your code once looking for errors is sensible, staring at it for hours is rarely productive. You need to actively hunt down the errors. Sometimes this can be done by simply modifying the code to print out information as it runs, but more often the efficient way to proceed is to use a debugger to step through your code checking that it is producing what you expect at each step.       
\end{itemize}

R provides {\tt debug}, {\tt debugcall}, {\tt trace} and {\tt traceback} functions for debugging, and Rstudio also has a basic debugger. However Mark Bravington's {\tt debug} package is the most useful option currently available. It is not available on CRAN, so see section \ref{sec:software} for installation instructions. Use of {\tt debug} is as follows: 

\begin{enumerate}
\item Load the package into R with \lstinline+require(debug)+ or \lstinline+library(debug)+.
\item Use \lstinline+mtrace+ to set which functions you want to debug. e.g. \lstinline+mtrace(lm)+ would set up debugging of R's own {\tt lm} function. \lstinline+mtrace(lm,FALSE)+ turns off debugging of a specific routine (here \lstinline+lm+), while \lstinline+mtrace.off()+ turns off all debugging.
\item To start debugging just call the function in the usual way. For example \lstinline+lm(rnorm(10)~runif(10))+, will launch \lstinline+lm+ in debugging mode once we have used \lstinline+mtrace+ to set this up. See screenshot below.
\item Once in debugging mode the R prompt changes to \verb+D(1)>+ and a new window appears showing the source code of the function being debugged. \index{debug!mtrace}\index{debug!bp}\index{debug!go}\index{debug!quit}\index{debug!break points}
\begin{itemize}
\item At the prompt you can issue R commands in the usual way. So setting or checking the values of variables is easy, for example.
\item Pressing the return key without an R command causes the next line of code of the function being debugged to be executed.
\item The command \lstinline+go()+ causes the function to run to completion, the first error, or to the next {\em breakpoint}.
\item Breakpoints are markers in the code indicating where you want execution to stop (to allow examination of intermediate variables, typically). They are set and cleared using the \lstinline+bp()+ function. For example:
\begin{enumerate}
\item \lstinline+bp(101)+ sets a breakpoint at line 101 of the current function, while \lstinline+bp(101,FALSE)+ turns it off again. The location of breakpoints is shown as a red star next to the appropriate code line in the source code window.
\item \lstinline+bp(12,i>6,foo)+ sets a breakpoint at line 12 of function {\tt foo}, indicating that execution should only stop at this line if the condition \verb+i>6+ is met.  
\end{enumerate}
See {\tt ?bp} for more details. Note that stepping over lengthy loops can be very time-consuming in the debugger (even one line loops), so setting a breakpoint after the loop and \lstinline+go()+ing to it is a common use of breakpoints, even in simple code. 
\item To step into a function called from the function you are debugging, either {\tt mtrace} it, or set a breakpoint inside it.
\item \lstinline+qqq()+ quits the debugger from within a function being debugged, without completing execution of the remaining function code.   
\item See \lstinline+https://www.maths.ed.ac.uk/~swood34/RCdebug/RCdebug.html+ for slightly more information, including how to deal with some minor niggles in {\tt debug}.
\end{itemize}  
\end{enumerate}

\eps{0}{0.3}{mtrace.png}


\subsection{Profiling}

Once you have working tested debugged code you may, in some circumstances, want to work on its computational speed\footnote{you might also be concerned about its memory footprint, but we won't cover that here.}. This can be especially important if coding statistical analyses of large data sets that may need to be repeated many times: for example the processing of medical images, satellite data, financial data, network traffic data, high throughput genetic data and so on.  

The {\tt Rprof} function in R profiles code by checking what function is being executed every \lstinline+interval+ seconds, where \lstinline+interval+ is an argument with default value 0.02, storing the results to a file (default \lstinline+"Rprof.out"+). Function \lstinline+summaryRprof+ can then be used to tabulate how long R spent in each function. Two times are reported, total time in the function, and time spent executing code solely within the function, as opposed to other functions called by the function. The basic operation is as follows:\index{Rprof} \index{summaryRprof}\index{profiling}
\begin{lstlisting}
Rprof()
## the code you want to profile goes here
## only the calls to run the code are needed 
## - not function definitions.
Rprof(NULL)
summaryRprof()
\end{lstlisting}
Here is a simple example. 
{\small \begin{verbatim}
> pointless <- function(A,B,y) { 
+   C <- A%*%B%*%y  ## compute ABy
+   D <- solve(A,B)%*%y  ## compute A^{-1}By
+   E <- A + B 
+   F <- A + t(B)
+ }
> n <- 2000; A <- matrix(runif(n*n),n,n)
> B <- matrix(runif(n*n),n,n); y <- runif(n)
> ## Profile a call to pointless...
> Rprof()
> pointless(A,B,y)
> Rprof(NULL)
> summaryRprof()
$by.self
                self.time self.pct total.time total.pct
"solve.default"      0.66    60.00       0.66     60.00
"%*%"                0.38    34.55       0.38     34.55
"t.default"          0.04     3.64       0.04      3.64
"+"                  0.02     1.82       0.02      1.82

$by.total
                total.time total.pct self.time self.pct
"pointless"           1.10    100.00      0.00     0.00
"solve.default"       0.66     60.00      0.66    60.00
"solve"               0.66     60.00      0.00     0.00
"%*%"                 0.38     34.55      0.38    34.55
"t.default"           0.04      3.64      0.04     3.64
"t"                   0.04      3.64      0.00     0.00
"+"                   0.02      1.82      0.02     1.82

$sample.interval
[1] 0.02

$sampling.time
[1] 1.1
\end{verbatim}}
Clearly {\tt pointless} is spending most of its time in {\tt solve} and \verb+%*%+, which might encourage you to look again at these. Then recalling the discussions of matrix computation efficiency from section \ref{sec:matrix}, you would modify the function to make it more efficient, and profile again.
{\small \begin{verbatim}
> pointless2 <- function(A,B,y) { 
+   C <- A%*%(B%*%y)  ## compute ABy
+   D <- solve(A,B%*%y)  ## compute A^{-1}By
+   E <- A + B
+   F <- A + t(B)
+ }
> Rprof()
> pointless2(A,B,y)
> Rprof(NULL)

> summaryRprof()
$by.self
                self.time self.pct total.time total.pct
"solve.default"      0.22    73.33       0.22     73.33
"t.default"          0.04    13.33       0.04     13.33
"%*%"                0.02     6.67       0.02      6.67
"+"                  0.02     6.67       0.02      6.67

$by.total
                total.time total.pct self.time self.pct
"pointless2"          0.30    100.00      0.00     0.00
"solve.default"       0.22     73.33      0.22    73.33
"solve"               0.22     73.33      0.00     0.00
"t.default"           0.04     13.33      0.04    13.33
"t"                   0.04     13.33      0.00     0.00
"%*%"                 0.02      6.67      0.02     6.67
"+"                   0.02      6.67      0.02     6.67

$sample.interval
[1] 0.02

$sampling.time
[1] 0.3
\end{verbatim}}
This is much better. Far less time is spent on matrix multiplication, as the unnecessary matrix-matrix multiplication is avoided. The time spent in {\tt solve} is also substantially reduced by only solving with a vector right hand side, rather than a matrix. \index{matrix!efficiency}


\section{Graphics}

Visualization of data and modelling results is an essential part of statistical analysis and data science more widely. As we have already seen, base R itself provides graphics functions, but there are also add on packages for various tasks. For example the {\tt lattice} package shipped with R is very useful for visualizing grouped data. Here let's cover base R graphics and the popular {\tt ggplot2} add on package. For a good many tasks {\tt ggplot2} lets you build quite complicated visually appealing plots fairly easily. The `gg' stands for `grammar of graphics' and in keeping with this the package uses its own language to describe plot elements, which is not always obvious. 

In broad overview, the difference between base R graphics and {\tt ggplot} graphics is as follows.
\begin{itemize}
\item In base R a high level plotting command is called with user specified data to produce an initial plot on a graphics device (a plot window or graphics file driver). That plot can then be added to using various auxiliary graphics functions for adding things like lines, arrows, points, polygons and text to the last high level plot produced. The plotting data is passed to these auxiliary functions as needed.  When we are done, we close the device or call the next high level plotting function. 
\item In {\tt ggplot2} plots are built up as R objects, and only actually plotted once the object is {\tt print}ed to a graphics device. A plot object consists of a data frame, a co-ordinate system set up with reference to variables in the data frame, and `geometric' elements such as points, lines, polygons, etc. The plot object is added to and changed by modifier functions. Each modifier function expects some standard variables (such as {\tt x} and {\tt y}), which it usually picks up from the plot's data frame. Mapping the data frame names to these standard variables, or providing the standard variable values directly, is achieved via the {\tt aes} function, as we will see. \index{ggplot}\index{ggplot!aes}    
\end{itemize}
So in {\tt ggplot2} you build up an R object representing a plot, which is then `printed' to a graphics device, and in base R you create plots by calling a sequence of functions whose side effect is to draw things on a graphics device. Base R high level plot functions tend to have huge numbers of arguments to control the look of the plot. {\tt ggplot2} instead has a set of standard plot modification functions, which can make the code for producing a plot less messy, and often quicker and easier to use. The downside of {\tt ggplot2} is that it has its own jargon, which often serves to obscure as much as it illuminates. A useful summary of {\tt ggplot2} functions is provided by\\ 
\verb+https://github.com/rstudio/cheatsheets/raw/master/data-visualization.pdf+\\
but it's incomprehensible without understanding some basics first!

\subsection{Scatterplots, ggplot and base R} 

Let's start by producing a scatterplot in base R and with {\tt ggplot2}. As an example consider the {\tt mpg} data frame supplied with {\tt ggplot2}, on fuel consumption of cars. The following code loads it, takes a look and simplifies the {\tt trans} variable so that it simply differentiates between automatic and manual {\tt transmission}. Then it plots miles per gallon in highway driving against engine displacement (capacity), plotting the data for manual cars in black and automatic in red. 
\begin{lstlisting}
library(ggplot2)
head(mpg)
mpg$transmission <- rep("manual",nrow(mpg)) ## simplified transmission variable
mpg$transmission[grep("auto",mpg$trans)] <- "automatic" ## note automatics
par(mfrow=c(1,2),mar=c(4,4,1,1)) 
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",
     pch=19,col=(mpg$transmission=="automatic")+1)
\end{lstlisting}\index{plot}\index{plot!pch}\index{plot!col}\index{plot!xlab}\index{par!mfrow}\index{par!mar}
The result is provided in the left hand plot below. Note that \lstinline+par(mfrow=c(1,2),mar=c(4,4,1,1))+ sets up the plot device to have 1 row of plots with 2 columns, and for the margins to be 4 characters deep on the left and bottom, and 1 deep on top and right. The {\tt plot} function call is fairly self explanatory. \lstinline+mpg$displ+ and \lstinline+mpg$hwy+ are provided as the $x$ and $y$ variables of the plot, and nice axis labels are also given. \verb+pch=19+ chooses the plotting symbol, while  \lstinline^col=(mpg$transmission=="automatic")+1^ sets the symbol colour to 2 (red) for an automatic, and 1 (black) otherwise.

The plot is fairly basic, and you might like a more modern look, with a nice grey background and white grid lines. Also the symbols are overlapping in an unhelpful way, so they could usefully be smaller. These changes are easy to produce, with a few lines of code.
\begin{lstlisting}
plot(mpg$displ,mpg$hwy,xlab="displacement",ylab="highway mpg",type="n") # empty plot
rect(0,0,8,50,col="lightgrey") # add a grey rectangle over plot area 
abline(v=2:7,h=seq(10,45,by=5),col="white") # add grid lines
points(mpg$displ,mpg$hwy,col=(mpg$transmission=="automatic")+1,pch=19,cex=.5) # data
\end{lstlisting}\index{rect}\index{abline}\index{points}\index{cex}
\lstinline+cex=.5+ sets the symbol size to half its default (we could have used it as an argument to {\tt plot} in the original plot).
\eps{-90}{.7}{mpg-base.eps}

{\tt ggplot2} produces plots like the right hand one with a bit less effort, and has lots of other built in features for quickly building plots that would otherwise take rather more coding in base R. For example to produce the sort of plot on the right hand side above might involve this:\index{ggplot}\index{ggplot!aes}\index{ggplot!\verb+geom_point+}
\begin{lstlisting}
a <- ggplot(mpg,aes(x=displ,y=hwy)) + geom_point(aes(colour=transmission)) +
      geom_smooth(method="gam",formula=y~s(x)) ## add a smooth as it's easy!
a    ## 'print' plot object 'a' - that is plot it!
\end{lstlisting}
\eps{0}{.7}{mpg-gg.pdf}
The first line of code is creating the R object representing the plot. 
\begin{itemize}
\item \lstinline+ggplot(mpg,aes(x=displ,y=hwy))+ sets up the plot object, specifying the data frame on which it is based and then which variables in the data frame will be the default $x,y$ variables in the plot. The co-ordinate system and axis are set up with reference to those. This mapping of data frame names to axis variables is achieved by the {\tt aes} function. Any modifications we make to the base plot will use these variable as the $x, y$ variables, unless we explicitly tell the modifier function to use a different mapping.
\item \lstinline^+ geom_point(aes(colour=transmission))^ adds points to the empty plot created by {\tt ggplot}. Modifications are always added to an existing plot using the \verb^+^ operator (there is no \verb^-^ operator). \lstinline+geom_point+ requires {\tt x} and {\tt y} variables to plot. Since we did not specify which variables to use for these, it will use the default $x,y$ variables already specified for the plot in the {\tt ggplot} call. Optionally we can also specify a {\tt colour} variable for the plotted points. Again {\tt colour} is a variable that may be mapped to a variable in the plot's data frame, so it is specified using the {\tt aes} function. In this case {\tt colour} is mapped to the {\tt transmission} variable. 
\item \lstinline^ + geom_smooth(method=gam,formula=y~s(x))^ adds a smooth curve fitted to the data (blue above). It uses the \lstinline+gam+ function to do this - a modelling function that expects a \lstinline+formula+, also supplied.
\item \lstinline+a+ by default invokes the {\tt print} function for a {\tt ggplot} plot object, which causes the plot to be displayed. If you were wanting to display the plot from within a function you have written, you would need to invoke {\tt print} explicitly. That is \lstinline+print(a)+.    
\end{itemize} 
There are two unusual things about {\tt ggplot2} relative to the standard function based R programming that we met so far. One is the \verb^+^ operator for adding plot modifications. The other is that some of the variables required by the modifier functions are not provided as function arguments, but rather as mappings to the plot data frame variables, or as explicit values, using the {\tt aes} function. To be comfortable with {\tt ggplot} you need to understand these.
\begin{enumerate}
\item Like all operators in R, the \verb^+^ operator for adding modifications to a plot is actually implemented as a function with 2 arguments. R has a mechanism for different versions of functions to be used for different classes of objects, so it knows to call the {\tt ggplot} specific version of the {\tt +} operator function when `adding' modifications to a plot. So instead of modification being done using code like \lstinline^a <- a + geom_point()^ ({\tt a} being a plot object), the {\tt ggplot2} authors could have chosen to implement modification like this \lstinline+a <- ggmod(a,geom_point())+ which is actually what the underlying code does. 
\item Supplying variables to a modifier function as mappings to the plotting data, rather than as function arguments emphasises the plot to data link. The {\tt aes} function used to create such mappings takes its name from some peculiar {\tt ggplot} terminology: the non-argument variables used by a function, such as {\tt x} and {\tt y}, are termed {\em aesthetics}. Given the dictionary definition of {\em aesthetic}, that's an odd term to use for the actual data being plotted. I tend to think of {\tt aes} as {\em any extra stuff} instead, which gets the notion of adding definitions not inherited from the existing plot. However, when looking up the help file for a modifier function, these non argument variables are listed under the heading {\tt Aesthetics} (and not under {\tt Arguments}, obviously). In case that's all too clear - some of these {\tt Aesthetics} can also be passed to the modifier functions as regular arguments matched to \lstinline+...+: for example if you want to change the plot {\tt colour} to \lstinline+"red"+.  
\end{enumerate}\index{ggplot!\verb+coord_cartesian+}\index{ggplot!axis range}\index{ggplot!aesthetics}
As a further example suppose we do not like the axis ranges of the last plot and want to modify them. This is easily achieved with the modifier \lstinline+coord_cartesian()+. Looking this up, we find that it is does not need to be supplied with any non-argument `Aesthetic' variables, just regular arguments, so the following would do the job.
\begin{lstlisting}
a <- a + coord_cartesian(xlim=c(0,8),ylim=c(0,50))
\end{lstlisting} 
But what if we wanted more general changes to the axes scales? Browsing the {\tt ggplot2} documentation we might use \lstinline+scale_x_continuous+, e.g.
\begin{lstlisting}
a <- a + scale_x_continuous(limits=c(1,7),name="displacement (litres)")
\end{lstlisting} 
Note that {\tt limits} here does not reset the axis limits unless the existing axis limits are insufficient to cover the range given in {\tt limits}, and those axis limits were set automatically, rather than by \lstinline+coord_cartesian+. In fact {\tt limits} is an altogether dangerous option: it drops the plotting data outside of the interval defined by the limits, which can have undesirable knock on effects. Sometimes you need to read {\tt ggplot2} documentation quite carefully, and then experiment to test your understanding! 

\subsection{Plotting to a file and graphical options}

What if you want to save your plot to a graphics file for incorporation in a report? For {\tt ggplot} or base R, you can simply open a file type graphics device, run the code to display your plot and then close the devise. For a complete list of the available graphics devices see {\tt ?Devices}. If you know how to use one graphics file device it is pretty easy to use any of them. So here is an example of producing an {\tt .eps} file using the {\tt postscript} device. 
\begin{lstlisting}
postscript("mpg-base.eps",width=8,height=3)
## the code to produce the plot goes here - e.g. from previous section
dev.off()
\end{lstlisting}
This was actually the command I used to produce the first plot in the previous section. The first argument to {\tt postscript} is the filename. You either need to change to the directory where you want the file to go using \lstinline+setwd+ before calling \lstinline+postscript+, or give a full path in the filename. {\tt width} and {\tt height} give the width and height of the plot in inches. See {\tt ?postscript} for how to use other units. It is essential to close the postscript file using \lstinline+dev.off()+ when done --- you won't get a properly formed file otherwise. Other commonly use devises are {\tt pdf}, {\tt png}, {\tt jpeg} and {\tt tiff}. \index{graphics file} \index{graphic device}

A very common adjustment needed when printing to file is to change the size of the axis lables and axis titles. In base R the arguments \lstinline+cex.axis+ and \lstinline+cex.lab+ can provided to most high level plot functions to change these. Values $>1$ enlarge, while $<1$ shrinks. You may also want to change plot symbols sizes, with \lstinline+cex+, or change the symbol with \lstinline+pch+ (a number or a character, including \lstinline+"."+ for the smallest possible point). Line widths are controlled with \lstinline+lwd+. See \lstinline+?par+ for all the graphical parameters that can be set. Several are high level parameters set at plot device level, but the rest can be supplied to most high level plotting functions. There are so many options that they are not listed for each high level plotting function, but are passed via \lstinline+...+, which makes sense as they often control lower level plotting functions. \index{plot options}\index{par}\index{plot!sizing}  

{\tt ggsave} is an even easier option for plotting {\tt ggplot2} plots to file. For example
\begin{lstlisting}
ggsave("mpg-gg.eps",width=5,height=3)
\end{lstlisting}
produced the second plot in the previous section. {\tt ggsave} infers the plot file type from its extension ({\tt .eps} here).\index{ggplot!graphics file} By default it saves the last plot displayed, but you can also explicitly provide a {\tt ggplot} object as the {\tt plot} argument of the function.

\subsection{Some univariate data plots}

Suppose that you are interested in visualizing the distribution of a single variable. For a continuous variable perhaps the simplest plot is a histogram. This divides the range of your data into a series of contiguous intervals, and counts the number data falling within each of them. The intervals (also know as {\em bins}) are then arranged along a horizontal axis, and bars are plotted above each interval, with {\em area} proportional to the number of observations in the interval. If the intervals all have the same width, then the {\em height} of the bar is also proportional to the number of observations in the interval.   

As an example the {\tt faithful} data frame in R contains 272 observations of the duration of the Old Faithful geyser's {\tt eruptions} in minutes (it also contains {\tt waiting} time between eruptions). Suppose we are interested in the distribution of eruption times. Here is the code to produce a couple of alternative histograms.
\begin{lstlisting}
par(mfrow=c(1,2)) ## set up to allow 2 histograms side by side
hist(faithful$eruptions) ## default
hist(faithful$eruptions,breaks=quantile(faithful$eruptions,seq(0,1,length=15)),
     xlab="eruption time (mins)",main="Old Faithful geyser",col="purple")
\end{lstlisting}
\eps{-90}{.6}{geyser-base.eps}
The left plot is from the first default call to \lstinline+hist+. The number and location of the x axis breaks defining the intervals has been chosen automatically, and the intervals are all of equal width. The y axis shows the count in each bar. Note the obvious interpretation of the bar heights as being proportional to counts per unit x axis interval. 

We do not have to stick with the automatically computed intervals. The second call to \lstinline+hist+ produces the right hand plot. In addition to its lurid colour and nicer labels, the main noticeable feature is that I have supplied the breaks between intervals, and have not spaced those breaks evenly. In fact I have used the {\tt quantile} function to set the breaks at 15 empirical quantiles (equally spaced in probability) of the distribution of eruption durations\footnote{Recall that the $q^{th}$ quantile, $\tau_q$, of random variable $X$ is defined so that $P(X<\tau_q) = q$.}. So here we have bars of uneven width, the height of which is still proportional to the number of observations per unit interval, but whose area is proportional to the counts in each interval. One caveat about `area' here: by area I mean width as measured on the x axis, multiplied by height as measured on the y axis. Since the axes are plotted with different scalings on the page, the bars will not have the same `on the page' area. Note also that the y axis label is changed to `Density', since while the bar heights are proportional to density, they are not any longer proportional to the frequency (count) of observations in the intervals. 

Actually uneven intervals are rarely used when plotting histograms, and it is more usual to use the {\tt breaks} argument  to supply a larger number of evenly spaced breaks, to provide more detail. One problem with histograms is that simply shifting the interval breaks a constant small amount can change the shape of the histogram, as points get moved between intervals. This is one reason for sometimes favouring a direct estimate of the pdf of the data, based on the sample. 

{\em Kernel density estimation} is a way to do this, which makes not too many assumption. The basic idea is to recognise that any data point we observed could equally well have been a bit bigger or a bit smaller than it turned out to be. So we could simulate a much larger sample than the one we have by simulating noisy versions of the data we observed. That much larger sample could be plotted using a histogram with very narrow bars, to get an almost continuous pdf estimate. For example:
\begin{lstlisting}
n <- nrow(faithful);set.seed(8)
erupt <- faithful$eruption + rnorm(n*1000,sd=.2) ## simulate much bigger sample
hist(erupt,breaks=100,freq=FALSE) ## plot it
\end{lstlisting}   
\eps{-90}{.6}{geyser-kernel.eps}
The plot is the left hand one above. Notice how the recycling rule has been used to produce 1000 slightly perturbed versions of each original eruption duration, and the resulting huge sample then yields a smooth and detailed histogram. It's worth running this code with different \lstinline+sd+ values -- larger values give smoother distributions, and smaller values more wiggly ones. 

But do we really need to simulate at all here? If we simulated an infinite number of perturbed observations around each original datum, we would end up with a sample for each datum that looked just like a normal pdf, centred on the datum. So when we pool the simulated observations we get what looks like the average of all those normal pdfs centred around each original observation. In that case we might as well cut the simulation step, and just average the pdfs. i.e. if $x_i$ denote data points, where $i=1,\ldots,n$, and $\phi(x|\mu,\sigma)$ denotes a normal pdf with mean $\mu$ and standard deviation $\sigma$, then the density estimate is 
$$
\hat \pi (x) = \frac{1}{n} \sum_{i=1}^n \phi(x|x_i,h)
$$     
where $h$ is our chosen value for the standard deviation. Such an estimate is known as a {\em kernel density estimate}, with {\em bandwidth} $h$. We could use any other valid pdf in place of $\phi$, of course. The {\tt density} function in R will compute such kernel density estimates, choosing the bandwidth parameter automatically. Sometimes the automatic selection appears to over or under smooth the density. For the eruption times it seems to over smooth putting a bit too much probability between the peaks. We can reduce the bandwidth relative to its automatic value by setting the {\tt adjust} parameter to something less than one. Here is code for plotting the density estimate on the right, above.  
\begin{lstlisting}
plot(density(faithful$eruptions,adjust=0.6),type="l",xlab="eruption time (mins)",
     main="kernel density estimate")
\end{lstlisting}

\noindent{\bf Exercise}: overlay a kernel density estimate over a histogram of the eruptions data (hint: used {\tt lines}).

Let's see how the task in the exercise could be done in {\tt ggplot2}.
\begin{lstlisting}
ggplot(faithful,aes(x=eruptions,after_stat(density))) + ## data, base mappings etc
geom_histogram(bins=20,col="green",fill="purple") +     ## garish histogram
geom_density(adjust=.5,col="red")                       ## added kernel density estimate
\end{lstlisting}
\eps{0}{.6}{geyser-gg.eps}
\begin{enumerate}
\item The \lstinline+gglot+ call sets up the data and x-axis for the plot. The \lstinline+after_stat+ function is needed to indicate that what is plotted should depend on densities, rather that counts. The documentation for this is not the clearest.
\item The \lstinline+geom_histogram+ turns the empty plot into a histogram with 20 bins (intervals). The colour of the histogram border and fill is set directly. Note the way that these are supplied as (undocumented) arguments, although described as {\em  aesthetics} in the documentation. This way of supplying options is quite common for modifier functions: it is implemented using the \verb+...+ agument.
\item The \lstinline+geom_density+ modifier adds a kernel density estimate to the plot. Without the 
\lstinline+adjust=.5+ argument this estimate is obviously too smooth. Unfortunately {\tt adjust} is not actually documented as an argument of \lstinline+geom_density+, but it works and is documented as an argument of another kernel density related function.    
\end{enumerate}
You will by now have noticed that the {\tt ggplot2} help files are not always as useful as the base R help files, and to really get to grips with {\tt ggplot2} requires finding online training materials and often searching online forums such as the programmers site {\tt stackexchange} for queries related to what you want to do. However many people who have spent the time getting to grips with it find it extremely useful and say that it saves them alot of time. 

Why did the automatic bandwidth choice lead to a kernel density estimate that was too smooth? Bandwidth selection methods assume that the original data are independent, and can go wrong when this is not the case. The eruption data are really a time series, and it could be that one eruption's duration influences the duration of subsequent eruptions. A standard check for this is to plot the {\em auto-correlation function} (ACF) of the data series. The ACF plots the correlation between data and the previous data point, and the correlation between data and the data 2 data points before it, and so on. The {\em partial} ACF does the same, except that the correlations are measured conditional on the correlations found at shorter lags - that is they are the differences between the correlation found and the correlation expected given the correlations found at shorter lags\footnote{e.g. if the autocorrelation at lag 1 is $0.7$ then the auto-correlation at lag 2 is expected to be $0.7^2 = 0.49$ purely on the basis of the lag 1 autocorrelation. It's really only the difference between the lag 2 correlation and 0.49 that is then interesting - which is what the PACF measures.}. Here is base R code:
\begin{lstlisting}
acf(faithful$eruptions); pacf(faithful$eruptions)
\end{lstlisting}
\eps{-90}{.6}{geyser-acf.eps}    
\ldots the correlations or partial correlations for each lag are plotted. Clearly there is strong negative correlations between the duration of one eruption and the next. Short eruptions follow long eruptions (and the other way around). The PACF suggests that the correlation at lag 2 is close to perfectly explained by the lag 1 correlation, but the negative correlation at lag 3 may be a bit less than would be expected from the lag 1 effect. In both plots the blue lines indicate how large a correlation has to be to be detectably different from zero at the 5\% level.  

\subsubsection{Discrete univariate data}

When the variables of interest are discrete categories, rather than continuous quantities, then a bar chart is often a useful visualization method. The idea is that you count the occurrences of each category and plot bars with height proportional to the count. This is a bit like a constant bar width histogram, except that the ordering of the bars may not matter at all in some cases. As an example of using bar charts, let's visualize the total number of deaths from all causes over the course of the Covid-19 epidemic to August 2021, by calendar age grouping, and compare this to the numbers of Covid deaths. The data are available from the UK Office for National Statistics (ONS). First read in the data\ldots

\begin{lstlisting}
d <- c(3362,428,290,369,978,1723,2471,3731,5607,7926,12934,20688,29905,40456,53579,
       84784,106448,136269,150325,178684) ## all deaths
cd <- c(3,1,3,10,26,65,133,257,438,740,1418,2587,4250,6337,8528,13388,18622,25220,
        27718,30247)  ## Covid-19 deaths
names(cd) <- names(d) <- c("0","1-4","5-9","10-14","15-19","20-24","25-29","30-34",
"35-39","40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79",
"80-84","85-89","90+") ## ONS age classes
\end{lstlisting}     
Then let's plot the deaths by age, side by side. Notice the use of \lstinline+ylim+ to force the same y axis scale. The \lstinline+las=2+ argument forces the category labels to be written vertically, so they fit. 
\begin{lstlisting}
par(mfrow=c(1,2))
barplot(d,las=2,main="All deaths Mar 20-Aug 21")
barplot(cd,ylim=c(0,max(d)),las=2,main="Covid deaths Mar 20-Aug 21")
\end{lstlisting} 
\eps{-90}{.5}{deaths1.eps}   
The side by side presentation is interesting, but because of the way death rate increases with age, it does not really allow comparison of the Covid and non-Covid risks for ages below 45. One way to do this might be to plot the Covid and non-Covid deaths stacked on the same bar chart, and then to produce a 'zoomed in' bar chart looking just at the under 45s. Stacked barcharts are easy. Instead of providing a single vector to \lstinline+barplot+ we just need to provide a matrix, where each row contains a set of counts to be plotted. Notice the arguments for including a legend indicating which bar colour is which.
\begin{lstlisting}
par(mfrow=c(1,2))
D <- rbind(cd,d-cd) ## create matrix of counts to plot
rownames(D) <- c("covid","not covid")
barplot(D,las=2,main="All deaths Mar 20-Aug 21",legend.text=TRUE,
args.legend=list(x=7,y=150000)) 
barplot(D[,1:10],las=2,main="Age 0-44 deaths  Mar 20-Aug 21")
\end{lstlisting}
\eps{-90}{.5}{deaths2.eps} 
This seems better, with the picture for under 45s now much clearer\footnote{If you are surprised by the data, that may be partly down to official messaging. The UK government advisory Scientific Pandemic Influenza Group on Behaviour (SPI-B) wrote in a 22 March 2020 report that: ``\ldots a substantial number of people still do not feel sufficiently personally threatened; it could be that they are reassured by the low death rate in their demographic group\ldots the perceived level of personal threat needs to be increased among those who are complacent, using hard hitting emotional messaging.'' and presumably not using data on the actual risk. }. We can produce a similar plot using {\tt ggplot}. The \lstinline+geom_bar+ function might seem the obvious way to do this, but it is actually designed for data frames in which a variable may take one of several categorical values, and the plotting function should do the counting up of numbers of occurrences of each category. To use it we would need to artificially expand the 20 ONS counts of deaths in each age class to some 0.8 million records of the age category for each death. Rather than do that, we'll need to use \lstinline+geom_col+ which can work with pre-tallied counts in categories.  
 
{\tt ggplot2} requires data in strictly tidy form, and so \lstinline+geom_col+ will not deal with multiple rows of data as \lstinline+barplot+ did. Rather it expects counts of {\tt deaths} to be a single variable in a single column, with another variable indicating the {\tt type} of death (covid or non-covid). Similarly the {\tt age} categories should be contained in another variable in its own column.   
\begin{lstlisting}
n <- length(d)
age <- factor(1:n);levels(age) <- names(d) ## age class as a factor
ggdat <- data.frame(age=c(age,age),deaths=c(cd,d-cd),
         type=c(rep("covid",n),rep("not covid",n))) ## tidy data frame
ggplot(ggdat,aes(x=age,y=deaths,fill=type)) + geom_col() +
       labs(title="All UK deaths Mar 2020 - Aug 2021")
\end{lstlisting}
\eps{0}{.5}{deaths3.eps} 
\begin{itemize}
\item The {\tt age} factor variable defines the age categories. The obvious code \lstinline+age <- factor(names(d))+, causes R to order the factor levels in way that puts the \verb+5-9+ level just before the \verb+50-54+ level, and {\tt ggplot} orders the bars by the order of the levels, not their order of appearance in the data frame. Since that is not what we want here, it is better to set the factor up first and then change the names assigned to its levels in a way that we control the ordering.
\item \lstinline+ggdat+ is simply set up in the tidy way that \lstinline+geom_col+ demands.
\item The plot setup is now straightforward. The mappings of data frame variables to the variables \lstinline+geom_col+ requires is done in \lstinline+ggplot+ here, but could also be done in \lstinline+geom_col+. By default  \lstinline+geom_col+ will stack up the bars corresponding to the same age class on top of each other. \lstinline+fill=type+ specifies that we want those stacked bars coloured according to type of death. The \lstinline+labs+ modifier just changes the title.    
\end{itemize}


\subsection{Boxplots and violin plots}

Statistical analysis is often concerned with how the distribution of some variable changes between different groups. That is with plotting a continuous variable against a categorical variable in a meaningful way. {\em Boxplots} are one useful way of summarizing a distribution. A box is plotted, showing the middle 50\% of the data, with a line across it marking the median. Lines (`whiskers') are drawn from either side of the box to the extremes of the data, after removal of any {\em outliers} more that 1.5 times the width of the box away from the box. The outliers are then plotted individually. To see this in action, consider the \lstinline+ToothGrowth+ data frame in R. This reports data from an experiment in which guinea pigs had their diet supplemented with 0.5, 1 or 2 mg of vitamin C per day, either administered as Orange Juice, or ascorbic acid. The response measured was odontoblast length at the end of the experiment. The following code uses boxplots to visualize the distribution of the lengths against dose, method of delivery ({\tt supp}), and their combination. 

\begin{lstlisting}
par(mfrow=c(1,3),mar=c(5,5,1,1))
boxplot(len~supp,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose,data=ToothGrowth,cex.lab=1.5)
boxplot(len~dose+supp,data=ToothGrowth,cex.lab=1.5)
\end{lstlisting}
\eps{-90}{.5}{boxplot1.eps} 
\ldots make sure you relate what is plotted back to the description of what a boxplot is. Clearly length increases with increased dose, and there is a suggestion that orange juice is the better delivery method. Looking at the plot on the right, there is a clear suggestion of an {\em interaction}: the effect of dose appears to change with delivery method (equivalently the effect of delivery method changes with dose). In other words it looks as if we can not simply add the effect of delivery method to the effect of dose. This notion of an interaction between predictor variables ({\tt supp} and {\tt dose}) is very important in statistical modelling --- so take some time to understand what it means here. 

{\tt ggplot2} also provides boxplots. If we want to look at interactions of variables, we need to explicitly construct a variable labelling the combinations of variables for which we require separate box plots. Such labels can be created using {\tt paste}. Note the use of the \lstinline+with+ function, which simply tells R to look for variable names in the given data frame. 
\begin{lstlisting}
ToothGrowth$comb <- with(ToothGrowth,paste(supp,dose))
ggplot(ToothGrowth,aes(y=len,group=comb)) + geom_boxplot()
\end{lstlisting}
\eps{0}{.5}{boxplot2.eps} 
Notice that with this small number of data, base R and {\tt ggplot} do not quite agree! {\tt ggplot} also allows slightly more detailed visualization of the data distribution in the form on {\em violin plots}. Rather than drawing simple boxes and whiskers, these draw a mirrored version of the kernel density estimate in each group, truncated at the maximum and minimum observed data values. Let's try this to visualize the (combined) effect of dose. As dose is numeric, it will need to be turned into a factor (group lable) to get what we want.
\begin{lstlisting}
ggplot(ToothGrowth,aes(y=len,x=as.factor(dose))) + geom_violin()
\end{lstlisting}
\eps{0}{.5}{violin.eps} 
so the width of the white shapes at some length value is proportional to the estimated probability density for length having that value.  
 
\subsection{3D plotting}

Sometimes it is useful to be able to plot functions of 2 variables. In statistical work these are often model output, representing, for example $E(z|x,y)$. For now consider the case of visualizing a simple function of two variables. Base R offers 3 approaches - perspective plots, image plots and contour plots. They all require the same input - vectors defining the $x$ and $y$ values against which to plot, and a matrix {\tt z} whose \lstinline+z[i,j]+ value gives the function value at \lstinline+x[i]+, \lstinline+y[i]+. Here is some example code.
\begin{lstlisting}
foo <- function(x,y) { ## function to be visualized
  r <- sqrt(x^2+y^2)
  exp(-r*2)*cos(r*10)
} ## foo
n <- 50; x <- seq(-1,1,length=n); y <- seq(-1,1,length=n)
xy = expand.grid(x=x,y=y) ## regular x,y grid over 
xy$z <- foo(xy$x,xy$y)    ## evaluate foo on the grid
z <- matrix(xy$z,n,n)     ## put evaluated function in a matrix
par(mfrow=c(1,3),mar=c(4,4,1,1)) ## divide plot devise into 3 columns
persp(x,y,z,theta=30,phi=30,col="lightblue",shade=.3) 
image(x,y,z)
contour(x,y,z,xlab="x",ylab="y")
\end{lstlisting}
\eps{-90}{.5}{3d.eps} 
The \lstinline+persp+ plot is self-explanatory, but it is very difficult to read the actual function values from it. Note that \lstinline+phi+ and \lstinline+theta+ control the orientation of the plot, while \lstinline+shade+ creates shadowing to aid visualization. The \lstinline+image+ plot shows the function as a colour map -- you can choose the colour scheme, but in this case red is high and white is low. The best option for actually displaying the surface numerically is a \lstinline+contour+ plot. This plots contours of equal value as continuous labelled curves. The plot can be read like a topographic map, and the value of the $z$ variable at any $x,y$ can be read approximately from the plot. A nice option is to overlay a contour plot on an image plot, using the \lstinline+add=TRUE+ argument to \lstinline+contour+. Then you get nice visualization and hard quantitative information on the same plot.


{\tt ggplot} also offers contouring and image plotting functions. Oddly the contour plot does not allow contours to be labelled, rendering the contours alone rather useless. However a combined plot is more useful.
\begin{lstlisting}
ggplot(xy,aes(x=x,y=y)) + geom_raster(aes(fill=z)) + geom_contour(aes(z=z))
\end{lstlisting}
\eps{0}{.5}{gg-contour.eps} 
We need to move on, but hopefully you have now a good enough grasp of base R and {\tt ggplot} graphics to be able to explore further independently.
%\makeindex
\printindex

\end{document}
