\documentclass[10pt] {article}
\usepackage{epsf}
\usepackage{latexsym}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{times}

\setlength{\textheight}{24cm} 
\setlength{\textwidth}{16cm}
\setlength{\oddsidemargin}{-5mm} 
\setlength{\topmargin}{-1cm}
\setlength{\evensidemargin}{-5mm}

%% Definitions
\newcommand {\hide}[1] {\typeout{ #1 }}
%Comment out to print all
%\newcommand {\hide}[1] {{\it #1 }}
%Comment out to hide some
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\dif}[2]{\frac{{\rm d} #1}{{\rm d} #2}}
\newcommand{\ildif}[2]{{\rm d} #1/{{\rm d} #2 }}
\newcommand{\ilpdif}[2]{\partial #1/{\partial #2 }}
\newcommand{\pdif}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\pddif}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\ilpddif}[3]{\partial^2 #1/{\partial #2 \partial #3}}
\newcommand{\comb}[2]{\left (\begin{array}{c}{#1}\\{#2}\end{array}\right )}
\newcommand{\gfrac}[2]{\mbox{$ { \textstyle{ \frac{#1}{#2} }\displaystyle}$}}
\newcommand{\R}{{\sf R }}
\newcommand{\X}{{\bf X}}
\newcommand{\Xt}{\X\ts}
\newcommand{\y}{{\bf y}}
\newcommand{\A}{{\bf A}}
\newcommand{\bp}{{\bm \beta}}

% comment out next line unless double spacing needed
%\renewcommand{\baselinestretch}{2}
\newcommand{\bmat}[1]{\left ( \begin{array}{#1}}
\newcommand{\emat}{\end{array}\right )}
% book specific

\newcommand{\ts}{^{\sf T}} %transposition
\newcommand{\its}{^{\sf -T}}
%% Set following to \iftrue or \iffalse to include or exclude solutions
\newcommand{\withsol}{\iffalse \noindent {\bf Solution}}

\begin {document}
\begin{center} {\bf \Large HW3: writing a Newton optimizer}\end{center}

\begin{center}
{\bf This homework is extremely difficult to complete without having watched lecture 13 (as well as 11 and 12), and attempted the week 6 exercises.} 
\end{center}
\bigskip

\noindent {\bf Programming Task:} to write an R function, {\tt newton}, implementing Newton's method for minimization of functions, and to provide example code using it to optimize Rosenbrock's function. Due 17:00 13/11/20. 

\bigskip

\noindent{\bf Specification:} Your {\tt newton} optimization function should operate broadly in the same way as {\tt nlm}. Note that the purpose is to have an independent implementation: you must code the optimization yourself, not simply call optimization code written by someone else. The function arguments should be as follows:
\begin{verbatim}
newton(theta,f,...,tol=1e-8,fscale=1,maxit=100,max.half=20)
\end{verbatim}
\begin{trivlist}
\item {\tt theta} is a vector of initial values for the optimization parameters.
\item {\tt f} is the objective function to minimize. Its first argument is the vector of optimization parameters. Remaining arguments will be passed from {\tt newton} using `\verb+...+'. The scalar value returned by {\tt f} will have 2 attributes, a {\tt gradient} vector and, optionally, a {\tt hessian} matrix.
\item \verb+...+ any arguments of {\tt f} after the first (the parameter vector) are passed using this (see hints below).
\item {\tt tol} the convergence tolerance.
\item {\tt fscale} a rough estimate of the magnitude of $f$ at the optimum - used in convergence testing.
\item {\tt maxit} the maximum number of Newton iterations to try before giving up.
\item {\tt max.half} the maximum number of times a step should be halved before concluding that the step has failed to improve the objective. 
\end{trivlist} 

\noindent {\tt newton} should return a list containing:
\begin{trivlist}
\item {\tt f} the value of the objective function at the minimum.
\item {\tt theta} the value of the parameters at the minimum.
\item {\tt iter} the number of iterations taken to reach the minimum.
\item {\tt g} the gradient vector at the minimum (so the user can judge closeness to numerical zero).
\item {\tt Hi} the inverse of the Hessian matrix at the minimum (useful if the objective is a negative log likelihood). 
\end{trivlist} 

The function should issue errors or warnings (using {\tt stop} or {\tt warning} as appropriate) in at least the following cases. 1. If the objective or derivatives are not finite at the initial {\tt theta}; 2. If the step fails to reduce the objective despite trying {\tt max.half} step halvings; 3. If {\tt maxit} is reached without convergence; 4. If the Hessian is not positive definite at convergence.

\bigskip

\noindent {\bf Other considerations:} 
\begin{enumerate} 
\item You can test whether your Hessian is positive definite, by seeing if {\tt chol} succeeds in finding its Cholesky factor. Read the documentation for {\tt try} to find out how to trap the error generated by {\tt chol} if it fails. If the Hessian is not positive definite, add a small multiple of the identity matrix to it and try again. One approach is to start by adding  $\epsilon {\bf I}$ where $\epsilon $ is the largest absolute value in your Hessian, multiplied by $10^{-8}$. If the perturbed Hessian is still not positive definite, keep multiplying $\epsilon $ by 10 until it is.  
\item If your Newton step does not reduce the objective, or leads to a non-finite objective or derivatives, you will need to repeatedly half the step until the objective is reduced. 
\item To judge whether the gradient vector is close enough to zero, you will need to consider the magnitude of the objective (you can't expect gradients to to be down at $10^{-10}$ if the objective is of order $10^{10}$, for example). So the gradients are judged to be zero when they are smaller than {\tt tol} multiplied by the objective. But then there is a problem it the objective is zero at the minimum - we can then never succeed in making the magnitude of the gradient less than the magnitude of the objective. So {\tt fscale} is provided. Then if {\tt f0} is the current value of the objective and {\tt g} the current gradient, 
\begin{verbatim}
max(abs(g)) < (abs(f0)+fscale)*tol
\end{verbatim}
is a suitable condition for convergence.
\item If no Hessian matrix is supplied, your code should generate one by finite differencing the gradient vector. Such an approximate Hessian will be asymmetric: \verb^H <- 0.5 * (t(H) + H)^ fixes that.
\end{enumerate}

\bigskip

\noindent {\bf Hints:}
\begin{enumerate} 
\item Recall from the lectures that Rosenbrock's function is
\begin{verbatim}
{ k*(z-x^2)^2 + (1-x)^2}
\end{verbatim} 
and that you can use {\tt deriv} to get the required gradient and Hessian. Note however that if you use {\tt deriv} you will need to write a wrapper function to call the function it creates. This is necessary because the objective function expected by {\tt newton} expects the optimization parameters ($z$ and $x$, here) to be in a single parameter vector. Also the gradient and Hessian returned by the function {\tt deriv} creates are not returned as a vector and matrix, so you will need to modify them slightly. $z=-.5$, $x=2$ are reasonable starting values to use as illustrative examples. 
\item The following code gives a simple example of passing named arguments `through' a function using the `\verb+...+' argument. This illustrates how `\verb+...+' can be used to pass the named arguments to {\tt f}, through {\tt newton}, without {\tt newton} having to `know' what those arguments are. 
\begin{verbatim}
printxt <- function(n,txt1,txt2) { cat(n,txt1,txt2,"\n") }
foo <- function(a=1,...) { printxt(10*a,...) }
foo(3,txt1="pizzas",txt2="please!")
\end{verbatim}
\item It is a good idea to write the code to deal with the case in which gradient and Hessian are available first. Then modify it to add the code to deal with the case in which the Hessian is not available.
\item It is a good idea to test your code on more than just Rosenbrock's function. 
\item In addition to the R Functions already mentioned, some of the following may be useful: {\tt abs}, {\tt attr},   {\tt backsolve}, {\tt chol}, {\tt diag}, {\tt forwardsolve}, {\tt inherits}, {\tt is.finite}, {\tt is.null}, {\tt length}, {\tt matrix}, {\tt max}, {\tt ncol}, {\tt nrow}.
\item We will look at your comments when marking. Good comments explain the purpose of each function and clarify what the code is supposed to do, so that someone reading the code can rapidly understand its purpose and why it is written as it is. Comments that simply describe mechanically what each line of code does  are useless. For example
\begin{verbatim}
f <- A %*% y ## multiplies y by A to get f
\end{verbatim}
simply describes what is obvious from the code itself and is useless, whereas
\begin{verbatim}
f <- A %*% y ## compute the fitted values, f, from the response, y
\end{verbatim}
tells the reader the purpose of the code. 

\end{enumerate}

\bigskip

\noindent {\bf What to submit:} Within the {\tt github} repository for your group you should create a single text file {\tt newton.R} containing the implementation of your {\tt newton} function.  A second file {\tt rosenbrock.R} should contain the code illustrating use of {\tt newton} to optimize the Rosenbrock function, with and without the exact Hessian supplied. {\tt rosenbrock.R} should source {\tt newton.R}, and sourcing {\tt rosenbrock.R}
in R studio should simply cause both examples to run. What is submitted should not contain other testing code, or code not required by {\tt newton} or the 2 examples. Other files in your repository will not be marked - you might want to put any such files in a subfolder of the repository to keep things tidy. 
\bigskip 

\noindent {\bf Marking:} Marks will be awarded for the supplied code running properly, and for it behaving correctly on some further test objective functions that you will not be given. Marks will also be awarded for code structure and commenting. Highest marks will be awarded for concise, efficient, well structured, well commented code behaving according to the specification. 
\end{document}

     